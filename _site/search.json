[
  {
    "objectID": "book/05-tidyverse.html",
    "href": "book/05-tidyverse.html",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "",
    "text": "Welcome to the exciting world of tidyverse! In this chapter, weâ€™ll build on our knowledge of R by exploring the tidyverse, a collection of R packages designed for data science. Weâ€™ll create a virtual slot machine to demonstrate the power and simplicity of tidyverse functions.",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check",
    "href": "book/05-tidyverse.html#learning-check",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n1.1 Learning Check ğŸ",
    "text": "1.1 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#filter-subset-rows",
    "href": "book/05-tidyverse.html#filter-subset-rows",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.1 filter(): Subset Rows",
    "text": "2.1 filter(): Subset Rows\n\n# Find all books published after 1900\nbooks %&gt;% \nfilter(year &gt; 1900)\n\n# A tibble: 4 Ã— 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\n\n\n\n\n\n\nThe Pipe Operator %&gt;%\n\n\n\n\n\nThe %&gt;% operator is called the â€œpipeâ€ operator. Itâ€™s a fundamental concept in the tidyverse that greatly enhances code readability and workflow. Hereâ€™s how it works:\n\nFunction chaining: The pipe takes the output of one function and passes it as the first argument to the next function. This allows us to chain multiple operations together in a logical sequence.\nLeft-to-right reading: Instead of nesting functions within each other, which can be hard to read, the pipe allows us to read our code from left to right, much like we read English.\nImproved readability: By using the pipe, we can break down complex operations into a series of smaller, more manageable steps.\n\nFor example, letâ€™s compare these two equivalent operations:\nWithout pipe:\n\nfilter(books, year &gt; 1900)\n\n# A tibble: 4 Ã— 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\nWith pipe:\n\nbooks %&gt;% filter(year &gt; 1900)\n\n# A tibble: 4 Ã— 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\nThe piped version can be read as â€œTake the books data, then filter it to keep only books published after 1900â€.\nFor more complex operations, the benefits become even clearer, which we will see in a moment.",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#select-choose-columns",
    "href": "book/05-tidyverse.html#select-choose-columns",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.2 select(): Choose Columns",
    "text": "2.2 select(): Choose Columns\n\n# Select only title and author columns\nbooks %&gt;%\nselect(title, author)\n\n# A tibble: 5 Ã— 2\n  title                  author    \n  &lt;chr&gt;                  &lt;chr&gt;     \n1 1984                   Orwell    \n2 Pride and Prejudice    Austen    \n3 The Great Gatsby       Fitzgerald\n4 To Kill a Mockingbird  Lee       \n5 The Catcher in the Rye Salinger",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#mutate-add-new-variables",
    "href": "book/05-tidyverse.html#mutate-add-new-variables",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.3 mutate(): Add New Variables",
    "text": "2.3 mutate(): Add New Variables\n\n# Add a new column for the book's age\nbooks %&gt;%\nmutate(age = 2024 - year)\n\n# A tibble: 5 Ã— 6\n  title                  author      year genre         pages   age\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328    75\n2 Pride and Prejudice    Austen      1813 Romance         432   211\n3 The Great Gatsby       Fitzgerald  1925 Modernist       180    99\n4 To Kill a Mockingbird  Lee         1960 Coming-of-age   281    64\n5 The Catcher in the Rye Salinger    1951 Coming-of-age   234    73",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#arrange-sort-rows",
    "href": "book/05-tidyverse.html#arrange-sort-rows",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.4 arrange(): Sort Rows",
    "text": "2.4 arrange(): Sort Rows\n\n# Sort books by year, oldest first\nbooks %&gt;%\narrange(year)\n\n# A tibble: 5 Ã— 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Pride and Prejudice    Austen      1813 Romance         432\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 1984                   Orwell      1949 Dystopian       328\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n5 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n\n\n\n\n\n\n\n\nComparing arrange() and order()\n\n\n\n\n\nIn Tidyverse, we use arrange() to sort data frames, which is often more intuitive and easier to use with multiple columns. In Base R, you typically use order() within square brackets or sort() for vectors.\nFor example:\nTidyverse: data %&gt;% arrange(column_name)\nBase R: data[order(data$column_name), ]\nThe Tidyverse method is more readable, especially when sorting by multiple columns or in descending order.",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#summarise-summarize-data",
    "href": "book/05-tidyverse.html#summarise-summarize-data",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.5 summarise(): Summarize Data",
    "text": "2.5 summarise(): Summarize Data\n\n# Calculate average number of pages\nbooks %&gt;%\nsummarise(avg_pages = mean(pages))\n\n# A tibble: 1 Ã— 1\n  avg_pages\n      &lt;dbl&gt;\n1       291",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#group_by-group-data-for-operations",
    "href": "book/05-tidyverse.html#group_by-group-data-for-operations",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.6 group_by(): Group Data for Operations",
    "text": "2.6 group_by(): Group Data for Operations\n\n# Average pages by genre\nbooks %&gt;%\ngroup_by(genre) %&gt;%\nsummarise(avg_pages = mean(pages))\n\n# A tibble: 4 Ã— 2\n  genre         avg_pages\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Coming-of-age      258.\n2 Dystopian          328 \n3 Modernist          180 \n4 Romance            432",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#chaining-multiple-actions",
    "href": "book/05-tidyverse.html#chaining-multiple-actions",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.7 Chaining Multiple Actions",
    "text": "2.7 Chaining Multiple Actions\nOne of the key advantages of Tidyverse is the ability to chain multiple actions together using the pipe operator (%&gt;%). Letâ€™s compare how we can perform a series of data manipulations using both Tidyverse and Base R.\nLetâ€™s say we want to: 1. Filter books published after 1900 2. Select only the title, author, and year columns 3. Sort the results by year 4. Get the first 3 entries\n\n2.7.1 Tidyverse Approach\n\nbooks %&gt;%\nfilter(year &gt; 1900) %&gt;%\nselect(title, author, year) %&gt;%\narrange(year) %&gt;%\nhead(3)\n\n# A tibble: 3 Ã— 3\n  title                  author      year\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt;\n1 The Great Gatsby       Fitzgerald  1925\n2 1984                   Orwell      1949\n3 The Catcher in the Rye Salinger    1951\n\n\nIn this Tidyverse approach, we can read the code from left to right, following the logical flow of operations. Each step is clearly defined, and the pipe operator (%&gt;%) passes the result of each operation to the next.\n\n2.7.2 Base R Approach\n\n# Filter books published after 1900\nfiltered_books &lt;- books[books$year &gt; 1900, ]\n# Select only title, author, and year columns\nselected_books &lt;- filtered_books[, c(\"title\", \"author\", \"year\")]\n# Sort by year\nsorted_books &lt;- selected_books[order(selected_books$year), ]\n# Get the first 3 entries\nresult &lt;- head(sorted_books, 3)\n# View the result\nresult\n\n# A tibble: 3 Ã— 3\n  title                  author      year\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt;\n1 The Great Gatsby       Fitzgerald  1925\n2 1984                   Orwell      1949\n3 The Catcher in the Rye Salinger    1951\n\n\nIn the Base R approach, we need to create intermediate variables at each step. The code reads from top to bottom, with each line representing a separate operation.",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check-1",
    "href": "book/05-tidyverse.html#learning-check-1",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.8 Learning Check ğŸ",
    "text": "2.8 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#hands-on-coding",
    "href": "book/05-tidyverse.html#hands-on-coding",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n2.9 Hands-On Coding ğŸ’»",
    "text": "2.9 Hands-On Coding ğŸ’»\nTry the following exercises:\n\nUse filter() to find all books written by Austen or Orwell.\nUse arrange() to sort the books by number of pages, from longest to shortest.\nUse mutate() to add a new column called words, assuming an average of 250 words per page.\nUse group_by() and summarise() to find the earliest publication year for each genre.\n\n\n2.9.1 Exercise 1: Filter books by Austen or Orwell\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the %in% operator within filter() to check if the author is in a vector of names.\n\n\n\n\n\n\nbooks %&gt;%\n  filter(author _ c(\"Austen\", \"Orwell\"))\n\n\n\n\nbooks %&gt;%\n  filter(author %in% c(\"Austen\", \"Orwell\"))\n\n# A tibble: 2 Ã— 5\n  title               author  year genre     pages\n  &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 1984                Orwell  1949 Dystopian   328\n2 Pride and Prejudice Austen  1813 Romance     432\n\n\n\n\n2.9.2 Exercise 2: Sort books by pages, longest to shortest\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse arrange() with desc() to sort in descending order.\n\n\n\n\n\n\nbooks %&gt;%\n  arrange(_(_))\n\n\n\n\nbooks %&gt;%\n  arrange(desc(pages))\n\n# A tibble: 5 Ã— 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Pride and Prejudice    Austen      1813 Romance         432\n2 1984                   Orwell      1949 Dystopian       328\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n5 The Great Gatsby       Fitzgerald  1925 Modernist       180\n\n\n\n\n2.9.3 Exercise 3: Add words column\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse mutate() to create a new column. Multiply the â€˜pagesâ€™ column by 250.\n\n\n\n\n\n\nbooks %&gt;%\n  mutate(words = _ * _)\n\n\n\n\nbooks %&gt;%\n  mutate(words = pages * 250)\n\n# A tibble: 5 Ã— 6\n  title                  author      year genre         pages  words\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328  82000\n2 Pride and Prejudice    Austen      1813 Romance         432 108000\n3 The Great Gatsby       Fitzgerald  1925 Modernist       180  45000\n4 To Kill a Mockingbird  Lee         1960 Coming-of-age   281  70250\n5 The Catcher in the Rye Salinger    1951 Coming-of-age   234  58500\n\n\n\n\n2.9.4 Exercise 4: Find earliest publication year by genre\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse group_by() followed by summarise(). Within summarise(), use min() to find the earliest year.\n\n\n\n\n\n\nbooks %&gt;%\n  group_by(_) %&gt;%\n  summarise(earliest_year = _(_))\n\n\n\n\nbooks %&gt;%\n  group_by(genre) %&gt;%\n  summarise(earliest_year = min(year))\n\n# A tibble: 4 Ã— 2\n  genre         earliest_year\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Coming-of-age          1951\n2 Dystopian              1949\n3 Modernist              1925\n4 Romance                1813",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#expand-the-books-dataset",
    "href": "book/05-tidyverse.html#expand-the-books-dataset",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.1 Expand the Books Dataset",
    "text": "3.1 Expand the Books Dataset\nLetâ€™s expand the books dataset to include some more variables for visualization purposes:\n\nnovels &lt;- books %&gt;%\n  mutate(\n    words = pages*250, # Estimating word count based on pages\n    characters = c(30, 25, 15, 20, 10), # Number of named characters (estimated)\n    rating = c(4.2, 4.5, 4.0, 4.3, 4.1), # Modern reader ratings (out of 5)\n    male_chars = c(20, 10, 10, 12, 7), # Number of male characters (estimated)\n    female_chars = c(10, 15, 5, 8, 3) # Number of female characters (estimated)\n  )\n# View the dataset\nnovels\n\n# A tibble: 5 Ã— 10\n  title             author  year genre pages  words characters rating male_chars\n  &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 1984              Orwell  1949 Dystâ€¦   328  82000         30    4.2         20\n2 Pride and Prejudâ€¦ Austen  1813 Romaâ€¦   432 108000         25    4.5         10\n3 The Great Gatsby  Fitzgâ€¦  1925 Modeâ€¦   180  45000         15    4           10\n4 To Kill a Mockinâ€¦ Lee     1960 Comiâ€¦   281  70250         20    4.3         12\n5 The Catcher in tâ€¦ Salinâ€¦  1951 Comiâ€¦   234  58500         10    4.1          7\n# â„¹ 1 more variable: female_chars &lt;dbl&gt;\n\n\nThis dataset gives us a rich set of variables to explore, including publication year, word count, genre, character gender representation, and modern reader ratings.",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#the-basic-structure-of-a-ggplot",
    "href": "book/05-tidyverse.html#the-basic-structure-of-a-ggplot",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.2 1. The Basic Structure of a ggplot",
    "text": "3.2 1. The Basic Structure of a ggplot\nEvery ggplot2 plot starts with the ggplot() function and uses + to add layers. The basic structure is:\n\nggplot(data = &lt;DATA&gt;) +\nGEOM_FUNCTION(mapping = aes(&lt;MAPPINGS&gt;))\n\nLetâ€™s create a simple scatter plot of publication year vs.Â word count (thousands):\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words / 1000))\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Concepts\n\n\n\n\n\n\n\nggplot(data = novels): Initializes the plot with our dataset\n\ngeom_point(): Adds a layer of points (for a scatter plot)\n\naes(x = year, y = words): Maps variables to aesthetic properties (here, x and y positions)",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#aesthetic-mappings",
    "href": "book/05-tidyverse.html#aesthetic-mappings",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.3 2. Aesthetic Mappings",
    "text": "3.3 2. Aesthetic Mappings\nAesthetics are visual properties of the objects in your plot. Common aesthetics include: - x and y positions - color - size - shape\nLetâ€™s map the rating to the color of the points:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words, color = rating))\n\n\n\n\n\n\n\nAlternatively, we can also use the size of the points to indicate the rating:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words, size = rating))",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#adding-labels-with-labs",
    "href": "book/05-tidyverse.html#adding-labels-with-labs",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.4 3. Adding Labels with labs()",
    "text": "3.4 3. Adding Labels with labs()\nWe can improve our plot by adding informative labels:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words/1000, size = rating)) +\nlabs(title = \"Classic Novels: Publication Year vs. Word Count\",\n     x = \"Year of Publication\",\n     y = \"Number of Words (thousands)\",\n     size = \"Rating\")",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#geometric-objects-geoms",
    "href": "book/05-tidyverse.html#geometric-objects-geoms",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.5 4. Geometric Objects (geoms)",
    "text": "3.5 4. Geometric Objects (geoms)\nDifferent geom functions create different types of plots. Letâ€™s create a bar plot of character counts:\n\nggplot(data = novels) +\ngeom_col(mapping = aes(x = title, y = characters)) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon geoms\n\n\n\n\n\n\n\ngeom_point(): Scatter plots\n\ngeom_line(): Line graphs\n\ngeom_col() or geom_bar(): Bar charts\n\ngeom_boxplot(): Box plots\n\n\n\n\n\n\n\n\n\n\nR Graph Gallery: Inspiration for Your Visualizations\n\n\n\n\n\nFor more inspiration and examples of whatâ€™s possible with ggplot2, check out the R Graph Gallery. This fantastic resource offers:\n\nA wide variety of chart types and styles\nReproducible code for each graph\nExplanations and use cases for different visualizations\nAdvanced techniques and customizations\n\nExploring the R Graph Gallery can help you discover new ways to visualize your data and improve your ggplot2 skills!",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check-2",
    "href": "book/05-tidyverse.html#learning-check-2",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n3.6 Learning Check ğŸ",
    "text": "3.6 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#hands-on-coding-1",
    "href": "book/05-tidyverse.html#hands-on-coding-1",
    "title": "Â§5 Spinning the Reels ğŸ°",
    "section": "\n4.1 Hands-On Coding ğŸ’»",
    "text": "4.1 Hands-On Coding ğŸ’»\nLetâ€™s explore our slot machine results with some exercises. Remember to use tidyverse functions like filter(), summarise(), group_by(), and ggplot().\n\n4.1.1 Exercise 1: Summarize the Results\nCalculate the total number of plays, number of wins, and the win percentage.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse summarise() with n(), sum() functions.\n\n\n\n\n\n\nresults %&gt;%\n  summarise(\n    total_plays = _(),\n    wins = _(win),\n    win_percentage = wins/ _ *100\n)\n\nresults %&gt;%summarise(n())\n\n\n\n\nresults %&gt;%\n  summarise(\n    total_plays = n(),\n    wins = sum(win),\n    win_percentage = wins / total_plays * 100\n  )\n\n# A tibble: 1 Ã— 3\n  total_plays  wins win_percentage\n        &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n1         100     1              1\n\n\n\n\n\n\n\n\nn() vs.Â nrow()\n\n\n\n\n\n\nn() is a dplyr function:\n\n\nItâ€™s designed to work seamlessly within dplyr verbs like summarise(), mutate(), and filter().\nItâ€™s particularly useful when working with grouped data.\n\n\nnrow() is a base R function:\n\n\nIt works on data frames and matrices, but not directly within dplyr pipelines.\nIt doesnâ€™t automatically respect grouping in dplyr operations.\n\n\nBehavior with grouped data:\n\n\nn() will give you the count for each group when used with group_by().\nnrow() will always return the total number of rows in the entire data frame.\n\n\nSyntax in dplyr operations:\n\n\nn() can be used directly: summarise(count = n())\nnrow() typically needs to be wrapped: summarise(count = nrow(.))\n\n\nPerformance:\n\n\nn() is optimized for dplyr operations and can be faster in some cases.\n\n\n\n\n\n\n4.1.2 Exercise 2: Find Winning Combinations\nCreate a new data frame showing only the winning plays and their symbol combinations.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse filter() to select winning plays, then select() to choose the columns you want to display.\n\n\n\n\n\n\nwinning_plays &lt;- results %&gt;%\n  filter(_ == TRUE) %&gt;%\n  select(play, symbol1, symbol2, symbol3)\n\nprint(winning_plays)\n\n\n\n\nwinning_plays &lt;- results %&gt;%\n  filter(win == TRUE) %&gt;%\n  select(play, symbol1, symbol2, symbol3)\nprint(winning_plays)\n\n# A tibble: 1 Ã— 4\n   play symbol1 symbol2 symbol3\n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1    28 ğŸ‹      ğŸ‹      ğŸ‹     \n\n\n\n\n4.1.3 Exercise 3: Visualize Symbol Distribution\nCreate a bar plot showing the distribution of symbols in the first reel (symbol1 column).\nEmojis often canâ€™t be rendered directly in plots. While there are packages like emojifont or ggtext that can handle emoji rendering, for simplicity, weâ€™ll use a text representation of the symbols.\n\nresults &lt;- results %&gt;%\n  mutate(symbol1_text = case_when(\n    symbol1 == \"ğŸ’\" ~ \"Cherry\",\n    symbol1 == \"ğŸ‹\" ~ \"Lemon\",\n    symbol1 == \"ğŸŠ\" ~ \"Orange\",\n    symbol1 == \"ğŸ‡\" ~ \"Grapes\",\n    symbol1 == \"ğŸ””\" ~ \"Bell\",\n    symbol1 == \"ğŸ’\" ~ \"Diamond\"\n  ))\n\n\n\n\n\n\n\n\nHint 1/2\n\n\n\n\n\nUse ggplot() with geom_bar() to create a bar chart of symbol1.\n\n\n\n\n\n\n\n\n\n\n\nHint 2/2\n\n\n\n\n\nIn aes(), map symbol1 to both x and fill for a colored bar chart.\n\n\n\n\n\n\nggplot(results, aes(x = _, fill = _)) +\n  geom_bar() +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") \n\n\n\nA Simple Plot\n\nggplot(results, aes(x = symbol1_text, fill = symbol1_text)) +\n  geom_bar() +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") \n\n\n\n\n\n\n\nA Customized Plot\nA strength of ggplot2 is that it allows various cutomizations of the plot. Below is an example where we manually set the fill colors of the bar plot. You can also check out packages such as RColorBrewer for presets of color pallettes. ggplot2 also has preset themes that you can use to immediately give the plot a cleaner or more professional look (e.g., the theme_minimal() used in the following example).\n\n# Define a custom color palette\nsymbol_colors &lt;- c(\n  \"Cherry\" = \"#D2042D\",  # Red\n  \"Lemon\" = \"#FFF700\",   # Yellow\n  \"Orange\" = \"#FFA500\",  # Orange\n  \"Grapes\" = \"#6F2DA8\",  # Purple\n  \"Bell\" = \"#FFD700\",    # Gold\n  \"Diamond\" = \"#B9F2FF\"  # Light Blue\n)\n\nggplot(results, aes(x = symbol1_text, fill = symbol1_text)) +\n  geom_bar() +\n  scale_fill_manual(values = symbol_colors) +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend as it's redundant\n\n\n\n\n\n\n\n\n\nCongratulations! Youâ€™ve now practiced using various tidyverse functions to analyze and visualize data from our virtual slot machine. These skills are fundamental in data manipulation and analysis, which are crucial in many digital humanities projects.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\n\nIn this chapter, weâ€™ve covered:\n\nThe basics of tidyverse and its core packages\nData manipulation with dplyr functions\nData visualization with ggplot2\nApplied tidyverse concepts to analyze our books dataset\nBuilt a virtual slot machine using tidyverse functions\n\n\n\n\nThese skills form an essential foundation for working with data in R using the tidyverse. As we progress in our digital humanities journey, weâ€™ll build upon these concepts to perform more complex data manipulations and analyses.\n\n\nTidyverse Basics\n\n\nData Manipulation\n\n\nData Visualization\n\n\nPractical Application",
    "crumbs": [
      "Book",
      "Â§5 Spinning the Reels ğŸ°"
    ]
  },
  {
    "objectID": "book/03-dice.html",
    "href": "book/03-dice.html",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "",
    "text": "Welcome to your first hands-on project with R! Weâ€™ll create a virtual pair of dice to learn fundamental R concepts. Just as historians analyze artifacts or literary scholars examine texts, weâ€™ll be examining the building blocks of data analysis.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#understanding-the-rstudio-interface",
    "href": "book/03-dice.html#understanding-the-rstudio-interface",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n1.1 Understanding the RStudio Interface ğŸ–¥ï¸",
    "text": "1.1 Understanding the RStudio Interface ğŸ–¥ï¸\nWhen you open RStudio, youâ€™ll see four main panes:\n\nConsole (usually bottom-left): This is where you type R commands and see results. Think of it as a conversation with R - you ask questions, and R answers.\nSource (usually top-left): Here you write and edit R scripts. Itâ€™s like a digital notebook for your code.\nEnvironment (usually top-right): This shows data and objects in Râ€™s memory. Imagine it as a shelf where R stores all the information youâ€™ve given it.\nFiles/Plots/Packages/Help (usually bottom-right): This multi-purpose pane is like a Swiss Army knife, offering file management, visualizations, add-ons, and documentation.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#running-r-code",
    "href": "book/03-dice.html#running-r-code",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n1.2 Running R Code ğŸƒâ€â™‚ï¸",
    "text": "1.2 Running R Code ğŸƒâ€â™‚ï¸\nThere are several ways to execute R code in RStudio:\n\n\nUsing the Console:\n\nType code directly into the Console and press Enter.\n\n\n\nUsing the Source Editor:\n\nWrite code in the Source Editor.\nRun a single line: Place cursor on the line and press Ctrl+Enter (Cmd+Enter on Mac).\nRun multiple lines: Highlight lines and press Ctrl+Enter (Cmd+Enter on Mac).\nRun entire script: Click â€œRunâ€ button or press Ctrl+Shift+Enter (Cmd+Shift+Enter on Mac).\n\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\n\n\nWriting code in the Source Editor allows you to save, edit, and rerun your entire analysis easily.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check",
    "href": "book/03-dice.html#learning-check",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n1.3 Learning Check ğŸ",
    "text": "1.3 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding",
    "href": "book/03-dice.html#hands-on-coding",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n1.4 Hands-On Coding ğŸ’»",
    "text": "1.4 Hands-On Coding ğŸ’»\nTry the following:\n\nOpen RStudio and create a new R script named â€œmy_first_script.Râ€\nIn the script, write a simple calculation (e.g., 5 * 7)\nRun this calculation using different methods:\n\nConsole\nRunning a single line\nRunning the entire script\n\n\nObserve where the result appears in the RStudio interface\n\n\n# In my_first_script.R\n5 * 7\n\n# In Console\n&gt; 5 * 7\n[1] 35",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#creating-a-die-as-an-object",
    "href": "book/03-dice.html#creating-a-die-as-an-object",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n2.1 Creating a Die as an Object ğŸ²",
    "text": "2.1 Creating a Die as an Object ğŸ²\nLetâ€™s start by creating a single die:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\n\n\n\n\n\n\nKey Concepts\n\n\n\n\n\n\n\nVector: A basic container that can hold multiple items of the same type.\n\n\n\n\n\n\n\n\n\n\nThe Colon Operator\n\n\n\n\n\nThe colon operator (:) returns every integer between two integers. Itâ€™s an easy way to create a sequence of numbers.\n\n\n\nğŸ™‹ğŸ»â€â™€ï¸ What if I want to create a vector thatâ€™s not a sequence of numbers?\n\n2.1.1 Combining Data: c()\n\nIn R, c() stands for â€œcombineâ€ or â€œconcatenateâ€. Itâ€™s used to create vectors, which are sequences of data elements of the same type. For example:\n\n# Creating a vector of numbers\nnumbers &lt;- c(2, 4, 6, 8, 10)\nnumbers\n\n[1]  2  4  6  8 10\n\n# Creating a vector of characters (text)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\nfruits\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n# Creating a vector of logical values\nlogical_values &lt;- c(TRUE, FALSE, TRUE, TRUE)\nlogical_values\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# You can also mix data types, but R will convert them to a single type\nmixed &lt;- c(1, \"two\", TRUE)\nmixed # R converts all to characters\n\n[1] \"1\"    \"two\"  \"TRUE\"\n\n\nThe c() function is versatile and essential for creating custom datasets in R. You can use it to make lists of any kind of data youâ€™re working with in your research.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#manipulating-objects",
    "href": "book/03-dice.html#manipulating-objects",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n2.2 Manipulating Objects",
    "text": "2.2 Manipulating Objects\nYou can perform various operations on your die object:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie * 2\n\n[1]  2  4  6  8 10 12\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\n\nğŸ¤¨ What do you notice about the results of the above operations?\n\n\n\n\n\n\nVectorized Operations in R\n\n\n\n\n\nR performs these operations element-wise on the die object because it uses vectorized operations. This means:\n\nEach element in the die vector is individually operated on.\nThe operation is applied to every element simultaneously.\nA new vector of the same length is returned as the result.\n\nThis vectorized approach makes R efficient for handling data and allows for concise, readable code when working with vectors or larger datasets.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-1",
    "href": "book/03-dice.html#learning-check-1",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n2.3 Learning Check ğŸ",
    "text": "2.3 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-1",
    "href": "book/03-dice.html#hands-on-coding-1",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n2.4 Hands-On Coding ğŸ’»",
    "text": "2.4 Hands-On Coding ğŸ’»\nTry the following:\n\nCreate an object called favorite_number and assign it your favorite number.\nCreate a vector called lucky_numbers with 3-5 numbers you consider lucky.\nUse the length() function to find out how many lucky numbers you have.\nCreate a new vector called unlucky_numbers by subtracting 1 from each of your lucky_numbers.\nCreate a vector called all_numbers that combines your lucky_numbers and unlucky_numbers.\n\n\n\n# 1. Create favorite_number\nfavorite_number &lt;- 7\n\n# 2. Create lucky_numbers\nlucky_numbers &lt;- c(3, 7, 13)\n\n# 3. Use length() function\nlength(lucky_numbers)\n\n[1] 3\n\n# 4. Create unlucky_numbers\nunlucky_numbers &lt;- lucky_numbers - 1\n\n# 5. Combine vectors\nall_numbers &lt;- c(lucky_numbers, unlucky_numbers)\nprint(all_numbers)\n\n[1]  3  7 13  2  6 12\n\n\n\n\n\n\n\n\nPrinting in R\n\n\n\n\n\nIn R, using print() is often optional when working in the console. Simply typing the object name (e.g., all_numbers) will automatically display its contents. However, print() is useful in functions or scripts where you want to ensure output is displayed. Itâ€™s also helpful for explicitly showing your intent to output a value.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#key-concepts-of-functions-in-r",
    "href": "book/03-dice.html#key-concepts-of-functions-in-r",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n3.1 Key Concepts of Functions in R",
    "text": "3.1 Key Concepts of Functions in R\n\nPurpose: Functions are reusable blocks of code that perform a specific task.\n\nStructure: A function typically has three parts:\n\n\nName: What you call the function (e.g., sum, mean, plot)\n\nArguments: Input values the function needs to do its job\n\nBody: The code that defines what the function does\n\nHereâ€™s the generic structure of a function in R:\n\nname_of_function &lt;- function(argument1, argument2, ...) {\n  # Body of the function\n  # Perform operations using the arguments\n\n  return(result)  # Output of the function\n}\n\nIn this structure:\n\n\nname_of_function is where you specify the functionâ€™s name\n\nfunction() keyword defines it as a function\n\n(argument1, argument2, ...) are the inputs the function accepts\nThe code between { and } is the body of the function\n\nreturn() specifies what the function outputs\n\nNow, letâ€™s look at a specific example:\n\ndouble_value &lt;- function(x) {\n  result &lt;- x * 2\n  return(result)\n}\n\nIn this example:\n\nThe function name is double_value\n\nIt has one argument: x\n\nThe body is the code between the curly braces { }\n\nThe return() statement specifies what the function outputs\n\nYou can use this function like this:\n\ndoubled_value &lt;- double_value(5)\ndoubled_value\n\n[1] 10\n\n\n\nInput and Output: Functions often take input (arguments) and return output (results).\nBuilt-in vs.Â Custom: R has many built-in functions, but you can also create your own. You can also set default values for the arguments so that the function can still run without input.\n\nSyntax: To use a function, type its name followed by parentheses containing any arguments:\nfunction_name(argument1, argument2, ...)\n\nDocumentation: You can learn about a functionâ€™s use and arguments with ?function_name in the console.\n\nRemember, functions are powerful tools that help you organize and streamline your code, making your analysis more efficient and readable.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#using-built-in-functions",
    "href": "book/03-dice.html#using-built-in-functions",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n3.2 Using Built-in Functions",
    "text": "3.2 Using Built-in Functions\nR comes with many built-in functions that you can use right away. These are like the basic tools that come in a starter toolkit. Letâ€™s look at a few examples:\n\n# The round() function rounds numbers\nround(3.14159)\n\n[1] 3\n\n# The sum() function adds numbers together\nsum(1, 2, 3, 4, 5)\n\n[1] 15\n\n# The length() function tells you how many items are in a list or vector\nlength(die)\n\n[1] 6\n\n\nUsing a function is straightforward. You write the name of the function followed by parentheses. Inside the parentheses, you put the information (called â€œargumentsâ€) that the function needs to do its job.\n\n\n\n\n\n\nComments in R\n\n\n\n\n\nDid you notice the # â€¦ part in the above code chunk?\nIn R, the # symbol is used to create comments. Anything after a # on a line is ignored by R when running the code. Comments are useful for:\n\nExplaining what your code does\nTemporarily disabling code without deleting it\nOrganizing your script into sections\n\n\n\n\n\n3.2.1 Simulating Randomness: sample()\n\nTo simulate rolling a die, we need a function that can randomly select a number. The sample() function is perfect for this task. Itâ€™s like reaching into a bag and pulling out a random item.\n\n# Roll the die once\nsample(x = die, size = 1)\n\n[1] 5\n\n# Roll the die again\nsample(x = die, size = 1)\n\n[1] 4\n\n\nIn this case, x = die tells the function what to choose from (our die object), and size = 1 says to pick one number.\n\n\n\n\n\n\nSampling With Replacement\n\n\n\n\n\nBy default, sample() doesnâ€™t put the number back in the â€œbagâ€ after selecting it. For dice rolling, we want to allow the same number to be selected multiple times. Use replace = TRUE in the sample() function to achieve this.\n\n\n\n\n# Roll two dice\nsample(die, size = 2, replace = TRUE)\n\n[1] 3 4",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#writing-your-own-functions",
    "href": "book/03-dice.html#writing-your-own-functions",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n3.3 Writing Your Own Functions",
    "text": "3.3 Writing Your Own Functions\nLetâ€™s create a function to roll two dice:\n\nroll &lt;- function() {\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nLetâ€™s break this down:\n\n\nroll &lt;- function() { ... } creates a new function named roll.\nInside the curly braces { } is the â€œbodyâ€ of the function - the instructions for what it should do.\n\ndice &lt;- sample(die, size = 2, replace = TRUE) rolls two dice.\n\nsum(dice) adds up the numbers rolled.\n\nNow, whenever you want to roll two dice and get the sum, you can simply use:\n\nroll()\n\n[1] 9\n\nroll()\n\n[1] 6",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-2",
    "href": "book/03-dice.html#learning-check-2",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n3.4 Learning Check ğŸ",
    "text": "3.4 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-2",
    "href": "book/03-dice.html#hands-on-coding-2",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n3.5 Hands-On Coding ğŸ’»",
    "text": "3.5 Hands-On Coding ğŸ’»\nLetâ€™s apply what weâ€™ve learned to simulate flipping a coin. This is similar to rolling a die, but with only two possible outcomes.\n\nCreate an object called coin that represents the two sides of a coin (use the numbers 1 and 2).\nUse the sample() function to simulate flipping the coin once.\nCreate a function called flip_coin() that simulates flipping a coin and returns the result.\nModify your flip_coin() function to have an argument named n_flips so that the user could specify the number of time the coin is flipped.\n\n\n\n# 1. Create coin object\ncoin &lt;- 1:2\n# 2. Flip coin once using sample()\nsample(coin, 1)\n\n[1] 1\n\n# 3. Create flip_coin() function\nflip_coin &lt;- function() {\nsample(coin, size = 1)\n}\n# 4. Modify flip_coin() to have a n_flips argument\nflip_coin &lt;- function(n_flips) {\n  sample(coin, size = n_flips, replace = TRUE)\n}",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#why-use-quarto",
    "href": "book/03-dice.html#why-use-quarto",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n4.1 Why Use Quarto?",
    "text": "4.1 Why Use Quarto?\n\n\nReproducibility: Quarto documents contain both code and narrative that make it easy to reproduce your analyses.\n\nProfessional Presentation: Create polished, professional-looking documents.\n\nFlexibility: Output to various formats like HTML, PDF, or slideshows.\n\n\n\n\n\n\n\nThe Importance of Reproducibility\n\n\n\n\n\nIn the humanities, as in all academic fields, being able to reproduce the analytical process is crucial. When you write your R code in scripts, youâ€™re creating a record of every step in your analysis. This is like creating detailed footnotes or citations in a research paper - it allows others (or future you!) to follow your work exactly.\nUsing scripts also makes it easy to make changes and rerun your entire analysis, which is much more efficient than trying to remember and retype everything in the console.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#quarto-document",
    "href": "book/03-dice.html#quarto-document",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n4.2 Quarto Document",
    "text": "4.2 Quarto Document\nA Quarto document has two main parts: - A header at the top (called YAML header) - The main content (text and code)\nBelow is an example:\nyaml\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\nformat: html\n---\nThis is where you write your content.\n\nYou can use markdown to format your text or use the \"visual editing mode\" to have an experience similar to Microsoft Word editor.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#essential-quarto-concepts",
    "href": "book/03-dice.html#essential-quarto-concepts",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n4.3 Essential Quarto Concepts",
    "text": "4.3 Essential Quarto Concepts\n\n4.3.1 Editing Quarto Files: Source vs.Â Visual Editor\nQuarto documents can be edited in two modes: Source and Visual.\n\n\nSource Editor: This mode shows you the raw Markdown and YAML content of your document. Itâ€™s great for:\n\nPrecise control over your documentâ€™s structure\nEditing code chunks directly\nWorking with advanced Quarto features\n\n\n\nVisual Editor: This mode provides a WYSIWYG (What You See Is What You Get) interface, similar to word processors like Microsoft Word. Itâ€™s useful for:\n\nFormatting text without knowing Markdown syntax\nEasily inserting tables, images, and other elements\nCollaborating with team members who may not be familiar with Markdown\n\n\n\n\n\n\n\n\n\nMarkdown Basics\n\n\n\n\n\nMarkdown is a lightweight markup language used in Quarto documents. Here are some basic syntax examples:\n\n\n# Heading 1, ## Heading 2, ### Heading 3, etc. for headings\n\n*italic* or _italic_ for italic text\n\n\n**bold** or __bold__ for bold text\n\n\n- item for unordered lists\n\n1. item for ordered lists\n\n[link text](URL) for links\n\n\n![alt text](image.jpg) for images\n\nFor a more comprehensive guide, check out Quartoâ€™s Markdown Basics.\n\n\n\n\n4.3.2 Rendering\nRendering is the process of converting your Quarto document into its final output format (e.g., HTML, PDF, or Word). To render your document:\n\nClick the â€œRenderâ€ button in RStudio, or\nUse the keyboard shortcut Ctrl+Shift+K (Cmd+Shift+K on Mac)\n\nWhen you render, Quarto executes all the code in your document and combines the results with your text to create the final output.\n\n4.3.3 (Optional) Code Chunk Options\nCode chunks in Quarto can be customized using options. These options control how the code is executed and displayed. Here are some common options:\n```{r}\n#| label: chunk-name\n#| echo: true\n#| eval: true\n#| warning: false\n#| message: false\n\n# Your R code here\n```\n\n\nlabel: Gives the chunk a unique name\n\necho: Controls whether the code is displayed in the output (true/false)\n\neval: Determines if the code should be executed (true/false)\n\nwarning: Shows or hides warnings (true/false)\n\nmessage: Shows or hides messages (true/false)\n\nYou can set these options globally in the YAML header or for individual chunks.\n\n\n\n\n\n\nPro Tip\n\n\n\n\n\nUse code chunk options to control what your readers see. For example, you might hide code for complex calculations but show the results.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-3",
    "href": "book/03-dice.html#learning-check-3",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n4.4 Learning Check ğŸ",
    "text": "4.4 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-3",
    "href": "book/03-dice.html#hands-on-coding-3",
    "title": "Â§3 Rolling the Dice ğŸ²",
    "section": "\n4.5 Hands-On Coding ğŸ’»",
    "text": "4.5 Hands-On Coding ğŸ’»\nLetâ€™s practice creating a simple Quarto document about our coin flipping simulation:\n\nCreate a new Quarto document in RStudio.\nAdd a title â€œCoin Flipping Simulationâ€ and your name as the author.\nWrite a brief introduction about coin flipping simulations.\nInsert a code chunk that defines your coin object and flip_coin() function.\nAdd another code chunk that uses your flip_coin() function to simulate flipping a coin 10 times.\nAdd some text explaining the results.\nRender it as an html file.\n\n\n---\ntitle: \"Coin Flipping Simulation\"\nauthor: \"Your Name\"\nformat: html\n---\n# Introduction\nIn this document, we'll simulate flipping coins using R functions we've created.\n\n# Defining Our Coin and Flip Function\nFirst, let's define our coin and create a function to flip it:\n\n\ncoin &lt;- 1:2   \n\nflip_coin &lt;- function(n_flips) {\n  sample(coin, size = n_flips, replace = TRUE)\n}\n\n# Flipping a Coin 10 Times\nNow, let's flip our coin 10 times:\n\nten_flips &lt;- flip_coin(10)\nprint(ten_flips)\n\n [1] 2 1 1 1 2 2 2 2 2 1\n\n\nThis simulates flipping a coin 10 times. Each 1 represents heads, and each 2 represents tails. We then count how many times we got heads.\nCongratulations! ğŸ‰ Youâ€™ve just created your first Quarto document combining narrative text and code analysis. This is an important step in your journey of digital humanities, as it allows you to present your research and analysis in a clear, reproducible format. As you continue to explore, youâ€™ll find that Quarto is a powerful tool for integrating your humanities insights with computational methods.",
    "crumbs": [
      "Book",
      "Â§3 Rolling the Dice ğŸ²"
    ]
  },
  {
    "objectID": "book/06-qta.html",
    "href": "book/06-qta.html",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "",
    "text": "Welcome to the world of quantitative text analysis! In this chapter, weâ€™ll explore the fundamental concepts and techniques used to prepare text data for analysis. Weâ€™ll use a collection of classic novels as our dataset to demonstrate these concepts.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#qta-in-digital-humanities",
    "href": "book/06-qta.html#qta-in-digital-humanities",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n1.1 QTA in Digital Humanities",
    "text": "1.1 QTA in Digital Humanities\nIn Digital Humanities, QTA offers powerful tools for exploring large text corpora:\n\nScale: Analyze vast collections of texts, revealing patterns across time periods, genres, or authors.\nDistant Reading: Observe broader patterns in literature and cultural production.\nHypothesis Testing: Empirically test literary and cultural theories across large datasets.\nDiscovery: Reveal unexpected patterns or connections, sparking new research questions.\nInterdisciplinary: Combine methods from linguistics, computer science, and statistics with humanistic inquiry.\nVisualization: Present textual data in new, visually interpretable ways.\n\nQTA complements traditional close reading, offering Digital Humanities scholars new perspectives on cultural, historical, and literary phenomena.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#install-package-manager-pacman",
    "href": "book/06-qta.html#install-package-manager-pacman",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.1 Install Package Manager pacman\n",
    "text": "2.1 Install Package Manager pacman\n\nBefore we begin our text analysis, letâ€™s introduce a helpful tool for managing R packages: pacman. The pacman package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.\nKey features of pacman:\n\nIt combines the functionality of install.packages() and library() into a single function p_load().\nIt automatically installs packages if theyâ€™re not already installed.\nIt can load multiple packages with a single line of code.\n\nLetâ€™s install and load pacman:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\nlibrary(pacman)\n\nNow we can use p_load() to efficiently load (and install if necessary) the packages weâ€™ll need for our text analysis:\n\np_load(janeaustenr, tidyverse)\n\nThis single line will ensure all the packages we need are installed and loaded, streamlining our setup process.\nFor this chapter, weâ€™ll use Jane Austenâ€™s â€œPride and Prejudiceâ€ to demonstrate text analysis techniques. To download the book dataset, run the following command:\n\npride_and_prejudice &lt;- austen_books() %&gt;%\n  filter(book == \"Pride & Prejudice\")\n\npride_and_prejudice\n\nAlternatively, you can also download it directly here:\nDownload Pride and Prejudice dataset\nYou can create a folder named â€œdataâ€ in save the csv file in your working directorya and load the data with the following command:\n\n# Load required packages\np_load(readr)\n\n# Now read the CSV file\npride_and_prejudice &lt;- read_csv(\"data/pride_and_prejudice.csv\")\n\nRows: 13030 Columns: 2\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (2): text, book\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npride_and_prejudice\n\n# A tibble: 13,030 Ã— 2\n   text                                                                    book \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 PRIDE AND PREJUDICE                                                     Pridâ€¦\n 2 &lt;NA&gt;                                                                    Pridâ€¦\n 3 By Jane Austen                                                          Pridâ€¦\n 4 &lt;NA&gt;                                                                    Pridâ€¦\n 5 &lt;NA&gt;                                                                    Pridâ€¦\n 6 &lt;NA&gt;                                                                    Pridâ€¦\n 7 Chapter 1                                                               Pridâ€¦\n 8 &lt;NA&gt;                                                                    Pridâ€¦\n 9 &lt;NA&gt;                                                                    Pridâ€¦\n10 It is a truth universally acknowledged, that a single man in possession Pridâ€¦\n# â„¹ 13,020 more rows\n\n\nThis gives us the full text of â€œPride and Prejudiceâ€ in a tidy format, with one line per row.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#lowercasing",
    "href": "book/06-qta.html#lowercasing",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.1 Lowercasing",
    "text": "2.1 Lowercasing\nLowercasing converts all text to lowercase, which helps standardize the text and ensures that words like â€œTheâ€ and â€œtheâ€ are treated as the same word.\n\n# Lowercasing\nlowercased_text &lt;- pride_and_prejudice %&gt;%\n  mutate(text = tolower(text))\n\n# Display a few rows to see the effect\nlowercased_text %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(text)\n\n# A tibble: 5 Ã— 1\n  text               \n  &lt;chr&gt;              \n1 pride and prejudice\n2 &lt;NA&gt;               \n3 by jane austen     \n4 &lt;NA&gt;               \n5 &lt;NA&gt;",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#removing-punctuation",
    "href": "book/06-qta.html#removing-punctuation",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.2 Removing Punctuation",
    "text": "2.2 Removing Punctuation\nRemoving punctuation can help in word-based analyses by separating words that might be connected by punctuation.\n\n# Removing punctuation\nno_punctuation &lt;- pride_and_prejudice %&gt;%\n  mutate(text = str_replace_all(text, \"[[:punct:]]\", \"\"))\n\n# Display a few rows to see the effect\nno_punctuation %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(text)\n\n# A tibble: 20 Ã— 1\n   text                                                                   \n   &lt;chr&gt;                                                                  \n 1 PRIDE AND PREJUDICE                                                    \n 2 &lt;NA&gt;                                                                   \n 3 By Jane Austen                                                         \n 4 &lt;NA&gt;                                                                   \n 5 &lt;NA&gt;                                                                   \n 6 &lt;NA&gt;                                                                   \n 7 Chapter 1                                                              \n 8 &lt;NA&gt;                                                                   \n 9 &lt;NA&gt;                                                                   \n10 It is a truth universally acknowledged that a single man in possession \n11 of a good fortune must be in want of a wife                            \n12 &lt;NA&gt;                                                                   \n13 However little known the feelings or views of such a man may be on his \n14 first entering a neighbourhood this truth is so well fixed in the minds\n15 of the surrounding families that he is considered the rightful property\n16 of some one or other of their daughters                                \n17 &lt;NA&gt;                                                                   \n18 My dear Mr Bennet said his lady to him one day have you heard that     \n19 Netherfield Park is let at last                                        \n20 &lt;NA&gt;                                                                   \n\n\n\n\n\n\n\n\nstringr Functions and Regex\n\n\n\n\n\nstr_replace_all() is a function from the stringr package (part of the tidyverse) that replaces all occurrences of a pattern in a string with another string. Common functions that are useful for working with text are:\n\nDetecting text: str_detect(), str_count()\n\nExtracting text: str_extract(), str_extract_all()\n\nReplacing text: str_replace(), str_replace_all()\n\nRemoving text: str_remove(), str_remove_all()\n\n\nRegex (regular expressions) are powerful tools for pattern matching in strings.\nHereâ€™s a breakdown of the regex used in the above example:\n\n\n\"[[:punct:]]\": This pattern matches any punctuation character.\n\n\n[] defines a character set. It matches any single character inside the brackets.\n\n[:punct:] is a POSIX character class representing all punctuation\n\n\n\nUsing these patterns with str_replace_all(), we can effectively clean our text by removing punctuation, numbers, and special characters.\nYou can check out this blogpost for a good variety of examples of how to use regular expressions to prepare text for analysis. Remember, the choice of regex patterns depends on your specific text cleaning needs.\n\n\n\n\n\n\n\n\n\nWhat is POSIX?\n\n\n\n\n\nPOSIX (Portable Operating System Interface) is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. In the context of regular expressions:\n\nPOSIX defines a standardized set of character classes that can be used across different programming languages and tools.\nThese character classes provide a convenient way to match groups of characters without explicitly listing them all.\n\nCommon POSIX character classes include:\n\n\n[:alnum:]: Alphanumeric characters (A-Z, a-z, 0-9)\n\n[:alpha:]: Alphabetic characters (A-Z, a-z)\n\n[:digit:]: Digits (0-9)\n\n[:lower:]: Lowercase letters (a-z)\n\n[:upper:]: Uppercase letters (A-Z)\n\n[:punct:]: Punctuation characters\n\n[:space:]: Whitespace characters (space, tab, newline, etc.)\n\n\nPOSIX character classes are particularly useful in text processing and data cleaning tasks, as they provide a consistent way to match character groups across different systems and languages.\nIn R and many other programming environments, these POSIX character classes can be used within square brackets in regular expressions, like [[:alnum:]] or [[:space:]].Using POSIX character classes can make your regular expressions more readable and portable across different systems and programming languages.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#learning-check",
    "href": "book/06-qta.html#learning-check",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.2 Learning Check ğŸ",
    "text": "2.2 Learning Check ğŸ\n\n\n\nNow letâ€™s proceed to preprocess the mock dataset!",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Introduction to Digital Humanities! We will cover several key topics in digital humanities and R programming. Each topic will be covered in theoretical/conceptual lectures, followed by practical sessions.\nHereâ€™s a breakdown of our semester:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\n9.9\nOrientation: Introduction to Digital Humanities\n\n\n2-4\n9.14, 9.23, 9.30, 10.14\nR Programming Basics\n\n\n5-7\n10.7 (holiday), 10.14, 10.21\nQuantitative Text Analysis Basics\n\n\n8-9\n10.28, 11.4\nSentiment Analysis\n\n\n10-12\n11.11, 11.18, 11.25\nTopic Modelling\n\n\n13-15\n12.2, 12.9, 12.16\nSocial Network Analysis\n\n\n16\n12.23\nEthics, AI and Future of Digital Humanities\n\n\n17-18\nExam Weeks\nFinal Assessment\n\n\n\n\n\n\n\n\n\nSchedule Subject to Change\n\n\n\nPlease note that this schedule is tentative and may be adjusted as needed throughout the semester to accommodate our learning progress and any unforeseen circumstances."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Introduction to Digital Humanities! We will cover several key topics in digital humanities and R programming. Each topic will be covered in theoretical/conceptual lectures, followed by practical sessions.\nHereâ€™s a breakdown of our semester:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\n9.9\nOrientation: Introduction to Digital Humanities\n\n\n2-4\n9.14, 9.23, 9.30, 10.14\nR Programming Basics\n\n\n5-7\n10.7 (holiday), 10.14, 10.21\nQuantitative Text Analysis Basics\n\n\n8-9\n10.28, 11.4\nSentiment Analysis\n\n\n10-12\n11.11, 11.18, 11.25\nTopic Modelling\n\n\n13-15\n12.2, 12.9, 12.16\nSocial Network Analysis\n\n\n16\n12.23\nEthics, AI and Future of Digital Humanities\n\n\n17-18\nExam Weeks\nFinal Assessment\n\n\n\n\n\n\n\n\n\nSchedule Subject to Change\n\n\n\nPlease note that this schedule is tentative and may be adjusted as needed throughout the semester to accommodate our learning progress and any unforeseen circumstances."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "Syllabus",
    "section": "ğŸ“Š Assessment",
    "text": "ğŸ“Š Assessment\nYour final grade will be composed of the following elements:\n\nMonthly Quiz ğŸ“: 30%\n\nRegular assessments to check your understanding of the course material. (å¾®åŠ©æ•™/é—®å·æ˜Ÿ)\n\nClass Participation and Attendance ğŸ™‹â€â™€ï¸: 30%\n\nYour active involvement in class discussions and activities is crucial! (å¾®åŠ©æ•™/Leaderboard)\n\n\n\n\n\n\n\nImportant\n\n\n\n\nAbsence of one-third of class time, excused or unexcused, will disqualify you from earning credits for the course.\n\n\n\nTerm Paper ğŸ“„: 40%\n\n\n\n\n\n\nğŸ¯ Term Project\n\n\n\nA digital humanities project of your choice! You should pick your topic for the term project by week 12. More details will be released later when you have learned R basics."
  },
  {
    "objectID": "syllabus.html#important-pep-talk",
    "href": "syllabus.html#important-pep-talk",
    "title": "Syllabus",
    "section": "ğŸš€ Important Pep Talk!",
    "text": "ğŸš€ Important Pep Talk!\n\n\n\n\n\n\nğŸ’ª Setting the Right Mindset\n\n\n\nRemember these key points while you embark on your DH journey:\n\n\n\nEmbrace the Learning Curve\n\nLearning digital tools and methods is like learning a new languageâ€”it takes time and practice.\nDonâ€™t be discouraged by initial difficulties; everyone faces challenges when starting out.\nCelebrate small victories and progress along the way.\n\n\n\nFrustration is Part of Growth\n\nFeeling frustrated? Congratulations! It means youâ€™re pushing your boundaries and learning.\nHadley Wickham (Father of modern R syntax) once said: â€œEveryone gets frustratedâ€¦ I still get frustrated occasionally when writing R code. Itâ€™s just a natural part of programming.â€\nTake breaks when needed, but donâ€™t give up. Return with fresh eyes and renewed determination.\n\n\n\nCultivate Curiosity\n\nApproach new concepts with an open and inquisitive mind.\nAsk questions, explore beyond the course material, and let your curiosity drive your learning.\nDigital humanities is a field of endless possibilitiesâ€”embrace the excitement of discovery.\n\n\n\nBuild Resilience\n\nExpect challenges and setbacksâ€”theyâ€™re not roadblocks, but opportunities to problem-solve.\nDevelop a growth mindset: view difficulties as chances to improve, not as personal shortcomings.\nRemember: every expert was once a beginner. Persistence is key to mastery.\n\n\n\n\n\n\n\nRemember\n\n\n\nI promise you can succeed in this class as long as you put your heart to it. Learning new digital skills may seem daunting at first, but with patience, persistence, and the right mindset, youâ€™ll be amazed at what you can achieve. Donâ€™t hesitate to reach out for help when you need itâ€”weâ€™re all in this learning journey together!"
  },
  {
    "objectID": "book/slides/s-preface.html#course-overview",
    "href": "book/slides/s-preface.html#course-overview",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\nğŸ’ Course design:\n\nWith a more pratical focus on DH methods using R\nLectures followed by practical sessions with R\nBring your laptop ğŸ’»\n\nğŸ  Course platform:\n\nğŸ’¬ Wechat: notifications + live chats\nğŸ›œ Course website: course materials/code scripts"
  },
  {
    "objectID": "book/slides/s-preface.html#why-focus-on-dh-methods",
    "href": "book/slides/s-preface.html#why-focus-on-dh-methods",
    "title": "Introduction to Digital Humanities",
    "section": "Why Focus on DH Methods?",
    "text": "Why Focus on DH Methods?\n\nğŸŒ‰ Common ground: Methods bridge diverse humanities disciplines\nğŸ”§ Transferable skills: Applicable across various research areas\nğŸš€ Future-proofing: Open the door to coding/programming to prepare for the AI age\nğŸ§  Critical thinking: Enhance analytical skills for all humanities\nğŸ’¡ Computational thinking: Develop a sense of data and computational thinking"
  },
  {
    "objectID": "book/slides/s-preface.html#course-overview-1",
    "href": "book/slides/s-preface.html#course-overview-1",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\nğŸ¯ Learning outcomes:\n\nUnderstand key DH concepts and methodologies\nDevelop basic programming skills in R\nApply digital methods to humanities research questions\nCritically evaluate DH projects and their implications"
  },
  {
    "objectID": "book/slides/s-preface.html#getting-ready",
    "href": "book/slides/s-preface.html#getting-ready",
    "title": "Introduction to Digital Humanities",
    "section": "Getting Ready",
    "text": "Getting Ready\n\nğŸ  Tour the website\nğŸ’¬ Create wechat group\nğŸ¤³ Join å¾®åŠ©æ•™ç­çº§"
  },
  {
    "objectID": "book/slides/s-preface.html#about-you",
    "href": "book/slides/s-preface.html#about-you",
    "title": "Introduction to Digital Humanities",
    "section": "About You",
    "text": "About You"
  },
  {
    "objectID": "book/index.html",
    "href": "book/index.html",
    "title": "Preface",
    "section": "",
    "text": "This is the textbook for the course Introduction to Digital Humanities taught by Meng Liu, at the School of English and International Studies, Beijing Foreign Studies University.\nIntroduction to Digital Humanities explores the intersection of humanities scholarship and digital methodologies. Students will learn how computational techniques can enhance humanities research, covering key concepts in digital humanities, modern R programming, and various analytical methods including quantitative text analysis, topic modeling, sentiment analysis, and social network analysis. Through hands-on application of digital tools to humanities data, students will develop both technical skills and critical thinking abilities. The course progresses from foundational programming to advanced analytical techniques, concluding with discussions on ethics and AI in humanities scholarship. By the end of the course, students will be equipped with a versatile toolkit for innovative, data-driven humanities research, suitable for those seeking to expand their methodological approach to cultural studies.\nThe content of the book is updated on a regular basis as the course progresses.",
    "crumbs": [
      "Book",
      "Preface"
    ]
  },
  {
    "objectID": "book/index.html#introduction-and-welcome-slides",
    "href": "book/index.html#introduction-and-welcome-slides",
    "title": "Preface",
    "section": "Introduction and Welcome Slides",
    "text": "Introduction and Welcome Slides",
    "crumbs": [
      "Book",
      "Preface"
    ]
  },
  {
    "objectID": "book/02-R.html",
    "href": "book/02-R.html",
    "title": "Â§2 Getting Started With R",
    "section": "",
    "text": "We will do all of your analysis with the open source (and free!) programming language R. We will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboardâ€”R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Book",
      "Â§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#r-and-rstudio",
    "href": "book/02-R.html#r-and-rstudio",
    "title": "Â§2 Getting Started With R",
    "section": "",
    "text": "We will do all of your analysis with the open source (and free!) programming language R. We will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboardâ€”R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Book",
      "Â§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#online-help",
    "href": "book/02-R.html#online-help",
    "title": "Â§2 Getting Started With R",
    "section": "2 Online Help",
    "text": "2 Online Help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if youâ€™ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and Posit Community (a forum specifically designed for people using RStudio and the tidyverse (i.e.Â you)).\nIf you use Mastodon or Twitter/X, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nChatGPT is generally relatively okay with R questions, but it may hallucinate responsesâ€”it sometimes imagine packages that donâ€™t exist and then give you answers with them. Think of it as glorified autocomplete. Iâ€™d encourage you to read the first few paragraphs here about a few important things to keep in mind when using ChatGPT, though.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for â€œr scatterplotâ€, but if it does struggle, try searching for â€œrstatsâ€ instead (e.g.Â â€œrstats scatterplotâ€). Also, since most of your R work will deal with {ggplot2}, itâ€™s often easier to just search for that instead of the letter â€œrâ€ (e.g.Â â€œggplot scatterplotâ€).",
    "crumbs": [
      "Book",
      "Â§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#installation-guide",
    "href": "book/02-R.html#installation-guide",
    "title": "Â§2 Getting Started With R",
    "section": "3 Installation Guide",
    "text": "3 Installation Guide\n\n3.1 Install R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network) website: https://cran.r-project.org/\nClick on â€œDownload R for XXXâ€, where XXX is either Mac or Windows: \n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, itâ€™s R-4.0.0.pkg; as of right now, the current version is 4.4.1) and download it.\n\n\n\nIf you use Windows, click â€œbaseâ€ (or click on the bolded â€œinstall R for the first timeâ€ link) and download it. \n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\n3.2 Install RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you wonâ€™t ever have to interact with it directly.\n\nGo to the free download location on RStudioâ€™s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it: \nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\n3.3 FAQs å¸¸è§å®‰è£…é—®é¢˜\n\næ‰“å¼€RStudioï¼Œå‡ºç°å¼¹çª—è®©æˆ‘é€‰æ‹©Rçš„ç‰ˆæœ¬ï¼ˆå¦‚ä¸‹å›¾ï¼‰:  è¿™è¯´æ˜ä½ çš„Rçš„å®‰è£…è·¯å¾„æ— æ³•è¢«RStudioè‡ªåŠ¨æ‰¾åˆ°ï¼Œéœ€è¦ç‚¹å‡»browseæŒ‰é’®æ‰‹åŠ¨é€‰æ‹©R.exeçš„ä½ç½®ï¼š è¯·æ‰¾åˆ°å¹¶ç‚¹å‡»Rå®‰è£…æ–‡ä»¶å¤¹(å¦‚æ–‡ä»¶åä¸ºR 4.4.1çš„æ–‡ä»¶å¤¹)ä¸­çš„binæ–‡ä»¶å¤¹ï¼Œå³å¯çœ‹åˆ°R.exeï¼Œé€‰ä¸­å³å¯ã€‚\næŠ¥é”™æ¶ˆæ¯ä¸­è¯´ç¼ºå°‘RTools: è¯·åˆ°RToolsç½‘ç«™ä¸‹è½½å¹¶å®‰è£…RTools https://cran.r-project.org/bin/windows/Rtools/\n\n\n\n3.4 Install tidyverse\nR packages are easy to install with RStudio. Select the packages panel, click on â€œInstall,â€ type the name of the package you want to install, and press enter. \nThis can sometimes be tedious when youâ€™re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including {ggplot2}) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on â€œInstall,â€ type â€œtidyverseâ€, and press enter. Youâ€™ll see a bunch of output in the RStudio console as all the tidyverse packages are installed. \nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n\n\n3.5 Install tinytex (Optional, this is for rendering PDF files)\nWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced â€œlay-tekâ€ or â€œlah-texâ€; for goofy nerdy reasons, the x is technically the â€œchâ€ sound in â€œBachâ€, but most people just say it as â€œkâ€â€”saying â€œlayteksâ€ is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but itâ€™s a huge programâ€”the macOS version, for instance, is nearly 4 GB! To make life easier, thereâ€™s an R package named {tinytex} that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHereâ€™s how to install {tinytex} so you can knit to pretty PDFs:\n\nUse the Packages in panel in RStudio to install {tinytex} like you did above with {tidyverse}. Alternatively, run install.packages(\"tinytex\") in the console.\nRun tinytex::install_tinytex() in the console.\nWait for a bit while R downloads and installs everything you need.\nThe end! You should now be able to knit to PDF.\n\nCredit: this installation guide was adpated from the guide by Dr Andrew Heiss at Georgia State University.",
    "crumbs": [
      "Book",
      "Â§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities",
    "href": "book/slides/s-intro.html#what-is-digital-humanities",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-1",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-1",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨\nWrite down three keywords that come to mind."
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-2",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-2",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨\nğŸ¤” What patterns do you see in the word cloud? Any suprising entries?"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-3",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-3",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-4",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-4",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-5",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-5",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-6",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-6",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-7",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-7",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-8",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-8",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?ğŸ¤¨",
    "text": "What is digital humanities?ğŸ¤¨\n\nâ€œDigital humanities work is done at the intersection of computational methods and humanities materials.â€\nâ€“ Johanna Drucker, 2021 ğŸ“š\n\n\nğŸ”€ Intersection of computation and humanities\nğŸ”— Combines traditional disciplines with digital tools\nğŸ¤ Collaborative and interdisciplinary\nğŸ”„ Iterative and public-facing\nğŸ”“ Open and accessible"
  },
  {
    "objectID": "book/slides/s-intro.html#components-of-dh-projects",
    "href": "book/slides/s-intro.html#components-of-dh-projects",
    "title": "Introduction to Digital Humanities",
    "section": "Components of DH Projects",
    "text": "Components of DH Projects\n\nğŸ“ MATERIALS\nâš™ï¸ PROCESSING\nğŸ¨ PRESENTATION"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-materials",
    "href": "book/slides/s-intro.html#dh-materials",
    "title": "Introduction to Digital Humanities",
    "section": "DH Materials",
    "text": "DH Materials\n\nğŸ’¾ Digital or digitized humanities sources\nğŸ–¼ï¸ Examples: texts, images, audio, video\nğŸ“ˆ Often large-scale datasets"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-processing",
    "href": "book/slides/s-intro.html#dh-processing",
    "title": "Introduction to Digital Humanities",
    "section": "DH Processing",
    "text": "DH Processing\n\nğŸ§® Computational methods and tools\nğŸ“Š Data analysis and modeling\nğŸ¤– Machine learning and AI applications"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-presentation",
    "href": "book/slides/s-intro.html#dh-presentation",
    "title": "Introduction to Digital Humanities",
    "section": "DH Presentation",
    "text": "DH Presentation\n\nğŸ“Š Visualizations\nğŸ—„ï¸ Digital archives\nğŸ–±ï¸ Interactive platforms\nğŸ—ƒï¸ Databases"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\n\nğŸ¿ Watch a video about the project"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-1",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-1",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nWhat it is\n\nğŸŒ One of the largest Digital Humanities initiatives worldwide\nğŸ›ï¸ Stanford-based project with international collaborations\nğŸš€ Groundbreaking effort in historical data visualization"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-2",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-2",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nWhat it does\n\nğŸ—ºï¸ Maps and analyzes intellectual networks of the 17th-18th century Enlightenment\nâœ‰ï¸ Visualizes over 200,000 letters exchanged by key historical figures\nğŸ” Reveals patterns of knowledge exchange across Europe and the Americas"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-3",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-3",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nCan you identify the core components of this DH project? ğŸ§ (quick learning check!)\n\nğŸ“ MATERIALS\nâš™ï¸ PROCESSING\nğŸ¨ PRESENTATION"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-4",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-4",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nCan you identify the core components of this DH project? ğŸ§\n\nğŸ“ MATERIALS: Letters and correspondence of Enlightenment thinkers\nâš™ï¸ PROCESSING: Network analysis, geospatial mapping\nğŸ¨ PRESENTATION: Interactive visualizations of intellectual networks"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-5",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-5",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nğŸ§ How is the project different from a traditional humanities project? Discuss in pairs to identify key differences between traditional humanities and DH."
  },
  {
    "objectID": "book/slides/s-intro.html#digital-humanities-vs-traditional-humanities",
    "href": "book/slides/s-intro.html#digital-humanities-vs-traditional-humanities",
    "title": "Introduction to Digital Humanities",
    "section": "Digital Humanities vs Traditional Humanities",
    "text": "Digital Humanities vs Traditional Humanities\n\n\nTraditional Humanities:\n\nğŸ” Limited by human reading capacity\nğŸ§‘â€ğŸ« Often individual research\nğŸ“š Primarily books and articles\nğŸ–‹ï¸ Close reading and interpretation\nğŸ›ï¸ Focus on canonical works\nğŸ—‚ï¸ Manual data collection and analysis\nğŸŒ Geographically limited research scope\nğŸ•°ï¸ Time-intensive archival research\nğŸ“Š Limited quantitative analysis\nğŸ­ Emphasis on qualitative methods\n\n\nDigital Humanities:\n\nğŸ“Š Handles large datasets and corpora\nğŸ‘¥ Often team-based, interdisciplinary projects\nğŸ–¥ï¸ Diverse digital outputs (visualizations, databases, etc.)\nğŸ” Distant reading and pattern recognition\nğŸ“œ Inclusion of non-canonical texts and artifacts\nğŸ¤– Automated data collection and processing\nğŸŒ Global reach through digital archives\nâš¡ Rapid processing of vast information\nğŸ“ˆ Advanced statistical and computational analysis\nğŸ”€ Integration of quantitative and qualitative methods"
  },
  {
    "objectID": "book/slides/s-intro.html#reflection",
    "href": "book/slides/s-intro.html#reflection",
    "title": "Introduction to Digital Humanities",
    "section": "Reflection",
    "text": "Reflection\nğŸ’¡ Does humanities need computational methods?\nğŸ§‘ Are we losing the â€œhumanâ€ in digital humanities?\nâš–ï¸ How might DH methods transform your area of study?\nDiscuss with a partner, then share with the class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Digital Humanities",
    "section": "",
    "text": "Introduction to Digital Humanitites æ•°å­—äººæ–‡å¯¼è®º â€¢ 2024 Fall ç§‹\nSEIS è‹±è¯­å­¦é™¢ â€¢ BFSU åŒ—äº¬å¤–å›½è¯­å¤§å­¦\n\n\n\n\n\nDr Meng Liu åˆ˜æ¢¦\n\nğŸ¢ Office: SEIS 202\nğŸ•°ï¸ Office Hours: Wednesday 3:20-4:20 pm | SEIS 202\nğŸ“§ Email: mengliu@bfsu.edu.cn\nğŸŒ Website: mengliu.info\n\n\n\n\n\nğŸ—“ï¸ Class Days/Times: Monday/Wednesday 13:30 - 15:05\nğŸ›ï¸ Format: In-person\nğŸ“ Location: é€¸å¤«äºŒé˜¶\n\n\n\n\n\nğŸ’¬ Wechat (group)\nğŸ“§ Email\n\n\n\n\n\n\n\nå¹³å°1ï¼šå¾®ä¿¡\n\nè¯¾å ‚äº’åŠ¨ï¼šå¾®åŠ©æ•™\nè¯¾åæ²Ÿé€šåŠæ¶ˆæ¯å‘å¸ƒï¼šå¾®ä¿¡ç¾¤\n\nå¹³å°2ï¼šè¯¾ç¨‹ç½‘ç«™\n\nè¯¾å ‚èµ„æ–™ä¿¡æ¯\n\n\n\n\n\n\nAnonymous Feedback Form"
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Introduction to Digital Humanities",
    "section": "",
    "text": "Introduction to Digital Humanitites æ•°å­—äººæ–‡å¯¼è®º â€¢ 2024 Fall ç§‹\nSEIS è‹±è¯­å­¦é™¢ â€¢ BFSU åŒ—äº¬å¤–å›½è¯­å¤§å­¦\n\n\n\n\n\nDr Meng Liu åˆ˜æ¢¦\n\nğŸ¢ Office: SEIS 202\nğŸ•°ï¸ Office Hours: Wednesday 3:20-4:20 pm | SEIS 202\nğŸ“§ Email: mengliu@bfsu.edu.cn\nğŸŒ Website: mengliu.info\n\n\n\n\n\nğŸ—“ï¸ Class Days/Times: Monday/Wednesday 13:30 - 15:05\nğŸ›ï¸ Format: In-person\nğŸ“ Location: é€¸å¤«äºŒé˜¶\n\n\n\n\n\nğŸ’¬ Wechat (group)\nğŸ“§ Email\n\n\n\n\n\n\n\nå¹³å°1ï¼šå¾®ä¿¡\n\nè¯¾å ‚äº’åŠ¨ï¼šå¾®åŠ©æ•™\nè¯¾åæ²Ÿé€šåŠæ¶ˆæ¯å‘å¸ƒï¼šå¾®ä¿¡ç¾¤\n\nå¹³å°2ï¼šè¯¾ç¨‹ç½‘ç«™\n\nè¯¾å ‚èµ„æ–™ä¿¡æ¯\n\n\n\n\n\n\nAnonymous Feedback Form"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\n\nğŸŒ± Foundations\n\nDigital Humanities Overview\n\n\n\nğŸ› ï¸ Core DH Techniques\n\nR Programming Basics\nQuantitative Text Analysis\n\n\n\nğŸš€ Specialized Topics\n\nTopic Modelling\nSentiment Analysis\nSocial Network Analysis"
  },
  {
    "objectID": "toolbox.html#poker-card-ballot-drawer",
    "href": "toolbox.html#poker-card-ballot-drawer",
    "title": "Toybox",
    "section": "ğŸƒ Poker Card Ballot Drawer",
    "text": "ğŸƒ Poker Card Ballot Drawer"
  },
  {
    "objectID": "toolbox.html#real-time-feedback-system",
    "href": "toolbox.html#real-time-feedback-system",
    "title": "Toybox",
    "section": "ğŸš¥ Real-Time Feedback System",
    "text": "ğŸš¥ Real-Time Feedback System\n\nReal-Time Personal Status âœ…â³ğŸ†˜\nThe floating traffic light ğŸš¥ in the bottom-right corner is your personal status indicator. Hereâ€™s how to use it:\n\nHover over the traffic light icon to expand it.\nEnter your name when using it for the first time. You only need to input it once. è¯·æŒ‰æ•°åŒ—é€‰è¯¾ç³»ç»Ÿçš„å§“åå¡«å†™\nClick on one of the three status icons to update your status:\n\nâœ… â€œGot it! ğŸ˜â€ - Youâ€™re understanding and progressing well.\nâ³ â€œHold onâ€¦ ğŸ¤”â€ - You need a moment to process or feel a little bit confused.\nğŸ†˜ â€œStuckâ€¦Help! ğŸ˜µâ€ğŸ’«â€ - Youâ€™re stuck and need assistance.\n\n\nRemember to update it when your status changes!\n\n\nReal-Time Class Status Overview ğŸ“Š\nThe floating chart ğŸ“Š in the bottom-left corner shows the overall class status:\n\nHover over the chart icon to expand it.\nIt displays the percentage of students in each status.\nThe bars and percentages update automatically every 10 seconds during class time.\nYou can see when the data was last updated at the bottom of the chart.\n\nThis tool helps all of us understand the overall class progress in real-time."
  },
  {
    "objectID": "book/01-introduction.html#digital-humanities-analysis-with-r",
    "href": "book/01-introduction.html#digital-humanities-analysis-with-r",
    "title": "Â§1 Introduction to Digital Humanities",
    "section": "\n2 Digital Humanities Analysis with R",
    "text": "2 Digital Humanities Analysis with R\nLetâ€™s explore some fascinating examples of how computational methods can be applied to humanities questions.\n\n2.1 Example 1: Text Analysis of Literary Works\n\nCodelibrary(pacman)\np_load(tidyverse, tidytext, wordcloud, gutenbergr, scales, hrbrthemes)\n\n# Function to download and process text\n# gutenberg_works(title == \"The Age of Innocence\")\ninno &lt;- gutenberg_download(541)\n\n# Analyze word frequencies\nword_freq &lt;- inno %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(20, n)\n\n# Create a bar plot of word frequencies\np &lt;- ggplot(word_freq, aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"#69b3a2\", width = 0.7) +\n  geom_text(aes(label = n), hjust = -0.3, size = 3) +\n  coord_flip() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(x = NULL, y = \"Frequency\", \n       title = \"Top 20 Most Frequent Words in The Age of Innocence\",\n       subtitle = \"After removing common stop words\",\n       caption = \"Source: Project Gutenberg\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 12, color = \"gray50\", margin = margin(b = 20)),\n    plot.caption = element_text(size = 10, color = \"gray50\", margin = margin(t = 10)),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(20, 20, 20, 20)\n  )\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\nThis word cloud visualizes the most frequent words in The Age of Innocence, giving us a quick insight into common themes and vocabulary.\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\n\nWhat words stand out to you in this visualization?\nHow might this kind of analysis complement traditional close reading of literary works?\nWhat limitations might this approach have for understanding the authorâ€™s language?\n\n\n\n\n\n2.2 Example 2: Sentiment Analysis of Jane Austenâ€™s Novels\n\nCodelibrary(janeaustenr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hrbrthemes)\n\n# Combine Austen's novels\nausten_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\", ignore_case = TRUE)))\n  ) %&gt;%\n  ungroup()\n\n# Perform sentiment analysis\nausten_sentiment &lt;- austen_books %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  inner_join(get_sentiments(\"bing\"), multiple=\"all\") %&gt;%\n  count(book, index = linenumber %/% 100, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\n# Plot sentiment over narrative time\nggplot(austen_sentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~book, ncol = 2, scales = \"free_x\") +\n  labs(\n    title = \"Sentiment Analysis of Jane Austen's Novels\",\n    subtitle = \"Emotional trajectory throughout the narrative\",\n    x = \"Narrative Time\",\n    y = \"Sentiment Score\",\n    caption = \"Data: janeaustenr package | Analysis: tidytext\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 12),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 14, color = \"gray50\"),\n    plot.caption = element_text(size = 10, color = \"gray50\"),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    plot.margin = margin(20, 20, 20, 20)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = expansion(mult = c(0.1, 0.1)))\n\n\n\n\n\n\nCode# Display summary statistics\nausten_summary &lt;- austen_sentiment %&gt;%\n  group_by(book) %&gt;%\n  summarize(\n    mean_sentiment = mean(sentiment),\n    max_sentiment = max(sentiment),\n    min_sentiment = min(sentiment)\n  )\n\nknitr::kable(austen_summary, caption = \"Summary Statistics of Sentiment Analysis\", digits = 2)\n\n\nSummary Statistics of Sentiment Analysis\n\nbook\nmean_sentiment\nmax_sentiment\nmin_sentiment\n\n\n\nSense & Sensibility\n9.94\n51\n-26\n\n\nPride & Prejudice\n10.69\n42\n-31\n\n\nMansfield Park\n12.47\n61\n-45\n\n\nEmma\n14.40\n48\n-31\n\n\nNorthanger Abbey\n9.19\n47\n-33\n\n\nPersuasion\n15.14\n59\n-13\n\n\n\n\n\nThis visualization shows the emotional trajectory of Jane Austenâ€™s novels over their narrative time.\n\n\n\n\n\n\nDiscussion: Narrative Emotions\n\n\n\n\n\n\nWhat patterns do you notice in the emotional arcs of Austenâ€™s novels?\nHow might this type of analysis enhance our understanding of narrative structure?\nWhat challenges might arise in applying sentiment analysis to historical texts?\n\n\n\n\n\n2.3 Example 3: Network Analysis of â€œEmpresses in the Palaceâ€ Characters\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n\n# Add Noto Sans CJK font\nfont_add_google(\"Noto Sans SC\", \"Noto Sans SC\")\nshowtext_auto()\n\n\n# Read the JSON data\nrelation_data &lt;- fromJSON(\"../book/data/relation.json\")\n\nrelation_data$nodes &lt;- relation_data$nodes %&gt;% rename(Bio=\"è§’è‰²æè¿°\")\n\nrelation_data$nodes &lt;- relation_data$nodes%&gt;%\nmutate(Alliance = ifelse(Alliance==\"çš‡åé˜µå®¹\",\"çš‡åé˜µè¥\",Alliance))\n\n# Ensure that the 'source' and 'target' in edges match the 'ID' in nodes\nrelation_data$edges$source &lt;- as.character(relation_data$edges$source)\nrelation_data$edges$target &lt;- as.character(relation_data$edges$target)\nrelation_data$nodes$ID &lt;- as.character(relation_data$nodes$ID)\nedges &lt;- relation_data$edges %&gt;% select(-Relationship)\n# Set the ID as the row names for the nodes dataframe\nrownames(relation_data$nodes) &lt;- relation_data$nodes$ID\n\n# Create the directed graph\nempresses_graph &lt;- graph_from_data_frame(d = edges,\n                                         vertices = relation_data$nodes$ID,\n                                         directed = TRUE)\n\n# Set node attributes\nV(empresses_graph)$Alliance &lt;- relation_data$nodes$Alliance\nV(empresses_graph)$Label &lt;- relation_data$nodes$Label\n\n# Create a color palette for alliances\nalliance_colors &lt;- c(\n  \"çš‡å®¤æˆå‘˜\" = \"#4E79A7\",  # Royal Blue\n  \"çš‡åé˜µè¥\" = \"#F28E2B\",  # Warm Orange\n  \"ç”„å¬›é˜µè¥\" = \"#E15759\",  # Soft Red\n  \"åå¦ƒé˜µè¥\" = \"#76B7B2\"   # Teal\n)\n\n# Calculate node size based on total degree centrality (in + out)\nV(empresses_graph)$size &lt;- degree(empresses_graph, mode = \"total\") * 0.5 + 3\nE(empresses_graph)$Relationship &lt;- relation_data$edges$Relationship\n\n# Plot the network\nset.seed(123) # for reproducibility\nplot &lt;- ggraph(empresses_graph, layout = \"fr\") +\n  geom_edge_link0(aes(edge_color = Relationship), \n                  arrow = arrow(length = unit(0.2, \"inches\"),\n                  ends = \"last\", type = \"closed\"), \n                  show.legend = FALSE, \n                  width = 1) +\n  geom_node_point(aes(color = Alliance, size = size), \n                  shape = 20, show.legend = FALSE) +\n  geom_node_text(aes(label = Label), repel = TRUE, size = 6) +\n  scale_color_manual(values = alliance_colors) +\n  scale_edge_colour_discrete() +\n  scale_size_continuous(range = c(5, 20)) +\n  theme_void() +\n  labs(title = \"Character Network in 'Empresses in the Palace'\",\n       subtitle = \"Directed relationships between key figures in the Chinese drama\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 14),\n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 10, face = \"bold\")\n  )\n\n# Plot\nplot\n\n\n\n\n\n\n\nThis visualization shows the complex network of relationships between characters in the Chinese drama â€œEmpresses in the Palaceâ€ (ç”„å¬›ä¼ ).\n\n\n\n\n\n\nDiscussion: Character Networks in Historical Dramas\n\n\n\n\n\n\nWhat insights can we gain from this network visualization? What information is presented in the plot?\nWhich characters appear to be central to the network? How might their positions reflect their importance in the narrative?\nHow could this type of analysis complement traditional literary analysis of historical dramas or novels?\nWhat limitations might this network analysis have in representing the complex relationships and dynamics of the story?\n\n\n\n\nLetâ€™s verify our impressions!\n\nView Analyses\n\n\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n# Create dummy data for legends\nalliance_data &lt;- data.frame(\n  Alliance = levels(factor(V(empresses_graph)$Alliance)),\n  x = 1,\n  y = 1:length(levels(factor(V(empresses_graph)$Alliance)))\n)\n\nsize_data &lt;- data.frame(\n  size = c(5, 10, 15, 20),\n  x = 1,\n  y = 1:4\n)\n\nrelationship_data &lt;- data.frame(\n  Relationship = levels(factor(E(empresses_graph)$Relationship)),\n  x = 1,\n  y = 1:length(levels(factor(E(empresses_graph)$Relationship)))\n)\n\n# Create separate legends\nnode_legend &lt;- cowplot::get_legend(\n  ggplot() +\n    geom_point(data = alliance_data, aes(x, y, color = Alliance), size = 5) +\n    geom_point(data = size_data, aes(x, y, size = size), color = \"black\") +\n    scale_color_manual(values = alliance_colors, name = \"Alliance\") +\n    scale_size_continuous(range = c(5, 20), name = \"Connections\") +\n    guides(\n      color = guide_legend(override.aes = list(size = 5)),\n      size = guide_legend(override.aes = list(color = \"steelblue\"))\n    ) +\n    theme(\n      legend.background = element_blank(),\n      legend.key = element_blank(),\n      legend.spacing.y = unit(0.5, \"cm\")\n    )\n)\n\nedge_legend &lt;- cowplot::get_legend(\n  ggplot(relationship_data, aes(x, y, color = Relationship)) +\n    geom_segment(aes(x = 0, xend = 1, yend = y),\n                 arrow = arrow(length = unit(0.2, \"inches\"), type = \"closed\")) +\n    scale_color_discrete(name = \"Relationship\") +\n    theme(\n      legend.background = element_blank(),\n      legend.key = element_blank(),\n      legend.spacing.y = unit(0.2, \"cm\"),\n      legend.text = element_text(margin = margin(r = 15))\n    )\n)\n\n# Combine plot and legends\ncombined_legend &lt;- plot_grid(\n  node_legend, \n  edge_legend, \n  ncol = 1, \n  rel_heights = c(1, 1.5),\n  align = 'v',\n  axis = 'l'\n)\n\n# Combine main plot and legends\nfinal_plot &lt;- plot_grid(\n  plot, \n  combined_legend,\n  rel_widths = c(5, 2),\n  align = 'h',\n  axis = 'tb'\n)\n\n# Display the final plot\nfinal_plot\n\n\n\n\n\n\n\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n\n# Display summary statistics\ncharacter_summary &lt;- tibble(\n  Alliance = names(table(V(empresses_graph)$Alliance)),\n  Count = as.numeric(table(V(empresses_graph)$Alliance))\n)%&gt;%\n  arrange(desc(Count)) \n\nknitr::kable(character_summary, caption = \"Summary of Character Alliances\")\n\n\nSummary of Character Alliances\n\nAlliance\nCount\n\n\n\nç”„å¬›é˜µè¥\n22\n\n\nçš‡åé˜µè¥\n12\n\n\nåå¦ƒé˜µè¥\n6\n\n\nçš‡å®¤æˆå‘˜\n4\n\n\n\n\nCode# Calculate and display top 5 characters by total degree centrality\ntop_characters &lt;- tibble(\n  Character = V(empresses_graph)$Label,\n  InDegree = degree(empresses_graph, mode = \"in\"),\n  OutDegree = degree(empresses_graph, mode = \"out\"),\n  TotalDegree = degree(empresses_graph, mode = \"total\")\n) %&gt;%\n  arrange(desc(TotalDegree)) %&gt;%\n  left_join(.,relation_data$nodes%&gt;%select(Label,Title,Alliance,Bio),by=(c(\"Character\"=\"Label\")))%&gt;%\n  slice_head(n = 5)\n\nknitr::kable(top_characters, caption = \"Top 5 Characters by Total Connections\")\n\n\nTop 5 Characters by Total Connections\n\n\n\n\n\n\n\n\n\n\nCharacter\nInDegree\nOutDegree\nTotalDegree\nTitle\nAlliance\nBio\n\n\n\nç”„å¬›\n16\n22\n38\nçš‡è´µå¦ƒ\nç”„å¬›é˜µè¥\nå› å®¹è²Œé…·ä¼¼å·²é€çš„çº¯å…ƒçš‡åè¢«é€‰ä¸­ï¼Œä¸€ç›´è’™å—æ©å® ã€‚å¯¹çš‡å¸çœŸå¿ƒå®æ„ã€‚å› ä¸ºåœ¨åå®«ç®—è®¡ä¸­å—åˆ°å½±å“ï¼Œä¸€æ­¥æ­¥å˜å¾—åŸåºœè‡³æ·±ï¼Œç»“æœå´æ˜¯å¥¹çš„ä¸€å¢æƒ…æ„¿ï¼Œç¦»å¼€åå®«ï¼Œé‡åˆ°äº†æœéƒ¡ç‹ï¼Œå¿ƒå¿ƒç›¸çˆ±ã€‚åæ¥è®¾è®¡å›å®«ã€‚ç»å†äº†åå®«æ— æ•°çš„æ˜äº‰æš—æ–—ï¼Œå‹¾å¿ƒæ–—è§’ï¼Œå®³æ­»çš‡å¸ï¼Œæ‰¶å…»å­ç™»ä½ï¼Œç»ˆäºæˆä¸ºå¤ªåã€‚\n\n\né›æ­£\n13\n18\n31\nçš‡ä¸Š\nçš‡å®¤æˆå‘˜\nä¹é¾™å¤ºå«¡ä¹‹åç»ˆäºç™»ä¸Šäº†å®åº§ã€‚åœ¨ä»–çš„çœ¼ä¸­ä»»ä½•äººéƒ½å¯ä»¥æˆä¸ºä»–å·©å›ºæ±Ÿå±±çš„æ£‹å­ã€‚çº¯å…ƒçš‡åå»ä¸–å¤šå¹´ï¼Œç—´æƒ…çš„çš‡å¸å¯¹å¥¹ä¾ç„¶å¿µå¿µä¸å¿˜ï¼Œäºæ˜¯å¯¹é…·ä¼¼çº¯å…ƒçš‡åçš„ç”„å¬›æ„ˆåŠ å® çˆ±ï¼Œæ¸æ¸è¶…è¿‡äº†åå¦ƒã€‚ç”„å¬›å›å®«ï¼Œçš‡ä¸Šå¯¹å…¶æå…¶ä¿¡ä»»ï¼Œç™¾ä¾ç™¾é¡ºã€‚ä½†æœ€åè¿˜æ˜¯è¢«æ•è¾¹äººç®—è®¡ã€‚\n\n\nå®œä¿®\n8\n11\n19\nçš‡å\nçš‡åé˜µè¥\nçº¯å…ƒçš‡åçš„åº¶å‡ºå¦¹å¦¹ã€‚å¤–è¡¨ç¨³é‡ç«¯åº„ï¼Œä½†æ“…äºéšå¿ï¼ŒåŸåºœä¹‹æ·±ï¼Œæçˆ±åä½ï¼Œåå®«å¦ƒå­çš†ä¸ºå¥¹çš„æ£‹å­ã€‚åˆ©ç”¨ç”„å¬›æ‰“è´¥äº†åå¦ƒï¼Œç»§è€Œå°†ç”„å¬›èµ¶å‡ºå®«å¤–ã€‚ç”„å¬›å›å®«åç»§ç»­æ­¥æ­¥ä¸ºè¥ï¼Œä½†æœ€åå› å®³æ­»çº¯å…ƒçš‡åçš„äº‹æƒ…ä¸œçª—äº‹å‘ï¼Œåœ¨å¤ªåçš„åŠ›ä¿ä¹‹ä¸‹æ²¡è¢«åºŸåï¼Œä½†æ°¸ç”Ÿç¦è¶³äºæ™¯ä»å®«ï¼ŒäºŒäººæ­»ç”Ÿä¸å¤ç›¸è§ï¼Œæœ€åæ–°å¸ç™»åŸºï¼Œå¿ƒæ‚¸è€Œæ­»ã€‚\n\n\nå…ç¤¼\n7\n7\n14\næœäº²ç‹\nçš‡å®¤æˆå‘˜\næœéƒ¡ç‹ä¸åŒäºå®«ä¸­äº‰æƒå¤ºåŠ¿çš„çš‡å®¤å­å¼Ÿï¼Œä»–åº†å¹¸è‡ªå·±å¹¶éå¸ç‹ä¹‹èº«ï¼Œä¸å¿…å°†æœæ”¿çäº‹è¦ç»•äºèº«ï¼Œä»–ä¸æ±‚å¨‡å¦»ç¾å¦¾å¦‚äº‘ï¼Œåªç›¼ä¸çˆ±äººå®å®ˆåˆ°è€ã€‚æœéƒ¡ç‹çˆ±ç€ç”„å¬›ï¼Œå´åˆä¸èƒ½åƒ­è¶Šå®«å»·ä¹‹ç¤¼ã€‚ä»–å–„è‰¯ã€ç—´æƒ…ã€é£æµå€œå‚¥ï¼Œç»ˆå…¶ä¸€ç”Ÿéƒ½æ˜¯åœ¨å¯»æ±‚ä¸€ä»½çˆ±ï¼Œå¥‹ä¸é¡¾èº«åœ°ä¿æŠ¤ä¸€ä»½çˆ±ï¼Œè¯´ä»–æ˜¯ä¸ºçˆ±è€Œç”Ÿä¹Ÿä¸ä¸ºè¿‡ã€‚\n\n\nå¹´ä¸–å…°\n6\n7\n13\næ­£ä¸‰å“è´µå«”\nåå¦ƒé˜µè¥\nå¤§å°†å†›çš„å¦¹å¦¹ï¼Œä»—ç€èº«ä¸–å’Œå® çˆ±ï¼Œå¯¹å…¶ä»–çš„å¦ƒå«”ä¸æ‹©æ‰‹æ®µï¼Œä½†æœ¬æ€§å¹¶ä¸åï¼Œå› ä¸ºå­©å­è¢«æ‰“æ‰è€Œç—›æ¨ç«¯å¦ƒã€‚ç”„å¬›åœ¨å…¶æ‰“å…¥å†·å®«åè¿˜ä¸èƒ½æ€€å­•çš„äº‹å®å‘Šè¯‰å¥¹ï¼Œç»æœ›çš„åå¦ƒæ’å¢™è€Œæ­»ã€‚æœ€ç»ˆå¥¹è¢«è¿½å°ä¸ºæ•¦è‚ƒçš‡è´µå¦ƒã€‚",
    "crumbs": [
      "Book",
      "Â§1 Introduction to Digital Humanities"
    ]
  },
  {
    "objectID": "book/01-introduction.html#conclusion",
    "href": "book/01-introduction.html#conclusion",
    "title": "Â§1 Introduction to Digital Humanities",
    "section": "\n3 Conclusion",
    "text": "3 Conclusion\nThese demonstrations showcase just a few of the exciting possibilities that digital humanities offers for exploring and analyzing humanities data. By combining computational methods with traditional humanities scholarship, we can uncover new patterns, ask novel questions, and gain fresh insights into cultural and historical materials.\n\n\n\n\n\n\nFinal Reflection\n\n\n\n\n\nConsider the examples weâ€™ve explored today:\n\nWhich technique (word frequency analysis, sentiment analysis, or network analysis) do you find most intriguing? Why?\nCan you think of a humanities question or topic from your own interests that might benefit from one of these computational approaches?\n\nShare your thoughts with a partner or the class if time allows.",
    "crumbs": [
      "Book",
      "Â§1 Introduction to Digital Humanities"
    ]
  },
  {
    "objectID": "book/04-cards.html",
    "href": "book/04-cards.html",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "",
    "text": "Welcome to your second project with R! Weâ€™ll create a virtual deck of poker cards to explore more advanced R concepts.",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#numeric",
    "href": "book/04-cards.html#numeric",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.1 Numeric",
    "text": "1.1 Numeric\nWeâ€™ve already seen this with our dice. It includes both integers and decimal numbers:\n\nmy_integer &lt;- 42\nmy_deci &lt;- 3.14\nclass(my_integer)\n\n[1] \"numeric\"\n\nclass(my_deci)\n\n[1] \"numeric\"",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#character",
    "href": "book/04-cards.html#character",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.2 Character",
    "text": "1.2 Character\nThis is used for text data, like words or sentences:\n\nmy_name &lt;- \"Shakespeare\"\nmy_quote &lt;- \"To be or not to be\"\nclass(my_name)\n\n[1] \"character\"\n\nclass(my_quote)\n\n[1] \"character\"",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#logical",
    "href": "book/04-cards.html#logical",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.3 Logical",
    "text": "1.3 Logical\nThis represents true/false values:\n\nis_sunny &lt;- TRUE\nis_raining &lt;- FALSE\nclass(is_sunny)\n\n[1] \"logical\"\n\nclass(is_raining)\n\n[1] \"logical\"",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#factor",
    "href": "book/04-cards.html#factor",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.4 Factor",
    "text": "1.4 Factor\nThis is used for categorical data, like genres in literature:\n\ngenres &lt;- factor(c(\"Poetry\", \"Prose\", \"Drama\"))\nclass(genres)\n\n[1] \"factor\"\n\n\n\n\n\n\n\n\nWhat is the Difference Between Factor and Character?\n\n\n\n\n\nCharacter and factor may seem similar, but they serve different purposes:\n\n\nCharacter\n\nThis is simply text data.\nIt can be any combination of letters, numbers, or symbols.\nExamples: \"apple\", \"banana\", \"cherry123\"\n\n\n\n\nFactor\n\nThis is for categorical data, made up of distinct levels or categories.\nItâ€™s used when data falls into a limited number of specific groups.\nExamples: Days of the week, book genres, or rating scales\n\n\n\nKey differences:\n\nCharacters can be any text, while factors consist of predefined levels or categories.\nEach level in a factor is assigned a unique integer behind the scenes, making factors more efficient for certain analyses.\nFactors are ideal for data with a fixed set of possible values or categories.",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#learning-check",
    "href": "book/04-cards.html#learning-check",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.5 Learning Check ğŸ",
    "text": "1.5 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#hands-on-coding",
    "href": "book/04-cards.html#hands-on-coding",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n1.6 Hands-On Coding ğŸ’»",
    "text": "1.6 Hands-On Coding ğŸ’»\nTry the following:\n\nCreate a numeric variable called year_published and assign it the year your favorite book was published.\nCreate a character variable called book_title with the title of your favorite book.\nCreate a logical variable called is_fiction indicating whether your favorite book is fiction (TRUE) or non-fiction (FALSE).\nCreate a factor variable called book_genre with a few genres (e.g., â€œMysteryâ€, â€œRomanceâ€, â€œScience Fictionâ€).\n\n\n\n# Year published\nyear_published &lt;- 1960\n# Book title\nbook_title &lt;- \"To Kill a Mockingbird\"\n# Fiction or non-fiction\nis_fiction &lt;- TRUE\n# Book genre\nbook_genre &lt;- factor(c(\"Mystery\", \"Romance\", \"Science Fiction\"))",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#accessing-data-in-a-data-frame",
    "href": "book/04-cards.html#accessing-data-in-a-data-frame",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n2.1 Accessing Data in a Data Frame",
    "text": "2.1 Accessing Data in a Data Frame\nYou can access specific parts of a data frame using various methods.\n\n\n\n\n\n\n$ and [row_number, column_number]\n\n\n\n\n\n\n\nColumns: Use $ followed by the column name (e.g., books$title).\n\nThis gives you all the values in that column.\n\n\n\nRows: Use [row_number, ] (e.g., books[2, ]).\n\nThe comma is important! It means â€œall columnsâ€.\nThis gives you all the data in that specific row.\n\n\n\nIndividual Cells: Use [row_number, column_number] (e.g., books[1, 2]).\n\nThis gives you the value in a specific row and column.\n\n\n\n\n\n\n\n\n\n\n\n\nAcessing the books Data Frame\n\n\n\n\n\nData frames are essentially just spread sheet tables. Our books data frame looks like this:\n\n\ntitle\nauthor\nyear\n\n\n\n1984\nOrwell\n1949\n\n\nPride and Prejudice\nAusten\n1813\n\n\nThe Great Gatsby\nFitzgerald\n1925\n\n\n\n\n\nbooks$title would give: 1984, Pride and Prejudice, The Great Gatsby\n\n\nbooks[2, ] would give: Pride and Prejudice, Austen, 1813\n\n\nbooks[1, 2] would give: Orwell\n\n\n\n\n\n\n# Get a specific column\nbooks$title\n\n[1] \"1984\"                \"Pride and Prejudice\" \"The Great Gatsby\"   \n\n# Get a specific row\nbooks[2, ]\n\n                title author year\n2 Pride and Prejudice Austen 1813\n\n# Get a specific cell\nbooks[1, 2]\n\n[1] \"Orwell\"\n\n# Get a specific cell using row number and column name\nbooks[2, \"title\"] # Get the second title\n\n[1] \"Pride and Prejudice\"\n\n# Get the record based on a specific value\nbooks[books$author == \"Austen\", ] # Get all records by Austen\n\n                title author year\n2 Pride and Prejudice Austen 1813\n\n\n\n\n\n\n\n\nUnderstanding Nested Functions\n\n\n\n\n\nThe line books[books$author == \"Austen\", ] might look complex, but letâ€™s break it down:\n\n\nbooks$author: This part gets the author column from our books data frame.\n\nbooks$author == \"Austen\": This compares each author name to â€œAustenâ€.\n\n\nIt creates a list of TRUE/FALSE values (TRUE where the author is Austen, FALSE otherwise).\nFor example, if we had 3 books and only the second was by Austen, this would give: [FALSE, TRUE, FALSE]\n\n\n\nbooks[...]: This is like asking R to look inside the books data frame.\n\nbooks[books$author == \"Austen\", ]: This combines steps 2 and 3.\n\n\nIt tells R: â€œFrom the books data frame, give me all rows where the author is Austenâ€.\nThe comma at the end means â€œgive me all columns for these rowsâ€.\n\nThink of it like a librarian (R) searching through a catalog (the data frame):\n\nYou ask: â€œCan you find all books by Austen?â€\nThe librarian checks each bookâ€™s author (TRUE/FALSE for Austen).\nThen returns all information about the books that matched.\n\n\n\n\n\n\n\n\n\n\nFiltering Data Based on Conditions\n\n\n\n\n\nThis example demonstrates a powerful technique called â€œfilteringâ€:\n\nCreating a condition: books$author == \"Austen\" creates a logical vector (TRUE/FALSE) for each row where the author is Austen.\nUsing the condition: We use this vector inside the square brackets [ ] to select only those rows that meet our condition.\nSelecting columns: The comma after the condition (books [condition, ]) means â€œselect all columns for these rowsâ€.\n\nThis method allows you to extract specific records based on any condition you specify. For example:\n\nFind all books published after 1900: books[books$year &gt; 1900, ]\nFind all books with â€œWarâ€ in the title: books[grepl(â€œWarâ€, books$title), ]\n\nFiltering is a crucial skill in data manipulation and analysis, allowing you to focus on specific subsets of your data.",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#learning-check-1",
    "href": "book/04-cards.html#learning-check-1",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n2.2 Learning Check ğŸ",
    "text": "2.2 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#hands-on-coding-1",
    "href": "book/04-cards.html#hands-on-coding-1",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n2.3 Hands-On Coding ğŸ’»",
    "text": "2.3 Hands-On Coding ğŸ’»\nLetâ€™s apply our data frame skills to a new context: famous paintings. Weâ€™ll create a data frame of artworks and practice accessing its data in various ways.\nTry the following exercises:\n\nCreate a data frame called paintings with columns for title, artist, and year_created.\n\n\n\n\n\n\n\nExample Data\n\n\n\n\n\nFeel free to use your own favorite paintings for this exercise! If you prefer, you can use the following code:\n\npaintings &lt;- data.frame(\n    title = c(\"Mona Lisa\", \"The Starry Night\", \"The Persistence of Memory\"),\n    artist = c(\"Leonardo da Vinci\", \"Vincent van Gogh\", \"Salvador Dali\"),\n    year_created = c(1503, 1889, 1931)\n)\n\n\n\n\n\nAccess the â€˜artistâ€™ column of the data frame.\nGet the third row of the data frame.\nFind the year the second painting in the data frame was created.\n(Optional challenge) Add a new column called â€˜styleâ€™ to the data frame (e.g., â€œRenaissanceâ€, â€œPost-Impressionismâ€, â€œSurrealismâ€).\n\n\n\n# 1. Create the paintings data frame\npaintings &lt;- data.frame(\n    title = c(\"Mona Lisa\", \"The Starry Night\", \"The Persistence of Memory\"),\n    artist = c(\"Leonardo da Vinci\", \"Vincent van Gogh\", \"Salvador Dali\"),\n    year_created = c(1503, 1889, 1931)\n)\n# 2. Access the 'artist' column\npaintings$artist\n\n[1] \"Leonardo da Vinci\" \"Vincent van Gogh\"  \"Salvador Dali\"    \n\n# 3. Get the third row\npaintings[3, ]\n\n                      title        artist year_created\n3 The Persistence of Memory Salvador Dali         1931\n\n# 4. Find the year the second painting was created\npaintings[2, \"year_created\"]\n\n[1] 1889\n\n# 5. Add a new 'style' column\npaintings$style &lt;- c(\"Renaissance\", \"Post-Impressionism\", \"Surrealism\")\n# View the updated data frame\nprint(paintings)\n\n                      title            artist year_created              style\n1                 Mona Lisa Leonardo da Vinci         1503        Renaissance\n2          The Starry Night  Vincent van Gogh         1889 Post-Impressionism\n3 The Persistence of Memory     Salvador Dali         1931         Surrealism",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#create-the-deck",
    "href": "book/04-cards.html#create-the-deck",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n3.1 Create the Deck",
    "text": "3.1 Create the Deck\n\n\n\n\n\n\nStructure of a Deck\n\n\n\n\n\nRemember, a standard deck of cards consists of:\n\n4 suits: Heartsâ™¥ï¸, Diamondsâ™¦ï¸, Clubsâ™£ï¸, and Spadesâ™ ï¸\n13 ranks in each suit: Ace, 2, 3, â€¦, 10, Jack, Queen, King\nA total of 52 cards (4 suits Ã— 13 ranks)\n\n\n\n\nCreate a data frame called deck with three columns: suit, rank, and value.\n\n\nsuit should contain all four suits, repeated 13 times each.\n\nrank should contain all 13 ranks, repeated for each suit.\n\nvalue should assign numeric values to the ranks (Ace = 1, Jack = 11, Queen = 12, King = 13, others as their numeric value).\n\n\n\n\n\n\n\n\nHint 1/2\n\n\n\n\n\nUse the rep() function to repeat values. For example: - rep(c(\"A\", \"B\"), each = 3) gives \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" - rep(c(\"A\", \"B\"), times = 3) gives \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\n\n\n\n\n\n\n\n\n\nHint 2/2\n\n\n\n\n\nFor the suit column: rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = 13) For the rank column: rep(c(\"Ace\", 2:10, \"Jack\", \"Queen\", \"King\"), times = 4) For the value column: rep(1:13, times = 4)\n\n\n\n\n\n\ndeck &lt;- data.frame(\n    suit = rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = _),\n    rank = rep(c(\"Ace\", _, \"Jack\", \"Queen\", \"King\"), times = _),\n    value = rep(_, times = 4)\n)\n\n\n\n\ndeck &lt;- data.frame(\nsuit = rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = 13),\nrank = rep(c(\"Ace\", 2:10, \"Jack\", \"Queen\", \"King\"), times = 4),\nvalue = rep(1:13, times = 4)\n)\n# View the first few rows\nhead(deck)\n\n    suit rank value\n1 Hearts  Ace     1\n2 Hearts    2     2\n3 Hearts    3     3\n4 Hearts    4     4\n5 Hearts    5     5\n6 Hearts    6     6",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/04-cards.html#accessing-information",
    "href": "book/04-cards.html#accessing-information",
    "title": "Â§4 Dealing With Cards ğŸƒ",
    "section": "\n3.2 Accessing Information",
    "text": "3.2 Accessing Information\nNow that we have our deck, letâ€™s practice accessing information from it.\n\n3.2.1 How many cards are in the deck?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse nrow() to count the number of rows in a data frame.\n\n\n\n\n\n\nnrow(_)\n\n\n\n\nnrow(deck)\n\n[1] 52\n\n\n\n\n3.2.2 What are all the unique suits in the deck?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the unique() function on the suit column of the deck.\n\n\n\n\n\n\nunique(deck$_)\n\n\n\n\nunique(deck$suit)\n\n[1] \"Hearts\"   \"Diamonds\" \"Clubs\"    \"Spades\"  \n\n\n\n\n3.2.3 View high value cards\nView only the cards with a value greater than 10.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse logical indexing to filter the deck. The condition should be deck$value &gt; 10.\n\n\n\n\n\n\nhigh_value_cards &lt;- deck[deck$_ &gt; _, ]\nprint(high_value_cards)\n\n\n\n\nhigh_value_cards &lt;- deck[deck$value &gt; 10, ]\nprint(high_value_cards)\n\n       suit  rank value\n11   Hearts  Jack    11\n12   Hearts Queen    12\n13   Hearts  King    13\n24 Diamonds  Jack    11\n25 Diamonds Queen    12\n26 Diamonds  King    13\n37    Clubs  Jack    11\n38    Clubs Queen    12\n39    Clubs  King    13\n50   Spades  Jack    11\n51   Spades Queen    12\n52   Spades  King    13",
    "crumbs": [
      "Book",
      "Â§4 Dealing With Cards ğŸƒ"
    ]
  },
  {
    "objectID": "book/06-qta.html#other-preprocessing-operations",
    "href": "book/06-qta.html#other-preprocessing-operations",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.3 Other Preprocessing Operations",
    "text": "2.3 Other Preprocessing Operations\n\n2.3.1 Removing Numbers\nRemoving numbers can be useful when focusing on textual content rather than numerical data. Below we use some made-up examples to more easily showcase these operations.\n\n# Example of removing numbers\nstr_replace_all(\"Hello, world! 123\", \"\\\\d+\", \"\")\n\n[1] \"Hello, world! \"\n\n\n\n\n\n\n\n\nRegex Breakdown\n\n\n\n\n\n\n\n\"\\\\d+\": This pattern matches one or more digits.\n\n\n\\\\d represents any digit (equivalent to [0-9])\n\n+ means â€œone or moreâ€ of the preceding element\n\n\n\n\n\n\n\n2.3.2 Removing Whitespace\nSometimes the text may contain extra whitespace that is unwanted, which may impact the matching of patterns if not handled properly. Two functions are handy for removing whitespace: str_trim() and str_squish(). Trimming removes leading and trailing whitespace, while squishing replaces multiple spaces with a single space.\n\n# Example of str_trim()\nstr_trim(\"  Hello, world!  \")\n\n[1] \"Hello, world!\"\n\n# Example of str_squish()\nstr_squish(\"Hello,   world!  How   are  you?\")\n\n[1] \"Hello, world! How are you?\"\n\n# Combining str_trim() and str_squish()\nstr_squish(str_trim(\"  Hello,   world!  How   are  you?  \"))\n\n[1] \"Hello, world! How are you?\"\n\n\n\n2.3.3 Removing Special Characters\nRemoving special characters can help standardize text, especially when dealing with different character encodings. This may be particularly relevant when dealing with social media data.\n\n\n\n\n\n\nCharacter Encodings\n\n\n\n\n\nCharacter encoding is a method used to represent characters in digital form. Itâ€™s crucial to understand character encodings when working with text data, especially in multilingual or international contexts:\n\nDefinition: Character encoding assigns a unique number to every character in a writing system, allowing computers to store, process, and display text.\n\nCommon Encodings:\n\nASCII: Represents basic Latin characters and some control characters.\nUTF-8: A variable-width encoding that can represent all Unicode characters.\nISO-8859-1 (Latin-1): Covers most Western European languages.\n\n\nUnicode: A universal character encoding standard that aims to represent all writing systems in the world.\n\nImportance in Text Analysis:\n\nIncorrect encoding can lead to garbled text or misinterpretation of characters.\nDifferent languages and scripts may require specific encodings.\nConsistent encoding is crucial for accurate text processing and analysis.\n\n\n\nHandling in R:\n\nR generally uses UTF-8 encoding by default.\nFunctions like Encoding() and iconv() can help manage different encodings.\n\n\n\nBest Practices:\n\nAlways specify the encoding when reading or writing text files.\nUse UTF-8 when possible for its universal compatibility.\nBe aware of potential encoding issues when working with text from diverse sources.\n\n\n\nUnderstanding character encodings is essential for ensuring accurate representation and analysis of text data, especially in multilingual or cross-cultural digital humanities projects.\n\n\n\n\n# Example of removing special characters\nstr_replace_all(\"Hello, world! 123 @#$ Ã± â‚¬ ÃŸ\", \"[^[:alnum:][:space:]]\", \"\")\n\n[1] \"Hello world 123  Ã±  ÃŸ\"\n\n\n\n\n\n\n\n\nRegex Breakdown\n\n\n\n\n\nThe regex pattern [^[:alnum:][:space:]] is used to match any character that is not alphanumeric or whitespace. Letâ€™s break it down:\n\n[]: Defines a character set. It matches any single character inside the brackets.\n^: When used as the first character inside [], it negates the set. It means â€œmatch any character that is NOT in this setâ€.\n[:alnum:]: POSIX character class for all alphanumeric characters.\n[:space:]: POSIX character class for all whitespace characters.\n\nSo, [^[:alnum:][:space:]] means: - Match any character that is neither alphanumeric nor whitespace.\nThis effectively matches all special characters, punctuation, and non-ASCII characters, which are then replaced with an empty string, removing them from the text.\n\n\n\nThese preprocessing steps can be combined for a more comprehensive text cleaning:\n\n# Combining all preprocessing steps\nfully_preprocessed &lt;- pride_and_prejudice %&gt;%\n  mutate(\n    text = tolower(text),\n    text = str_replace_all(text, \"[[:punct:]]\", \"\"),\n    text = str_replace_all(text, \"\\\\d+\", \"\"),\n    text = str_trim(text),\n    text = str_squish(text),\n    text = str_replace_all(text, \"[^[:alnum:][:space:]]\", \"\")\n  )\n\n# Display a few rows to see the combined effect\nfully_preprocessed %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(text)\n\n# A tibble: 20 Ã— 1\n   text                                                                   \n   &lt;chr&gt;                                                                  \n 1 pride and prejudice                                                    \n 2 &lt;NA&gt;                                                                   \n 3 by jane austen                                                         \n 4 &lt;NA&gt;                                                                   \n 5 &lt;NA&gt;                                                                   \n 6 &lt;NA&gt;                                                                   \n 7 chapter                                                                \n 8 &lt;NA&gt;                                                                   \n 9 &lt;NA&gt;                                                                   \n10 it is a truth universally acknowledged that a single man in possession \n11 of a good fortune must be in want of a wife                            \n12 &lt;NA&gt;                                                                   \n13 however little known the feelings or views of such a man may be on his \n14 first entering a neighbourhood this truth is so well fixed in the minds\n15 of the surrounding families that he is considered the rightful property\n16 of some one or other of their daughters                                \n17 &lt;NA&gt;                                                                   \n18 my dear mr bennet said his lady to him one day have you heard that     \n19 netherfield park is let at last                                        \n20 &lt;NA&gt;                                                                   \n\n\nEach of these preprocessing steps serves a specific purpose in preparing the text for analysis. The choice of which steps to apply depends on the specific goals of your analysis and the nature of your text data.\n\n\n\n\n\n\nUnderstanding the Implications of Preprocessing\n\n\n\n\n\nItâ€™s crucial to carefully consider and document each preprocessing step you apply to your text data. These decisions can significantly impact your analysis results:\n\n\nInformation Loss: Some preprocessing steps (like removing punctuation or lowercasing) can remove potentially important information.\n\nAnalysis Bias: Certain preprocessing choices might inadvertently introduce bias into your analysis.\n\nInterpretability: Your preprocessing decisions affect how you can interpret your results later.\n\nReproducibility: Documenting your preprocessing steps is essential for others to reproduce or build upon your work.\n\nDomain Specificity: The appropriate preprocessing steps can vary depending on your field of study and the specific texts youâ€™re analyzing.\n\nAlways think critically about why youâ€™re applying each preprocessing step and how it might affect your analysis outcomes. When in doubt, itâ€™s often helpful to try multiple preprocessing approaches and compare the results.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#hands-on-coding",
    "href": "book/06-qta.html#hands-on-coding",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.5 Hands-On Coding ğŸ’»",
    "text": "2.5 Hands-On Coding ğŸ’»\nNow, letâ€™s apply our text preprocessing skills to a real dataset. Weâ€™ll use the answers submitted by you in response to the â€œwhat is digital humanities? write down three keywords that come to mindâ€ question. Our goal is to clean and prepare this text data for analysis.\nFirst, letâ€™s download the dataset:\nDownload DH Keywords dataset\nSave it in the folder name data in your working directory.\nWe can then load the csv file into RStudio:\n\np_load(tidyverse)\n\n# Load the dataset\ndh_keywords &lt;- read_csv(\"data/dh_keywords.csv\", col_names = FALSE)\n\nRows: 180 Columns: 1\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (1): X1\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(dh_keywords) &lt;- \"keyword\"\n\n# Display the first few rows\nhead(dh_keywords)\n\n# A tibble: 6 Ã— 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 AI     \n4 ai     \n5 AI     \n6 ai     \n\n\n\n2.5.1 Exercise 1: Lowercase Conversion\nConvert all keywords to lowercase.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the mutate() function with tolower() to convert the text to lowercase.\n\n\n\n\n\n\ndh_keywords_lower &lt;- dh_keywords %&gt;%\n  mutate(keyword = _(keyword))\n\nhead(dh_keywords_lower)\n\n\n\n\ndh_keywords_lower &lt;- dh_keywords %&gt;%\n  mutate(keyword = tolower(keyword))\n\nhead(dh_keywords_lower)\n\n# A tibble: 6 Ã— 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n\n\n\n\n2.5.2 Exercise 2: Remove Punctuation\nRemove all punctuation from the keywords.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse str_replace_all() with a regular expression to remove punctuation.\n\n\n\n\n\n\ndh_keywords_clean &lt;- dh_keywords_lower %&gt;%\n  mutate(keyword = str_replace_all(keyword, _, \"\"))\n\nhead(dh_keywords_clean)\n\n\n\n\ndh_keywords_clean &lt;- dh_keywords_lower %&gt;%\n  mutate(keyword = str_replace_all(keyword, \"[[:punct:]]\", \"\"))\n\nhead(dh_keywords_clean)\n\n# A tibble: 6 Ã— 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n\n\n\n\n2.5.3 Exercise 3: Remove Extra Whitespace\nRemove any leading, trailing, or extra whitespace from the keywords.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse str_trim() to remove leading and trailing whitespace, and str_squish() to replace multiple spaces with a single space.\n\n\n\n\n\n\ndh_keywords_trimmed &lt;- dh_keywords_clean %&gt;%\n  mutate(keyword = str_trim(_(_)))\n\nhead(dh_keywords_trimmed)\n\n\n\n\ndh_keywords_trimmed &lt;- dh_keywords_clean %&gt;%\n  mutate(keyword = str_trim(str_squish(keyword)))\n\nhead(dh_keywords_trimmed)\n\n# A tibble: 6 Ã— 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n\n\n\n\n2.5.4 Exercise 4: Remove Empty Entries\nRemove any rows that are empty after our preprocessing steps.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse filter() with nchar() to keep only non-empty entries.\n\n\n\n\n\n\ndh_keywords_final &lt;- dh_keywords_trimmed %&gt;%\n  filter(_(keyword) &gt; _)\n\n# Display the final result and the number of rows\nprint(dh_keywords_final)\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_final))\n\n\n\n\ndh_keywords_final &lt;- dh_keywords_trimmed %&gt;%\n  filter(nchar(keyword) &gt; 0)\n\n# Display the final result and the number of rows\nprint(dh_keywords_final)\n\n# A tibble: 180 Ã— 1\n   keyword\n   &lt;chr&gt;  \n 1 math   \n 2 chatgpt\n 3 ai     \n 4 ai     \n 5 ai     \n 6 ai     \n 7 coding \n 8 ai     \n 9 ai     \n10 data   \n# â„¹ 170 more rows\n\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_final))\n\n[1] \"Number of keywords after preprocessing: 180\"\n\n\nCombined version of all the preprocessing steps:\n\ndh_keywords_final &lt;- dh_keywords %&gt;%\n  mutate(keyword = tolower(keyword),\n         keyword = str_replace_all(keyword, \"[[:punct:]]\", \"\"),\n         keyword = str_trim(str_squish(keyword))) %&gt;%\n  filter(nchar(keyword) &gt; 0)\n\n\nCongratulations! Youâ€™ve successfully preprocessed the DH keywords dataset. These clean keywords are now ready for further analysis, such as frequency counting, visualization, or more advanced text analysis techniques.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#tokenization",
    "href": "book/06-qta.html#tokenization",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n3.1 Tokenization",
    "text": "3.1 Tokenization\nTokenization is a foundational step in text analysis, involving the process of breaking down text into smaller, meaningful units called tokens. These tokens can be words, phrases, or even sentences, depending on the level of analysis youâ€™re performing.\n\n3.1.1 Word Tokenization\nWord tokenization is the most common form of tokenization, where text is split into individual words. This process allows us to analyze text at its most granular level. Letâ€™s use our preprocessed DH keywords dataset to demonstrate:\n\np_load(tidytext, tidyverse)\n\nword_tokens &lt;- dh_keywords_final %&gt;%\n  unnest_tokens(word, keyword)\n\n# Display the first few tokens\nword_tokens %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 Ã— 1\n   word   \n   &lt;chr&gt;  \n 1 math   \n 2 chatgpt\n 3 ai     \n 4 ai     \n 5 ai     \n 6 ai     \n 7 coding \n 8 ai     \n 9 ai     \n10 data   \n\n\n:: {.callout-tip} ## The unnest_tokens() function The unnest_tokens() function from the tidytext package is particularly useful because it integrates seamlessly with the tidyverse ecosystem. It splits the text into tokens and transforms the data into a one-token-per-row format, which is ideal for further analysis using dplyr, ggplot2, and other tidy tools. :::\nIn this example, our keywords are already individual words, so the tokenization might not show a significant difference. However, for longer texts like novels or articles, this process would break down sentences into individual words, allowing for more detailed analysis.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#stopword-removal",
    "href": "book/06-qta.html#stopword-removal",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n3.4 Stopword Removal",
    "text": "3.4 Stopword Removal\nStopwords are common words (like â€œtheâ€, â€œisâ€, â€œatâ€) that are often removed from text analysis because they typically donâ€™t carry much meaning on their own. Removing stopwords can help focus the analysis on more meaningful content words.\n\np_load(tidytext)\n\ndata(stop_words)\n\nwords_without_stopwords &lt;- word_tokens %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n# Compare word counts before and after stopword removal\npaste(\"Words before stopword removal:\", nrow(word_tokens))\n\n[1] \"Words before stopword removal: 393\"\n\npaste(\"Words after stopword removal:\", nrow(words_without_stopwords))\n\n[1] \"Words after stopword removal: 374\"\n\n\n\n\n\n\n\n\nThe anti_join() function\n\n\n\n\n\nThe anti_join() function is used here to remove stopwords from our dataset. It keeps only the rows from the first dataset (word_tokens) that do not have a match in the second dataset (stop_words). This effectively removes all the stopwords from our text data.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#word-frequency-analysis",
    "href": "book/06-qta.html#word-frequency-analysis",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n3.5 Word Frequency Analysis",
    "text": "3.5 Word Frequency Analysis\nWord frequency analysis involves counting how often each word appears in a text. This can give us insights into the most important or prevalent themes in the text.\n\nword_frequencies &lt;- words_without_stopwords %&gt;%\n  count(word, sort = TRUE)\n\n# Display the top 10 most frequent words\nword_frequencies %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 Ã— 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 data           53\n 2 ai             39\n 3 coding         31\n 4 analysis       18\n 5 technology     17\n 6 science        13\n 7 computer       12\n 8 statistics      8\n 9 programming     7\n10 digital         5",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#word-clouds",
    "href": "book/06-qta.html#word-clouds",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n4.1 Word Clouds",
    "text": "4.1 Word Clouds\nWord clouds are a popular way to visualize the most frequent words in a text, with the size of each word proportional to its frequency.\n\np_load(wordcloud2)\n\n# Create a word cloud\nwordcloud2(data = word_frequencies %&gt;% slice_head(n = 50), size = 0.5)\n\n\n\n\n# Make it more aesthetically pleasing\np_load(RColorBrewer)\n\n# Create a color palette\ncolor_palette &lt;- brewer.pal(8, \"Dark2\")\n\nwordcloud2(\n  data = word_frequencies %&gt;% slice_head(n = 50),  # Use top 50 most frequent words\n  size = 0.6,                    # Increase text size for better readability\n  color = rep_len(color_palette, 50),  # Apply color palette to words\n  backgroundColor = \"white\",     # Set background color to white\n  rotateRatio = 0.3,             # Reduce word rotation for cleaner look\n  shape = \"circle\",              # Set overall shape of the word cloud\n  fontFamily = \"Arial\",          # Use Arial font for consistency\n  fontWeight = \"bold\",           # Make text bold for emphasis\n  minRotation = -pi/6,           # Set minimum rotation angle (30 degrees left)\n  maxRotation = pi/6             # Set maximum rotation angle (30 degrees right)\n)",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#bar-charts-of-word-frequency",
    "href": "book/06-qta.html#bar-charts-of-word-frequency",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n4.2 Bar Charts of Word Frequency",
    "text": "4.2 Bar Charts of Word Frequency\nBar charts offer a more precise way to visualize word frequencies, especially for comparing the most common words.\n\np_load(ggplot2)\n\nword_frequencies %&gt;%\n  slice_head(n = 10) %&gt;%\n  ggplot(aes(x = reorder(word, n), y = n, fill = desc(n))) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal()+\n  theme(legend.position = \"none\") +\n  labs(x = \"Word\", y = \"Frequency\", title = \"Top 10 Most Frequent DH Keywords\")",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#our-dataset-pride-and-prejudice",
    "href": "book/06-qta.html#our-dataset-pride-and-prejudice",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n1.3 Our Dataset: Pride and Prejudice ğŸ“š",
    "text": "1.3 Our Dataset: Pride and Prejudice ğŸ“š\nFor this chapter, weâ€™ll use Jane Austenâ€™s â€œPride and Prejudiceâ€ to demonstrate text analysis techniques. Weâ€™ll access this novel using the janeaustenr package.\n\n1.3.1 Install Package Manager pacman\n\nBefore we begin our text analysis, letâ€™s introduce a helpful tool for managing R packages: pacman. The pacman package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.\nKey features of pacman:\n\nIt combines the functionality of install.packages() and library() into a single function p_load().\nIt automatically installs packages if theyâ€™re not already installed.\nIt can load multiple packages with a single line of code.\n\nLetâ€™s install and load pacman:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\nlibrary(pacman)\n\nNow we can use p_load() to efficiently load (and install if necessary) the packages weâ€™ll need for our text analysis:\n\np_load(janeaustenr, tidyverse)\n\nThis single line will ensure all the packages we need are installed and loaded, streamlining our setup process.\nFor this chapter, weâ€™ll use Jane Austenâ€™s â€œPride and Prejudiceâ€ to demonstrate text analysis techniques. To download the book dataset, run the following command:\n\npride_and_prejudice &lt;- austen_books() %&gt;%\n  filter(book == \"Pride & Prejudice\")\n\npride_and_prejudice\n\nAlternatively, you can also download it directly here:\nDownload Pride and Prejudice dataset\nYou can create a folder named â€œdataâ€ in save the csv file in your working directorya and load the data with the following command:\n\n# Load required packages\np_load(readr)\n\n# Now read the CSV file\npride_and_prejudice &lt;- read_csv(\"data/pride_and_prejudice.csv\")\n\nRows: 13030 Columns: 2\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (2): text, book\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npride_and_prejudice\n\n# A tibble: 13,030 Ã— 2\n   text                                                                    book \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 PRIDE AND PREJUDICE                                                     Pridâ€¦\n 2 &lt;NA&gt;                                                                    Pridâ€¦\n 3 By Jane Austen                                                          Pridâ€¦\n 4 &lt;NA&gt;                                                                    Pridâ€¦\n 5 &lt;NA&gt;                                                                    Pridâ€¦\n 6 &lt;NA&gt;                                                                    Pridâ€¦\n 7 Chapter 1                                                               Pridâ€¦\n 8 &lt;NA&gt;                                                                    Pridâ€¦\n 9 &lt;NA&gt;                                                                    Pridâ€¦\n10 It is a truth universally acknowledged, that a single man in possession Pridâ€¦\n# â„¹ 13,020 more rows\n\n\nThis gives us the full text of â€œPride and Prejudiceâ€ in a tidy format, with one line per row.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#learning-check-1",
    "href": "book/06-qta.html#learning-check-1",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.4 Learning Check ğŸ",
    "text": "2.4 Learning Check ğŸ",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#qta-workflow",
    "href": "book/06-qta.html#qta-workflow",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n1.2 QTA Workflow",
    "text": "1.2 QTA Workflow\n\n\n\n\n\n\n\nQTA Workflow\n\n\n\n\n\nThe Quantitative Text Analysis (QTA) workflow illustrates the systematic process of analyzing textual data using computational methods. This workflow can be divided into two main stages: Preprocessing Steps and Quantitative Text Analysis.\nPreprocessing Steps: This initial stage is crucial for preparing raw text data for analysis. It involves several operations:\n\n\nTidy Structure: Organizing the data into a consistent, clean format. Tidy format refers to one observation/value per row.\n\nLowercase Conversion: Transforming all text to lowercase for uniformity.\n\nPunctuation and/or Special Character Removal: Eliminating punctuation marks and/or special characters that might interfere with analysis.\n\nNumbers Removal: Deleting numerical digits if theyâ€™re not relevant to the study.\n\nTokenization: Breaking down the text into individual units (usually words).\n\nNormalization: Standardizing text to a more consistent form (e.g., handling special characters, expanding contractions).\n\nStopwords Removal: Eliminating common words that typically donâ€™t carry significant meaning.\n\nStemming/Lemmatization: Reducing words to their root form to group similar words together.\n\nThese steps are not always applied in a fixed order and may be selectively used depending on the specific requirements of the analysis. The output of this stage is processed text ready for analysis.\nQuantitative Text Analysis:\nThis stage involves applying various analytical techniques to the preprocessed text. Some common methods include:\n\n\nWord Frequency Analysis: Counting and analyzing the occurrence of words.\n\nN-gram Analysis: Examining sequences of N words to understand context and phrases.\n\nTopic Modeling: Discovering abstract topics that occur in a collection of documents.\n\nSentiment Analysis: Determining the emotional tone behind a body of text.\nAnd moreâ€¦\n\nThis workflow emphasizes the importance of proper text preprocessing before conducting any advanced analysis. It provides a framework for researchers to follow, ensuring that their textual data is appropriately prepared and analyzed using quantitative methods. The diagramâ€™s structure, with its clear progression from preprocessing to analysis, highlights the sequential nature of the QTA process while also indicating the flexibility within each stage.\n\n\n\n\n\n\n\n\n\nInstall Package Manager pacman\n\n\n\n\n\nWe will be using some new packages that you probably havenâ€™t installed. To streamline the process of package installation, letâ€™s introduce a helpful tool for managing R packages: pacman. The pacman package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.\nKey features of pacman:\n\nIt combines the functionality of install.packages() and library() into a single function p_load().\nIt automatically installs packages if theyâ€™re not already installed.\nIt can load multiple packages with a single line of code.\n\nLetâ€™s install pacman:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\n\nOnce we load the pacman library, we can use p_load() to efficiently load (and install if necessary) the packages weâ€™ll need:\n\nlibrary(pacman)\np_load(tidyverse, tidytext)\n\nThis single line will ensure all the packages we need are installed and loaded, streamlining our setup process.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#stemming-and-lemmatization",
    "href": "book/06-qta.html#stemming-and-lemmatization",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n3.2 Stemming and Lemmatization",
    "text": "3.2 Stemming and Lemmatization\nStemming and lemmatization are techniques used to reduce words to their base or root form. This process is crucial for grouping similar words together and reducing the vocabulary size, which can improve the efficiency and effectiveness of text analysis.\n\n3.2.1 Stemming\nStemming is a simple, rule-based approach to reduce words to their stem or root form. It typically involves removing the ends of words, which can sometimes result in stems that are not actual words.\n\np_load(SnowballC)\n\nstemmed_words &lt;- word_tokens %&gt;%\n  mutate(stem = wordStem(word))\n\n# Display a few examples\nstemmed_words %&gt;%\n  select(word, stem) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 Ã— 2\n   word          stem   \n   &lt;chr&gt;         &lt;chr&gt;  \n 1 math          math   \n 2 chatgpt       chatgpt\n 3 ai            ai     \n 4 coding        code   \n 5 data          data   \n 6 plus          plu    \n 7 humanbeing    humanb \n 8 social        social \n 9 media         media  \n10 informational inform \n\n\n\n\n\n\n\n\nThe wordStem() function\n\n\n\n\n\nThe wordStem() function from the SnowballC package implements the Porter stemming algorithm, which is widely used in text processing. While stemming is fast and easy to implement, it can sometimes produce stems that are not real words, which might be confusing in some contexts.\n\n\n\n\n3.2.2 Lemmatization\nLemmatization is a more sophisticated approach that considers the context and part of speech of a word to determine its base form or lemma. Unlike stemming, lemmatization always produces real words.\n\np_load(textstem)\n\nlemmatized_words &lt;- word_tokens %&gt;%\n  mutate(lemma = lemmatize_words(word))\n\n# Display a few examples\nlemmatized_words %&gt;%\n  select(word, lemma) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 Ã— 2\n   word          lemma        \n   &lt;chr&gt;         &lt;chr&gt;        \n 1 math          math         \n 2 chatgpt       chatgpt      \n 3 ai            ai           \n 4 coding        code         \n 5 data          datum        \n 6 plus          plus         \n 7 humanbeing    humanbeing   \n 8 social        social       \n 9 media         medium       \n10 informational informational\n\n\n\n\n\n\n\n\nThe lemmatize_words() function\n\n\n\n\n\nThe lemmatize_words() function from the textstem package uses a dictionary-based approach to find the lemma of each word. Lemmatization is generally more accurate than stemming but can be slower and more computationally intensive.\n\n\n\n\n\n\n\n\n\nHow to Choose Between Stemming and Lemmatization?\n\n\n\n\n\nChoosing between stemming and lemmatization depends on your specific needs, resources, and the nature of your text analysis project:\n\n\nSpeed vs.Â Accuracy:\n\nStemming is faster but can produce non-words or incorrect roots.\nLemmatization is slower but more accurate, always producing real words.\n\n\n\nLanguage Complexity:\n\nFor languages with simple morphology (like English), stemming might suffice.\nFor languages with complex morphology (like German or Arabic), lemmatization is often preferred.\nFor languages without clear word boundaries (like Chinese or Japanese):\n\nNeither traditional stemming nor lemmatization applies directly.\nWord segmentation is a crucial first step.\nAfter segmentation, techniques similar to lemmatization may be used to normalize words.\n\n\n\n\n\nDomain Specificity:\n\nIn specialized fields (e.g., medical texts), lemmatizationâ€™s accuracy can be crucial because it preserves the precise meanings of complex, domain-specific terms that might be lost or conflated through simpler stemming methods.\nFor general-purpose analysis, stemmingâ€™s speed might be more valuable.\n\n\n\nAvailable Resources:\n\nStemming requires less computational power and no additional linguistic resources.\nLemmatization often needs dictionaries and more processing power.\n\n\n\nProject Goals:\n\nFor quick, exploratory analysis, stemming can be sufficient.\nFor in-depth linguistic analysis or when meaning preservation is crucial, use lemmatization.\n\n\n\nText Quality:\n\nFor well-edited texts, both methods work well.\nFor informal texts (e.g., social media), stemming might handle variations and misspellings better.\n\n\n\nInterpretability:\n\nStemmed words can be harder for humans to interpret.\nLemmatized words are always real words, making results more readable.\n\n\n\nIn practice, many researchers try both methods and compare results. For humanities projects, especially those involving literary analysis or historical texts, lemmatization often provides more meaningful and interpretable results, despite the additional computational cost.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#n-grams",
    "href": "book/06-qta.html#n-grams",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n3.3 N-grams",
    "text": "3.3 N-grams\nN-grams are contiguous sequences of n items from a given text. These items can be words, characters, or even syllables. N-grams help capture phrases and word associations, providing context that single words might miss.\n\n\n\n\n\n\nUnderstanding N-grams\n\n\n\n\n\n\nUnigrams (n=1): Single words, e.g., â€œdigitalâ€, â€œhumanitiesâ€\nBigrams (n=2): Two consecutive words, e.g., â€œdigital humanitiesâ€\nTrigrams (n=3): Three consecutive words, e.g., â€œnatural language processingâ€\nAnd so on for larger nâ€¦\n\nN-grams preserve word order, which can be crucial for understanding meaning and context in text analysis.\n\n\n\nLetâ€™s create bigrams from our original (non-preprocessed) DH keywords dataset:\n\nbigrams &lt;- dh_keywords %&gt;%\n  unnest_tokens(bigram, keyword, token = \"ngrams\", n = 2)\n\n# Display the most common bigrams\nbigrams %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  slice_head(n = 10)\n\n# A tibble: 10 Ã— 2\n   bigram               n\n   &lt;chr&gt;            &lt;int&gt;\n 1 &lt;NA&gt;                67\n 2 ai coding            6\n 3 coding data          6\n 4 data ai              6\n 5 data coding          6\n 6 computer science     5\n 7 data analysis        5\n 8 ai data              4\n 9 social science       4\n10 analysis coding      3\n\n\n\n\n\n\n\n\nWhy use bigrams?\n\n\n\n\n\nBigrams can reveal common phrases and word pairs in the text that single words miss. For instance, â€œdigital humanitiesâ€ carries more specific meaning than â€œdigitalâ€ and â€œhumanitiesâ€ separately. In literary analysis, bigrams like â€œstream consciousnessâ€ might indicate modernist writing techniques.\n\n\n\nN-grams are particularly useful for:\n\n\nPhrase Detection: Identifying common expressions or technical terms.\n\nContextual Analysis: Understanding how words are used in context.\n\nLanguage Modeling: Predicting the next word based on previous words.\n\nStylometric Analysis: Studying an authorâ€™s writing style.\n\n\n\n\n\n\n\nConsidering Multiple N-gram Sizes\n\n\n\n\n\nResearchers often use a combination of n-gram sizes (e.g., unigrams, bigrams, and trigrams) for several reasons:\n\n\nComprehensive Coverage: Different n-gram sizes capture different aspects of language.\n\nBalancing Specificity and Generality: Larger n-grams are more specific but occur less frequently.\n\nHandling Varied Phrases: Some meaningful phrases are two words, others three or more.\n\nHowever, using multiple n-gram sizes also has challenges: - Increased Complexity: More data to process and analyze. - Overlapping Information: Bigrams and trigrams may contain redundant information. - Sparse Data: Larger n-grams may occur very infrequently, leading to sparse data issues.\nThe choice depends on your research questions and the nature of your texts.\n\n\n\nHereâ€™s an example of how we might generate n-grams of different sizes:\n\nlibrary(tidytext)\nlibrary(dplyr)\n\n# Function to generate n-grams\ngenerate_ngrams &lt;- function(data, n) {\n  data %&gt;%\n    unnest_tokens(ngram, keyword, token = \"ngrams\", n = n) %&gt;%\n    count(ngram, sort = TRUE) %&gt;%\n    mutate(n = n)\n}\n\n# Generate unigrams, bigrams, and trigrams\nunigrams &lt;- generate_ngrams(dh_keywords, 1)\nbigrams &lt;- generate_ngrams(dh_keywords, 2)\ntrigrams &lt;- generate_ngrams(dh_keywords, 3)\n\n# Combine the results\nall_ngrams &lt;- bind_rows(unigrams, bigrams, trigrams)\n\n# Display the top 5 of each\nall_ngrams %&gt;%\n  group_by(n) %&gt;%\n  slice_head(n = 5) %&gt;%\n  arrange(n, desc(nn))\n\nThis approach allows us to see the most common phrases of different lengths in our dataset, providing a more comprehensive view of the language used in digital humanities keywords.\n\n\n\n\n\n\nPreprocessing Considerations\n\n\n\n\n\nWhen working with n-grams, itâ€™s often better to use non-preprocessed text:\n\nPunctuation and capitalization can be meaningful for phrase detection.\nStopwords might be important parts of meaningful phrases.\nWord order matters, so stemming or lemmatization might obscure important distinctions.\n\nPreprocessing can be applied after n-gram generation if needed, but this allows for more nuanced analysis of phrases and expressions.\n\n\n\nBy considering different n-gram sizes and using non-preprocessed text, we can gain richer insights into the language patterns and meaningful phrases in our digital humanities corpus.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#tidy-structure",
    "href": "book/06-qta.html#tidy-structure",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.3 Tidy Structure",
    "text": "2.3 Tidy Structure\nThe first step in our preprocessing pipeline is to ensure our data is in a tidy format, with one sentence per row. Weâ€™ll use separate_rows() with a regular expression (regex) pattern to achieve this:\n\ntidy_data &lt;- mock_data %&gt;%\n  separate_rows(text, sep = \"(?&lt;=[.!?])\\\\s+(?=[A-Z])\")\n\nprint(tidy_data)\n\n# A tibble: 8 Ã— 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 The Quick Brown Fox Jumps Over the Lazy Dog!                                  \n2 Data Science meets Cultural Studies.                                          \n3 Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literatâ€¦\n4 R Programming for Text Analysis - Chapter 3.                                  \n5 Machine Learning for Textual Analysis                                         \n6 NLP techniques & their applications in DH research; Computational Methods in â€¦\n7 20+ ways to visualize data: graphs, charts, and more!                         \n8 Digital Archives and Text Mining Techniques.                                  \n\n\n\n\n\n\n\n\nUnderstanding separate_rows()\n\n\n\n\n\n\n\nseparate_rows() Function:\n\nPart of the tidyr package in tidyverse\nSeparates a column into multiple rows based on a delimiter\nSyntax: separate_rows(data, column, sep = delimiter)\n\n\n\nRegular Expression (Regex) Pattern:\n\n\n(?&lt;=[.!?]): Positive lookbehind, matches a position after a period, exclamation mark, or question mark\n\n\\\\s+: Matches one or more whitespace characters\n\n(?=[A-Z]): Positive lookahead, matches a position before an uppercase letter\nCombined, this pattern splits the text at sentence boundaries\n\n\nHow it works:\n\n\nseparate_rows() applies the regex pattern to split the â€˜textâ€™ column\nEach resulting sentence becomes a new row in the dataframe\nThe original rowâ€™s other columns (if any) are duplicated for each new sentence row",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#understanding-regular-expressions",
    "href": "book/06-qta.html#understanding-regular-expressions",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n2.1 Understanding Regular Expressions",
    "text": "2.1 Understanding Regular Expressions\nBefore we dive into analyzing our mock data, letâ€™s explore a powerful tool in text analysis: Regular Expressions.\nHave you ever wondered how computer programs like Excel or Word can find the exact word or phrase youâ€™re searching for? Or how they can replace all instances of a word throughout a document in just seconds? These everyday text operations are powered by a concept called pattern matching, and regular expressions take this idea to a whole new level.\nIn this section, weâ€™ll introduce the basics of regular expressions, focusing on concepts and examples relevant to humanities research. Weâ€™ll start with simple patterns similar to what you might use in Excel, and gradually move to more powerful expressions. By the end, youâ€™ll have a foundation that will help you better understand and implement the text preprocessing techniques weâ€™ll use in our analysis.\n\n\n\n\n\n\nWhat are Regular Expressions?\n\n\n\n\n\nRegular Expressions, often called regex, are like a special language for describing patterns in text. Imagine youâ€™re a librarian with a magical magnifying glass that can find not just specific words, but patterns in books.\n\n2.1.1 The Concept\n\n\nPattern Matching: Instead of searching for exact words, regex lets you search for patterns. For example, you could search for:\n\nAll words that start with â€œpreâ€\nAny sequence of five letters\nAll dates in a specific format\n\n\n\nText Processing: Once you find these patterns, you can do things like:\n\nHighlight them\nReplace them with something else\nExtract them for further study\n\n\n\n2.1.2 Examples of Regex\nTo give you a better idea of what regular expressions look like and how they work, letâ€™s look at an example:\n\np_load(stringr)\n\n# Sample text\ntext &lt;- \"Jane Austen wrote Pride and Prejudice. Elizabeth Bennet is the protagonist.\"\n\n# Regex pattern for capitalized words\npattern &lt;- \"\\\\b[A-Z][a-z]+\\\\b\"\n\n# Find all matches\nmatches &lt;- str_extract_all(text, pattern)\n\n# Print the matches\nprint(matches)\n\n[[1]]\n[1] \"Jane\"      \"Austen\"    \"Pride\"     \"Prejudice\" \"Elizabeth\" \"Bennet\"   \n\n# To see which words were matched in context\nstr_view(text, pattern)\n\n[1] â”‚ &lt;Jane&gt; &lt;Austen&gt; wrote &lt;Pride&gt; and &lt;Prejudice&gt;. &lt;Elizabeth&gt; &lt;Bennet&gt; is the protagonist.\n\n\n\n\n\n\n\n\nRegex breakdown\n\n\n\n\n\nLetâ€™s break down the regex pattern \\\\b[A-Z][a-z]+\\\\b:\n\n\\\\b: This represents a word boundary. In R, we need to escape the backslash, so we use two. It ensures weâ€™re matching whole words, not parts of words.\n[A-Z]: This character class matches any single uppercase letter from A to Z.\n\n[a-z]+: This matches one or more lowercase letters.\n\n\n[a-z] is a character class that matches any single lowercase letter.\nThe + quantifier means â€œone or moreâ€ of the preceding element.\n\n\n\\\\b: Another word boundary to end the match.\n\nSo, this pattern matches: - Words that start with a capital letter (like names or the first word of a sentence) - Followed by one or more lowercase letters - As whole words, not parts of larger words\nIt wonâ€™t match: - ALL CAPS words - words with numbers or symbols - Single-letter capitalized words like â€œIâ€ or â€œAâ€\nThis pattern is useful for finding proper nouns in the middle of sentences, like names of people or places.\n\n\n\n\n2.1.3 POSIX Character Classes: A Friendly Starting Point\nYou may find that regex can be quite hard to read for humans. POSIX character classes are pre-defined sets of characters that make regex more accessible and portable across different systems. They simplify regex patterns and address some common challenges in text processing:\n\nSimplification: POSIX classes provide easy-to-remember shorthand for common character groups. Instead of writing [A-Za-z] to match any letter, you can use [:alpha:].\nConsistency: They ensure consistent behavior across different operating systems and programming languages. For example, [A-Z] might behave differently in some contexts depending on the locale settings, but [:upper:] is always consistent.\nInternationalization: POSIX classes can handle characters beyond the ASCII range, making them useful for working with texts in various languages.\nReadability: They make regex patterns more readable and self-explanatory, which is especially helpful when sharing code or working in teams.\n\nHere are some useful POSIX character classes:\n\n\n[:alpha:]: Matches any alphabetic character (equivalent to [A-Za-z] in English texts)\n\n[:digit:]: Matches any digit (equivalent to [0-9])\n\n[:lower:]: Matches any lowercase letter\n\n[:upper:]: Matches any uppercase letter\n\n[:punct:]: Matches any punctuation character\n\n[:space:]: Matches any whitespace character (spaces, tabs, newlines)\n\nBy using these classes, you can create more robust and readable regex patterns. For example, instead of [A-Za-z0-9] to match any alphanumeric character, you could use [[:alpha:][:digit:]], which is clearer in its intent and works across different language settings. ````\n\n2.1.4 Examples in Humanities Context\n\nFinding capitalized words:\n\nPattern: [[:upper:]][[:lower:]]+\n\nThis would match words like â€œShakespeareâ€, â€œLondonâ€, â€œRenaissanceâ€\n\n\nIdentifying years:\n\nPattern: [[:digit:]]{4}\n\nThis would match years like â€œ1564â€, â€œ1616â€, â€œ2023â€\n\n\nLocating punctuation:\n\nPattern: [[:punct:]]\n\nThis would find all punctuation marks in a text\n\n\n\nRemember, regex is a tool that becomes more useful as you practice. Start simple, and youâ€™ll gradually be able to create more complex patterns for your research needs!",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "book/06-qta.html#learning-check-2",
    "href": "book/06-qta.html#learning-check-2",
    "title": "Â§6 Quantitative Text Analysis Basics ğŸ“Š",
    "section": "\n4.3 Learning Check ğŸ",
    "text": "4.3 Learning Check ğŸ\n\n\n\nThis learning check will help reinforce the key concepts covered in this section on basic text analysis and visualization techniques.",
    "crumbs": [
      "Book",
      "Â§6 Quantitative Text Analysis Basics ğŸ“Š"
    ]
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Monthly Quiz",
    "section": "",
    "text": "Student Information\n\n Verify\n\n\n\n\n\n\n\n\n\n\n\n\nShow Analysis"
  }
]