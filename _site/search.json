[
  {
    "objectID": "book/05-tidyverse.html",
    "href": "book/05-tidyverse.html",
    "title": "§5 Spinning the Reels 🎰",
    "section": "",
    "text": "Welcome to the exciting world of tidyverse! In this chapter, we’ll build on our knowledge of R by exploring the tidyverse, a collection of R packages designed for data science. We’ll create a virtual slot machine to demonstrate the power and simplicity of tidyverse functions.",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check",
    "href": "book/05-tidyverse.html#learning-check",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n1.1 Learning Check 🏁",
    "text": "1.1 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#filter-subset-rows",
    "href": "book/05-tidyverse.html#filter-subset-rows",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.1 filter(): Subset Rows",
    "text": "2.1 filter(): Subset Rows\n\n# Find all books published after 1900\nbooks %&gt;% \nfilter(year &gt; 1900)\n\n# A tibble: 4 × 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\n\n\n\n\n\n\nThe Pipe Operator %&gt;%\n\n\n\n\n\nThe %&gt;% operator is called the “pipe” operator. It’s a fundamental concept in the tidyverse that greatly enhances code readability and workflow. Here’s how it works:\n\nFunction chaining: The pipe takes the output of one function and passes it as the first argument to the next function. This allows us to chain multiple operations together in a logical sequence.\nLeft-to-right reading: Instead of nesting functions within each other, which can be hard to read, the pipe allows us to read our code from left to right, much like we read English.\nImproved readability: By using the pipe, we can break down complex operations into a series of smaller, more manageable steps.\n\nFor example, let’s compare these two equivalent operations:\nWithout pipe:\n\nfilter(books, year &gt; 1900)\n\n# A tibble: 4 × 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\nWith pipe:\n\nbooks %&gt;% filter(year &gt; 1900)\n\n# A tibble: 4 × 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n\n\nThe piped version can be read as “Take the books data, then filter it to keep only books published after 1900”.\nFor more complex operations, the benefits become even clearer, which we will see in a moment.",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#select-choose-columns",
    "href": "book/05-tidyverse.html#select-choose-columns",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.2 select(): Choose Columns",
    "text": "2.2 select(): Choose Columns\n\n# Select only title and author columns\nbooks %&gt;%\nselect(title, author)\n\n# A tibble: 5 × 2\n  title                  author    \n  &lt;chr&gt;                  &lt;chr&gt;     \n1 1984                   Orwell    \n2 Pride and Prejudice    Austen    \n3 The Great Gatsby       Fitzgerald\n4 To Kill a Mockingbird  Lee       \n5 The Catcher in the Rye Salinger",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#mutate-add-new-variables",
    "href": "book/05-tidyverse.html#mutate-add-new-variables",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.3 mutate(): Add New Variables",
    "text": "2.3 mutate(): Add New Variables\n\n# Add a new column for the book's age\nbooks %&gt;%\nmutate(age = 2024 - year)\n\n# A tibble: 5 × 6\n  title                  author      year genre         pages   age\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328    75\n2 Pride and Prejudice    Austen      1813 Romance         432   211\n3 The Great Gatsby       Fitzgerald  1925 Modernist       180    99\n4 To Kill a Mockingbird  Lee         1960 Coming-of-age   281    64\n5 The Catcher in the Rye Salinger    1951 Coming-of-age   234    73",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#arrange-sort-rows",
    "href": "book/05-tidyverse.html#arrange-sort-rows",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.4 arrange(): Sort Rows",
    "text": "2.4 arrange(): Sort Rows\n\n# Sort books by year, oldest first\nbooks %&gt;%\narrange(year)\n\n# A tibble: 5 × 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Pride and Prejudice    Austen      1813 Romance         432\n2 The Great Gatsby       Fitzgerald  1925 Modernist       180\n3 1984                   Orwell      1949 Dystopian       328\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n5 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n\n\n\n\n\n\n\n\nComparing arrange() and order()\n\n\n\n\n\nIn Tidyverse, we use arrange() to sort data frames, which is often more intuitive and easier to use with multiple columns. In Base R, you typically use order() within square brackets or sort() for vectors.\nFor example:\nTidyverse: data %&gt;% arrange(column_name)\nBase R: data[order(data$column_name), ]\nThe Tidyverse method is more readable, especially when sorting by multiple columns or in descending order.",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#summarise-summarize-data",
    "href": "book/05-tidyverse.html#summarise-summarize-data",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.5 summarise(): Summarize Data",
    "text": "2.5 summarise(): Summarize Data\n\n# Calculate average number of pages\nbooks %&gt;%\nsummarise(avg_pages = mean(pages))\n\n# A tibble: 1 × 1\n  avg_pages\n      &lt;dbl&gt;\n1       291",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#group_by-group-data-for-operations",
    "href": "book/05-tidyverse.html#group_by-group-data-for-operations",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.6 group_by(): Group Data for Operations",
    "text": "2.6 group_by(): Group Data for Operations\n\n# Average pages by genre\nbooks %&gt;%\ngroup_by(genre) %&gt;%\nsummarise(avg_pages = mean(pages))\n\n# A tibble: 4 × 2\n  genre         avg_pages\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Coming-of-age      258.\n2 Dystopian          328 \n3 Modernist          180 \n4 Romance            432",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#chaining-multiple-actions",
    "href": "book/05-tidyverse.html#chaining-multiple-actions",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.7 Chaining Multiple Actions",
    "text": "2.7 Chaining Multiple Actions\nOne of the key advantages of Tidyverse is the ability to chain multiple actions together using the pipe operator (%&gt;%). Let’s compare how we can perform a series of data manipulations using both Tidyverse and Base R.\nLet’s say we want to: 1. Filter books published after 1900 2. Select only the title, author, and year columns 3. Sort the results by year 4. Get the first 3 entries\n\n2.7.1 Tidyverse Approach\n\nbooks %&gt;%\nfilter(year &gt; 1900) %&gt;%\nselect(title, author, year) %&gt;%\narrange(year) %&gt;%\nhead(3)\n\n# A tibble: 3 × 3\n  title                  author      year\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt;\n1 The Great Gatsby       Fitzgerald  1925\n2 1984                   Orwell      1949\n3 The Catcher in the Rye Salinger    1951\n\n\nIn this Tidyverse approach, we can read the code from left to right, following the logical flow of operations. Each step is clearly defined, and the pipe operator (%&gt;%) passes the result of each operation to the next.\n\n2.7.2 Base R Approach\n\n# Filter books published after 1900\nfiltered_books &lt;- books[books$year &gt; 1900, ]\n# Select only title, author, and year columns\nselected_books &lt;- filtered_books[, c(\"title\", \"author\", \"year\")]\n# Sort by year\nsorted_books &lt;- selected_books[order(selected_books$year), ]\n# Get the first 3 entries\nresult &lt;- head(sorted_books, 3)\n# View the result\nresult\n\n# A tibble: 3 × 3\n  title                  author      year\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt;\n1 The Great Gatsby       Fitzgerald  1925\n2 1984                   Orwell      1949\n3 The Catcher in the Rye Salinger    1951\n\n\nIn the Base R approach, we need to create intermediate variables at each step. The code reads from top to bottom, with each line representing a separate operation.",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check-1",
    "href": "book/05-tidyverse.html#learning-check-1",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.8 Learning Check 🏁",
    "text": "2.8 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#hands-on-coding",
    "href": "book/05-tidyverse.html#hands-on-coding",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n2.9 Hands-On Coding 💻",
    "text": "2.9 Hands-On Coding 💻\nTry the following exercises:\n\nUse filter() to find all books written by Austen or Orwell.\nUse arrange() to sort the books by number of pages, from longest to shortest.\nUse mutate() to add a new column called words, assuming an average of 250 words per page.\nUse group_by() and summarise() to find the earliest publication year for each genre.\n\n\n2.9.1 Exercise 1: Filter books by Austen or Orwell\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the %in% operator within filter() to check if the author is in a vector of names.\n\n\n\n\n\n\nbooks %&gt;%\n  filter(author _ c(\"Austen\", \"Orwell\"))\n\n\n\n\nbooks %&gt;%\n  filter(author %in% c(\"Austen\", \"Orwell\"))\n\n# A tibble: 2 × 5\n  title               author  year genre     pages\n  &lt;chr&gt;               &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 1984                Orwell  1949 Dystopian   328\n2 Pride and Prejudice Austen  1813 Romance     432\n\n\n\n\n2.9.2 Exercise 2: Sort books by pages, longest to shortest\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse arrange() with desc() to sort in descending order.\n\n\n\n\n\n\nbooks %&gt;%\n  arrange(_(_))\n\n\n\n\nbooks %&gt;%\n  arrange(desc(pages))\n\n# A tibble: 5 × 5\n  title                  author      year genre         pages\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Pride and Prejudice    Austen      1813 Romance         432\n2 1984                   Orwell      1949 Dystopian       328\n3 To Kill a Mockingbird  Lee         1960 Coming-of-age   281\n4 The Catcher in the Rye Salinger    1951 Coming-of-age   234\n5 The Great Gatsby       Fitzgerald  1925 Modernist       180\n\n\n\n\n2.9.3 Exercise 3: Add words column\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse mutate() to create a new column. Multiply the ‘pages’ column by 250.\n\n\n\n\n\n\nbooks %&gt;%\n  mutate(words = _ * _)\n\n\n\n\nbooks %&gt;%\n  mutate(words = pages * 250)\n\n# A tibble: 5 × 6\n  title                  author      year genre         pages  words\n  &lt;chr&gt;                  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 1984                   Orwell      1949 Dystopian       328  82000\n2 Pride and Prejudice    Austen      1813 Romance         432 108000\n3 The Great Gatsby       Fitzgerald  1925 Modernist       180  45000\n4 To Kill a Mockingbird  Lee         1960 Coming-of-age   281  70250\n5 The Catcher in the Rye Salinger    1951 Coming-of-age   234  58500\n\n\n\n\n2.9.4 Exercise 4: Find earliest publication year by genre\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse group_by() followed by summarise(). Within summarise(), use min() to find the earliest year.\n\n\n\n\n\n\nbooks %&gt;%\n  group_by(_) %&gt;%\n  summarise(earliest_year = _(_))\n\n\n\n\nbooks %&gt;%\n  group_by(genre) %&gt;%\n  summarise(earliest_year = min(year))\n\n# A tibble: 4 × 2\n  genre         earliest_year\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Coming-of-age          1951\n2 Dystopian              1949\n3 Modernist              1925\n4 Romance                1813",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#expand-the-books-dataset",
    "href": "book/05-tidyverse.html#expand-the-books-dataset",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.1 Expand the Books Dataset",
    "text": "3.1 Expand the Books Dataset\nLet’s expand the books dataset to include some more variables for visualization purposes:\n\nnovels &lt;- books %&gt;%\n  mutate(\n    words = pages*250, # Estimating word count based on pages\n    characters = c(30, 25, 15, 20, 10), # Number of named characters (estimated)\n    rating = c(4.2, 4.5, 4.0, 4.3, 4.1), # Modern reader ratings (out of 5)\n    male_chars = c(20, 10, 10, 12, 7), # Number of male characters (estimated)\n    female_chars = c(10, 15, 5, 8, 3) # Number of female characters (estimated)\n  )\n# View the dataset\nnovels\n\n# A tibble: 5 × 10\n  title             author  year genre pages  words characters rating male_chars\n  &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 1984              Orwell  1949 Dyst…   328  82000         30    4.2         20\n2 Pride and Prejud… Austen  1813 Roma…   432 108000         25    4.5         10\n3 The Great Gatsby  Fitzg…  1925 Mode…   180  45000         15    4           10\n4 To Kill a Mockin… Lee     1960 Comi…   281  70250         20    4.3         12\n5 The Catcher in t… Salin…  1951 Comi…   234  58500         10    4.1          7\n# ℹ 1 more variable: female_chars &lt;dbl&gt;\n\n\nThis dataset gives us a rich set of variables to explore, including publication year, word count, genre, character gender representation, and modern reader ratings.",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#the-basic-structure-of-a-ggplot",
    "href": "book/05-tidyverse.html#the-basic-structure-of-a-ggplot",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.2 1. The Basic Structure of a ggplot",
    "text": "3.2 1. The Basic Structure of a ggplot\nEvery ggplot2 plot starts with the ggplot() function and uses + to add layers. The basic structure is:\n\nggplot(data = &lt;DATA&gt;) +\nGEOM_FUNCTION(mapping = aes(&lt;MAPPINGS&gt;))\n\nLet’s create a simple scatter plot of publication year vs. word count (thousands):\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words / 1000))\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Concepts\n\n\n\n\n\n\n\nggplot(data = novels): Initializes the plot with our dataset\n\ngeom_point(): Adds a layer of points (for a scatter plot)\n\naes(x = year, y = words): Maps variables to aesthetic properties (here, x and y positions)",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#aesthetic-mappings",
    "href": "book/05-tidyverse.html#aesthetic-mappings",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.3 2. Aesthetic Mappings",
    "text": "3.3 2. Aesthetic Mappings\nAesthetics are visual properties of the objects in your plot. Common aesthetics include: - x and y positions - color - size - shape\nLet’s map the rating to the color of the points:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words, color = rating))\n\n\n\n\n\n\n\nAlternatively, we can also use the size of the points to indicate the rating:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words, size = rating))",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#adding-labels-with-labs",
    "href": "book/05-tidyverse.html#adding-labels-with-labs",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.4 3. Adding Labels with labs()",
    "text": "3.4 3. Adding Labels with labs()\nWe can improve our plot by adding informative labels:\n\nggplot(data = novels) +\ngeom_point(mapping = aes(x = year, y = words/1000, size = rating)) +\nlabs(title = \"Classic Novels: Publication Year vs. Word Count\",\n     x = \"Year of Publication\",\n     y = \"Number of Words (thousands)\",\n     size = \"Rating\")",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#geometric-objects-geoms",
    "href": "book/05-tidyverse.html#geometric-objects-geoms",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.5 4. Geometric Objects (geoms)",
    "text": "3.5 4. Geometric Objects (geoms)\nDifferent geom functions create different types of plots. Let’s create a bar plot of character counts:\n\nggplot(data = novels) +\ngeom_col(mapping = aes(x = title, y = characters)) +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon geoms\n\n\n\n\n\n\n\ngeom_point(): Scatter plots\n\ngeom_line(): Line graphs\n\ngeom_col() or geom_bar(): Bar charts\n\ngeom_boxplot(): Box plots\n\n\n\n\n\n\n\n\n\n\nR Graph Gallery: Inspiration for Your Visualizations\n\n\n\n\n\nFor more inspiration and examples of what’s possible with ggplot2, check out the R Graph Gallery. This fantastic resource offers:\n\nA wide variety of chart types and styles\nReproducible code for each graph\nExplanations and use cases for different visualizations\nAdvanced techniques and customizations\n\nExploring the R Graph Gallery can help you discover new ways to visualize your data and improve your ggplot2 skills!",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#learning-check-2",
    "href": "book/05-tidyverse.html#learning-check-2",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n3.6 Learning Check 🏁",
    "text": "3.6 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/05-tidyverse.html#hands-on-coding-1",
    "href": "book/05-tidyverse.html#hands-on-coding-1",
    "title": "§5 Spinning the Reels 🎰",
    "section": "\n4.1 Hands-On Coding 💻",
    "text": "4.1 Hands-On Coding 💻\nLet’s explore our slot machine results with some exercises. Remember to use tidyverse functions like filter(), summarise(), group_by(), and ggplot().\n\n4.1.1 Exercise 1: Summarize the Results\nCalculate the total number of plays, number of wins, and the win percentage.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse summarise() with n(), sum() functions.\n\n\n\n\n\n\nresults %&gt;%\n  summarise(\n    total_plays = _(),\n    wins = _(win),\n    win_percentage = wins/ _ *100\n)\n\nresults %&gt;%summarise(n())\n\n\n\n\nresults %&gt;%\n  summarise(\n    total_plays = n(),\n    wins = sum(win),\n    win_percentage = wins / total_plays * 100\n  )\n\n# A tibble: 1 × 3\n  total_plays  wins win_percentage\n        &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;\n1         100     1              1\n\n\n\n\n\n\n\n\nn() vs. nrow()\n\n\n\n\n\n\nn() is a dplyr function:\n\n\nIt’s designed to work seamlessly within dplyr verbs like summarise(), mutate(), and filter().\nIt’s particularly useful when working with grouped data.\n\n\nnrow() is a base R function:\n\n\nIt works on data frames and matrices, but not directly within dplyr pipelines.\nIt doesn’t automatically respect grouping in dplyr operations.\n\n\nBehavior with grouped data:\n\n\nn() will give you the count for each group when used with group_by().\nnrow() will always return the total number of rows in the entire data frame.\n\n\nSyntax in dplyr operations:\n\n\nn() can be used directly: summarise(count = n())\nnrow() typically needs to be wrapped: summarise(count = nrow(.))\n\n\nPerformance:\n\n\nn() is optimized for dplyr operations and can be faster in some cases.\n\n\n\n\n\n\n4.1.2 Exercise 2: Find Winning Combinations\nCreate a new data frame showing only the winning plays and their symbol combinations.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse filter() to select winning plays, then select() to choose the columns you want to display.\n\n\n\n\n\n\nwinning_plays &lt;- results %&gt;%\n  filter(_ == TRUE) %&gt;%\n  select(play, symbol1, symbol2, symbol3)\n\nprint(winning_plays)\n\n\n\n\nwinning_plays &lt;- results %&gt;%\n  filter(win == TRUE) %&gt;%\n  select(play, symbol1, symbol2, symbol3)\nprint(winning_plays)\n\n# A tibble: 1 × 4\n   play symbol1 symbol2 symbol3\n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1    28 🍋      🍋      🍋     \n\n\n\n\n4.1.3 Exercise 3: Visualize Symbol Distribution\nCreate a bar plot showing the distribution of symbols in the first reel (symbol1 column).\nEmojis often can’t be rendered directly in plots. While there are packages like emojifont or ggtext that can handle emoji rendering, for simplicity, we’ll use a text representation of the symbols.\n\nresults &lt;- results %&gt;%\n  mutate(symbol1_text = case_when(\n    symbol1 == \"🍒\" ~ \"Cherry\",\n    symbol1 == \"🍋\" ~ \"Lemon\",\n    symbol1 == \"🍊\" ~ \"Orange\",\n    symbol1 == \"🍇\" ~ \"Grapes\",\n    symbol1 == \"🔔\" ~ \"Bell\",\n    symbol1 == \"💎\" ~ \"Diamond\"\n  ))\n\n\n\n\n\n\n\n\nHint 1/2\n\n\n\n\n\nUse ggplot() with geom_bar() to create a bar chart of symbol1.\n\n\n\n\n\n\n\n\n\n\n\nHint 2/2\n\n\n\n\n\nIn aes(), map symbol1 to both x and fill for a colored bar chart.\n\n\n\n\n\n\nggplot(results, aes(x = _, fill = _)) +\n  geom_bar() +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") \n\n\n\nA Simple Plot\n\nggplot(results, aes(x = symbol1_text, fill = symbol1_text)) +\n  geom_bar() +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") \n\n\n\n\n\n\n\nA Customized Plot\nA strength of ggplot2 is that it allows various cutomizations of the plot. Below is an example where we manually set the fill colors of the bar plot. You can also check out packages such as RColorBrewer for presets of color pallettes. ggplot2 also has preset themes that you can use to immediately give the plot a cleaner or more professional look (e.g., the theme_minimal() used in the following example).\n\n# Define a custom color palette\nsymbol_colors &lt;- c(\n  \"Cherry\" = \"#D2042D\",  # Red\n  \"Lemon\" = \"#FFF700\",   # Yellow\n  \"Orange\" = \"#FFA500\",  # Orange\n  \"Grapes\" = \"#6F2DA8\",  # Purple\n  \"Bell\" = \"#FFD700\",    # Gold\n  \"Diamond\" = \"#B9F2FF\"  # Light Blue\n)\n\nggplot(results, aes(x = symbol1_text, fill = symbol1_text)) +\n  geom_bar() +\n  scale_fill_manual(values = symbol_colors) +\n  labs(title = \"Distribution of Symbols in First Reel\", x = \"Symbol\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend as it's redundant\n\n\n\n\n\n\n\n\n\nCongratulations! You’ve now practiced using various tidyverse functions to analyze and visualize data from our virtual slot machine. These skills are fundamental in data manipulation and analysis, which are crucial in many digital humanities projects.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\n\nIn this chapter, we’ve covered:\n\nThe basics of tidyverse and its core packages\nData manipulation with dplyr functions\nData visualization with ggplot2\nApplied tidyverse concepts to analyze our books dataset\nBuilt a virtual slot machine using tidyverse functions\n\n\n\n\nThese skills form an essential foundation for working with data in R using the tidyverse. As we progress in our digital humanities journey, we’ll build upon these concepts to perform more complex data manipulations and analyses.\n\n\nTidyverse Basics\n\n\nData Manipulation\n\n\nData Visualization\n\n\nPractical Application",
    "crumbs": [
      "Book",
      "§5 Spinning the Reels 🎰"
    ]
  },
  {
    "objectID": "book/04-cards.html",
    "href": "book/04-cards.html",
    "title": "§4 Dealing With Cards 🃏",
    "section": "",
    "text": "Welcome to your second project with R! We’ll create a virtual deck of poker cards to explore more advanced R concepts.",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#numeric",
    "href": "book/04-cards.html#numeric",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.1 Numeric",
    "text": "1.1 Numeric\nWe’ve already seen this with our dice. It includes both integers and decimal numbers:\n\nmy_integer &lt;- 42\nmy_deci &lt;- 3.14\nclass(my_integer)\n\n[1] \"numeric\"\n\nclass(my_deci)\n\n[1] \"numeric\"",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#character",
    "href": "book/04-cards.html#character",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.2 Character",
    "text": "1.2 Character\nThis is used for text data, like words or sentences:\n\nmy_name &lt;- \"Shakespeare\"\nmy_quote &lt;- \"To be or not to be\"\nclass(my_name)\n\n[1] \"character\"\n\nclass(my_quote)\n\n[1] \"character\"",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#logical",
    "href": "book/04-cards.html#logical",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.3 Logical",
    "text": "1.3 Logical\nThis represents true/false values:\n\nis_sunny &lt;- TRUE\nis_raining &lt;- FALSE\nclass(is_sunny)\n\n[1] \"logical\"\n\nclass(is_raining)\n\n[1] \"logical\"",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#factor",
    "href": "book/04-cards.html#factor",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.4 Factor",
    "text": "1.4 Factor\nThis is used for categorical data, like genres in literature:\n\ngenres &lt;- factor(c(\"Poetry\", \"Prose\", \"Drama\"))\nclass(genres)\n\n[1] \"factor\"\n\n\n\n\n\n\n\n\nWhat is the Difference Between Factor and Character?\n\n\n\n\n\nCharacter and factor may seem similar, but they serve different purposes:\n\n\nCharacter\n\nThis is simply text data.\nIt can be any combination of letters, numbers, or symbols.\nExamples: \"apple\", \"banana\", \"cherry123\"\n\n\n\n\nFactor\n\nThis is for categorical data, made up of distinct levels or categories.\nIt’s used when data falls into a limited number of specific groups.\nExamples: Days of the week, book genres, or rating scales\n\n\n\nKey differences:\n\nCharacters can be any text, while factors consist of predefined levels or categories.\nEach level in a factor is assigned a unique integer behind the scenes, making factors more efficient for certain analyses.\nFactors are ideal for data with a fixed set of possible values or categories.",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#learning-check",
    "href": "book/04-cards.html#learning-check",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.5 Learning Check 🏁",
    "text": "1.5 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#hands-on-coding",
    "href": "book/04-cards.html#hands-on-coding",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n1.6 Hands-On Coding 💻",
    "text": "1.6 Hands-On Coding 💻\nTry the following:\n\nCreate a numeric variable called year_published and assign it the year your favorite book was published.\nCreate a character variable called book_title with the title of your favorite book.\nCreate a logical variable called is_fiction indicating whether your favorite book is fiction (TRUE) or non-fiction (FALSE).\nCreate a factor variable called book_genre with a few genres (e.g., “Mystery”, “Romance”, “Science Fiction”).\n\n\n\n# Year published\nyear_published &lt;- 1960\n# Book title\nbook_title &lt;- \"To Kill a Mockingbird\"\n# Fiction or non-fiction\nis_fiction &lt;- TRUE\n# Book genre\nbook_genre &lt;- factor(c(\"Mystery\", \"Romance\", \"Science Fiction\"))",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#accessing-data-in-a-data-frame",
    "href": "book/04-cards.html#accessing-data-in-a-data-frame",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n2.1 Accessing Data in a Data Frame",
    "text": "2.1 Accessing Data in a Data Frame\nYou can access specific parts of a data frame using various methods.\n\n\n\n\n\n\n$ and [row_number, column_number]\n\n\n\n\n\n\n\nColumns: Use $ followed by the column name (e.g., books$title).\n\nThis gives you all the values in that column.\n\n\n\nRows: Use [row_number, ] (e.g., books[2, ]).\n\nThe comma is important! It means “all columns”.\nThis gives you all the data in that specific row.\n\n\n\nIndividual Cells: Use [row_number, column_number] (e.g., books[1, 2]).\n\nThis gives you the value in a specific row and column.\n\n\n\n\n\n\n\n\n\n\n\n\nAcessing the books Data Frame\n\n\n\n\n\nData frames are essentially just spread sheet tables. Our books data frame looks like this:\n\n\ntitle\nauthor\nyear\n\n\n\n1984\nOrwell\n1949\n\n\nPride and Prejudice\nAusten\n1813\n\n\nThe Great Gatsby\nFitzgerald\n1925\n\n\n\n\n\nbooks$title would give: 1984, Pride and Prejudice, The Great Gatsby\n\n\nbooks[2, ] would give: Pride and Prejudice, Austen, 1813\n\n\nbooks[1, 2] would give: Orwell\n\n\n\n\n\n\n# Get a specific column\nbooks$title\n\n[1] \"1984\"                \"Pride and Prejudice\" \"The Great Gatsby\"   \n\n# Get a specific row\nbooks[2, ]\n\n                title author year\n2 Pride and Prejudice Austen 1813\n\n# Get a specific cell\nbooks[1, 2]\n\n[1] \"Orwell\"\n\n# Get a specific cell using row number and column name\nbooks[2, \"title\"] # Get the second title\n\n[1] \"Pride and Prejudice\"\n\n# Get the record based on a specific value\nbooks[books$author == \"Austen\", ] # Get all records by Austen\n\n                title author year\n2 Pride and Prejudice Austen 1813\n\n\n\n\n\n\n\n\nUnderstanding Nested Functions\n\n\n\n\n\nThe line books[books$author == \"Austen\", ] might look complex, but let’s break it down:\n\n\nbooks$author: This part gets the author column from our books data frame.\n\nbooks$author == \"Austen\": This compares each author name to “Austen”.\n\n\nIt creates a list of TRUE/FALSE values (TRUE where the author is Austen, FALSE otherwise).\nFor example, if we had 3 books and only the second was by Austen, this would give: [FALSE, TRUE, FALSE]\n\n\n\nbooks[...]: This is like asking R to look inside the books data frame.\n\nbooks[books$author == \"Austen\", ]: This combines steps 2 and 3.\n\n\nIt tells R: “From the books data frame, give me all rows where the author is Austen”.\nThe comma at the end means “give me all columns for these rows”.\n\nThink of it like a librarian (R) searching through a catalog (the data frame):\n\nYou ask: “Can you find all books by Austen?”\nThe librarian checks each book’s author (TRUE/FALSE for Austen).\nThen returns all information about the books that matched.\n\n\n\n\n\n\n\n\n\n\nFiltering Data Based on Conditions\n\n\n\n\n\nThis example demonstrates a powerful technique called “filtering”:\n\nCreating a condition: books$author == \"Austen\" creates a logical vector (TRUE/FALSE) for each row where the author is Austen.\nUsing the condition: We use this vector inside the square brackets [ ] to select only those rows that meet our condition.\nSelecting columns: The comma after the condition (books [condition, ]) means “select all columns for these rows”.\n\nThis method allows you to extract specific records based on any condition you specify. For example:\n\nFind all books published after 1900: books[books$year &gt; 1900, ]\nFind all books with “War” in the title: books[grepl(“War”, books$title), ]\n\nFiltering is a crucial skill in data manipulation and analysis, allowing you to focus on specific subsets of your data.",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#learning-check-1",
    "href": "book/04-cards.html#learning-check-1",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n2.2 Learning Check 🏁",
    "text": "2.2 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#hands-on-coding-1",
    "href": "book/04-cards.html#hands-on-coding-1",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n2.3 Hands-On Coding 💻",
    "text": "2.3 Hands-On Coding 💻\nLet’s apply our data frame skills to a new context: famous paintings. We’ll create a data frame of artworks and practice accessing its data in various ways.\nTry the following exercises:\n\nCreate a data frame called paintings with columns for title, artist, and year_created.\n\n\n\n\n\n\n\nExample Data\n\n\n\n\n\nFeel free to use your own favorite paintings for this exercise! If you prefer, you can use the following code:\n\npaintings &lt;- data.frame(\n    title = c(\"Mona Lisa\", \"The Starry Night\", \"The Persistence of Memory\"),\n    artist = c(\"Leonardo da Vinci\", \"Vincent van Gogh\", \"Salvador Dali\"),\n    year_created = c(1503, 1889, 1931)\n)\n\n\n\n\n\nAccess the ‘artist’ column of the data frame.\nGet the third row of the data frame.\nFind the year the second painting in the data frame was created.\n(Optional challenge) Add a new column called ‘style’ to the data frame (e.g., “Renaissance”, “Post-Impressionism”, “Surrealism”).\n\n\n\n# 1. Create the paintings data frame\npaintings &lt;- data.frame(\n    title = c(\"Mona Lisa\", \"The Starry Night\", \"The Persistence of Memory\"),\n    artist = c(\"Leonardo da Vinci\", \"Vincent van Gogh\", \"Salvador Dali\"),\n    year_created = c(1503, 1889, 1931)\n)\n# 2. Access the 'artist' column\npaintings$artist\n\n[1] \"Leonardo da Vinci\" \"Vincent van Gogh\"  \"Salvador Dali\"    \n\n# 3. Get the third row\npaintings[3, ]\n\n                      title        artist year_created\n3 The Persistence of Memory Salvador Dali         1931\n\n# 4. Find the year the second painting was created\npaintings[2, \"year_created\"]\n\n[1] 1889\n\n# 5. Add a new 'style' column\npaintings$style &lt;- c(\"Renaissance\", \"Post-Impressionism\", \"Surrealism\")\n# View the updated data frame\nprint(paintings)\n\n                      title            artist year_created              style\n1                 Mona Lisa Leonardo da Vinci         1503        Renaissance\n2          The Starry Night  Vincent van Gogh         1889 Post-Impressionism\n3 The Persistence of Memory     Salvador Dali         1931         Surrealism",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#create-the-deck",
    "href": "book/04-cards.html#create-the-deck",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n3.1 Create the Deck",
    "text": "3.1 Create the Deck\n\n\n\n\n\n\nStructure of a Deck\n\n\n\n\n\nRemember, a standard deck of cards consists of:\n\n4 suits: Hearts♥️, Diamonds♦️, Clubs♣️, and Spades♠️\n13 ranks in each suit: Ace, 2, 3, …, 10, Jack, Queen, King\nA total of 52 cards (4 suits × 13 ranks)\n\n\n\n\nCreate a data frame called deck with three columns: suit, rank, and value.\n\n\nsuit should contain all four suits, repeated 13 times each.\n\nrank should contain all 13 ranks, repeated for each suit.\n\nvalue should assign numeric values to the ranks (Ace = 1, Jack = 11, Queen = 12, King = 13, others as their numeric value).\n\n\n\n\n\n\n\n\nHint 1/2\n\n\n\n\n\nUse the rep() function to repeat values. For example: - rep(c(\"A\", \"B\"), each = 3) gives \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" - rep(c(\"A\", \"B\"), times = 3) gives \"A\" \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\n\n\n\n\n\n\n\n\n\nHint 2/2\n\n\n\n\n\nFor the suit column: rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = 13) For the rank column: rep(c(\"Ace\", 2:10, \"Jack\", \"Queen\", \"King\"), times = 4) For the value column: rep(1:13, times = 4)\n\n\n\n\n\n\ndeck &lt;- data.frame(\n    suit = rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = _),\n    rank = rep(c(\"Ace\", _, \"Jack\", \"Queen\", \"King\"), times = _),\n    value = rep(_, times = 4)\n)\n\n\n\n\ndeck &lt;- data.frame(\nsuit = rep(c(\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"), each = 13),\nrank = rep(c(\"Ace\", 2:10, \"Jack\", \"Queen\", \"King\"), times = 4),\nvalue = rep(1:13, times = 4)\n)\n# View the first few rows\nhead(deck)\n\n    suit rank value\n1 Hearts  Ace     1\n2 Hearts    2     2\n3 Hearts    3     3\n4 Hearts    4     4\n5 Hearts    5     5\n6 Hearts    6     6",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/04-cards.html#accessing-information",
    "href": "book/04-cards.html#accessing-information",
    "title": "§4 Dealing With Cards 🃏",
    "section": "\n3.2 Accessing Information",
    "text": "3.2 Accessing Information\nNow that we have our deck, let’s practice accessing information from it.\n\n3.2.1 How many cards are in the deck?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse nrow() to count the number of rows in a data frame.\n\n\n\n\n\n\nnrow(_)\n\n\n\n\nnrow(deck)\n\n[1] 52\n\n\n\n\n3.2.2 What are all the unique suits in the deck?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the unique() function on the suit column of the deck.\n\n\n\n\n\n\nunique(deck$_)\n\n\n\n\nunique(deck$suit)\n\n[1] \"Hearts\"   \"Diamonds\" \"Clubs\"    \"Spades\"  \n\n\n\n\n3.2.3 View high value cards\nView only the cards with a value greater than 10.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse logical indexing to filter the deck. The condition should be deck$value &gt; 10.\n\n\n\n\n\n\nhigh_value_cards &lt;- deck[deck$_ &gt; _, ]\nprint(high_value_cards)\n\n\n\n\nhigh_value_cards &lt;- deck[deck$value &gt; 10, ]\nprint(high_value_cards)\n\n       suit  rank value\n11   Hearts  Jack    11\n12   Hearts Queen    12\n13   Hearts  King    13\n24 Diamonds  Jack    11\n25 Diamonds Queen    12\n26 Diamonds  King    13\n37    Clubs  Jack    11\n38    Clubs Queen    12\n39    Clubs  King    13\n50   Spades  Jack    11\n51   Spades Queen    12\n52   Spades  King    13",
    "crumbs": [
      "Book",
      "§4 Dealing With Cards 🃏"
    ]
  },
  {
    "objectID": "book/01-introduction.html#digital-humanities-analysis-with-r",
    "href": "book/01-introduction.html#digital-humanities-analysis-with-r",
    "title": "§1 Introduction to Digital Humanities",
    "section": "\n2 Digital Humanities Analysis with R",
    "text": "2 Digital Humanities Analysis with R\nLet’s explore some fascinating examples of how computational methods can be applied to humanities questions.\n\n2.1 Example 1: Text Analysis of Literary Works\n\nCodelibrary(pacman)\np_load(tidyverse, tidytext, wordcloud, gutenbergr, scales, hrbrthemes)\n\n# Function to download and process text\n# gutenberg_works(title == \"The Age of Innocence\")\ninno &lt;- gutenberg_download(541)\n\n# Analyze word frequencies\nword_freq &lt;- inno %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  top_n(20, n)\n\n# Create a bar plot of word frequencies\np &lt;- ggplot(word_freq, aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"#69b3a2\", width = 0.7) +\n  geom_text(aes(label = n), hjust = -0.3, size = 3) +\n  coord_flip() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  labs(x = NULL, y = \"Frequency\", \n       title = \"Top 20 Most Frequent Words in The Age of Innocence\",\n       subtitle = \"After removing common stop words\",\n       caption = \"Source: Project Gutenberg\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 12, color = \"gray50\", margin = margin(b = 20)),\n    plot.caption = element_text(size = 10, color = \"gray50\", margin = margin(t = 10)),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    plot.margin = margin(20, 20, 20, 20)\n  )\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\nThis word cloud visualizes the most frequent words in The Age of Innocence, giving us a quick insight into common themes and vocabulary.\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\n\nWhat words stand out to you in this visualization?\nHow might this kind of analysis complement traditional close reading of literary works?\nWhat limitations might this approach have for understanding the author’s language?\n\n\n\n\n\n2.2 Example 2: Sentiment Analysis of Jane Austen’s Novels\n\nCodelibrary(janeaustenr)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(hrbrthemes)\n\n# Combine Austen's novels\nausten_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\", ignore_case = TRUE)))\n  ) %&gt;%\n  ungroup()\n\n# Perform sentiment analysis\nausten_sentiment &lt;- austen_books %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  inner_join(get_sentiments(\"bing\"), multiple=\"all\") %&gt;%\n  count(book, index = linenumber %/% 100, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\n# Plot sentiment over narrative time\nggplot(austen_sentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~book, ncol = 2, scales = \"free_x\") +\n  labs(\n    title = \"Sentiment Analysis of Jane Austen's Novels\",\n    subtitle = \"Emotional trajectory throughout the narrative\",\n    x = \"Narrative Time\",\n    y = \"Sentiment Score\",\n    caption = \"Data: janeaustenr package | Analysis: tidytext\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 12),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 14, color = \"gray50\"),\n    plot.caption = element_text(size = 10, color = \"gray50\"),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    plot.margin = margin(20, 20, 20, 20)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(expand = expansion(mult = c(0.1, 0.1)))\n\n\n\n\n\n\nCode# Display summary statistics\nausten_summary &lt;- austen_sentiment %&gt;%\n  group_by(book) %&gt;%\n  summarize(\n    mean_sentiment = mean(sentiment),\n    max_sentiment = max(sentiment),\n    min_sentiment = min(sentiment)\n  )\n\nknitr::kable(austen_summary, caption = \"Summary Statistics of Sentiment Analysis\", digits = 2)\n\n\nSummary Statistics of Sentiment Analysis\n\nbook\nmean_sentiment\nmax_sentiment\nmin_sentiment\n\n\n\nSense & Sensibility\n9.94\n51\n-26\n\n\nPride & Prejudice\n10.69\n42\n-31\n\n\nMansfield Park\n12.47\n61\n-45\n\n\nEmma\n14.40\n48\n-31\n\n\nNorthanger Abbey\n9.19\n47\n-33\n\n\nPersuasion\n15.14\n59\n-13\n\n\n\n\n\nThis visualization shows the emotional trajectory of Jane Austen’s novels over their narrative time.\n\n\n\n\n\n\nDiscussion: Narrative Emotions\n\n\n\n\n\n\nWhat patterns do you notice in the emotional arcs of Austen’s novels?\nHow might this type of analysis enhance our understanding of narrative structure?\nWhat challenges might arise in applying sentiment analysis to historical texts?\n\n\n\n\n\n2.3 Example 3: Network Analysis of “Empresses in the Palace” Characters\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n\n# Add Noto Sans CJK font\nfont_add_google(\"Noto Sans SC\", \"Noto Sans SC\")\nshowtext_auto()\n\n\n# Read the JSON data\nrelation_data &lt;- fromJSON(\"../book/data/relation.json\")\n\nrelation_data$nodes &lt;- relation_data$nodes %&gt;% rename(Bio=\"角色描述\")\n\nrelation_data$nodes &lt;- relation_data$nodes%&gt;%\nmutate(Alliance = ifelse(Alliance==\"皇后阵容\",\"皇后阵营\",Alliance))\n\n# Ensure that the 'source' and 'target' in edges match the 'ID' in nodes\nrelation_data$edges$source &lt;- as.character(relation_data$edges$source)\nrelation_data$edges$target &lt;- as.character(relation_data$edges$target)\nrelation_data$nodes$ID &lt;- as.character(relation_data$nodes$ID)\nedges &lt;- relation_data$edges %&gt;% select(-Relationship)\n# Set the ID as the row names for the nodes dataframe\nrownames(relation_data$nodes) &lt;- relation_data$nodes$ID\n\n# Create the directed graph\nempresses_graph &lt;- graph_from_data_frame(d = edges,\n                                         vertices = relation_data$nodes$ID,\n                                         directed = TRUE)\n\n# Set node attributes\nV(empresses_graph)$Alliance &lt;- relation_data$nodes$Alliance\nV(empresses_graph)$Label &lt;- relation_data$nodes$Label\n\n# Create a color palette for alliances\nalliance_colors &lt;- c(\n  \"皇室成员\" = \"#4E79A7\",  # Royal Blue\n  \"皇后阵营\" = \"#F28E2B\",  # Warm Orange\n  \"甄嬛阵营\" = \"#E15759\",  # Soft Red\n  \"华妃阵营\" = \"#76B7B2\"   # Teal\n)\n\n# Calculate node size based on total degree centrality (in + out)\nV(empresses_graph)$size &lt;- degree(empresses_graph, mode = \"total\") * 0.5 + 3\nE(empresses_graph)$Relationship &lt;- relation_data$edges$Relationship\n\n# Plot the network\nset.seed(123) # for reproducibility\nplot &lt;- ggraph(empresses_graph, layout = \"fr\") +\n  geom_edge_link0(aes(edge_color = Relationship), \n                  arrow = arrow(length = unit(0.2, \"inches\"),\n                  ends = \"last\", type = \"closed\"), \n                  show.legend = FALSE, \n                  width = 1) +\n  geom_node_point(aes(color = Alliance, size = size), \n                  shape = 20, show.legend = FALSE) +\n  geom_node_text(aes(label = Label), repel = TRUE, size = 6) +\n  scale_color_manual(values = alliance_colors) +\n  scale_edge_colour_discrete() +\n  scale_size_continuous(range = c(5, 20)) +\n  theme_void() +\n  labs(title = \"Character Network in 'Empresses in the Palace'\",\n       subtitle = \"Directed relationships between key figures in the Chinese drama\") +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 14),\n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 10, face = \"bold\")\n  )\n\n# Plot\nplot\n\n\n\n\n\n\n\nThis visualization shows the complex network of relationships between characters in the Chinese drama “Empresses in the Palace” (甄嬛传).\n\n\n\n\n\n\nDiscussion: Character Networks in Historical Dramas\n\n\n\n\n\n\nWhat insights can we gain from this network visualization? What information is presented in the plot?\nWhich characters appear to be central to the network? How might their positions reflect their importance in the narrative?\nHow could this type of analysis complement traditional literary analysis of historical dramas or novels?\nWhat limitations might this network analysis have in representing the complex relationships and dynamics of the story?\n\n\n\n\nLet’s verify our impressions!\n\nView Analyses\n\n\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n# Create dummy data for legends\nalliance_data &lt;- data.frame(\n  Alliance = levels(factor(V(empresses_graph)$Alliance)),\n  x = 1,\n  y = 1:length(levels(factor(V(empresses_graph)$Alliance)))\n)\n\nsize_data &lt;- data.frame(\n  size = c(5, 10, 15, 20),\n  x = 1,\n  y = 1:4\n)\n\nrelationship_data &lt;- data.frame(\n  Relationship = levels(factor(E(empresses_graph)$Relationship)),\n  x = 1,\n  y = 1:length(levels(factor(E(empresses_graph)$Relationship)))\n)\n\n# Create separate legends\nnode_legend &lt;- cowplot::get_legend(\n  ggplot() +\n    geom_point(data = alliance_data, aes(x, y, color = Alliance), size = 5) +\n    geom_point(data = size_data, aes(x, y, size = size), color = \"black\") +\n    scale_color_manual(values = alliance_colors, name = \"Alliance\") +\n    scale_size_continuous(range = c(5, 20), name = \"Connections\") +\n    guides(\n      color = guide_legend(override.aes = list(size = 5)),\n      size = guide_legend(override.aes = list(color = \"steelblue\"))\n    ) +\n    theme(\n      legend.background = element_blank(),\n      legend.key = element_blank(),\n      legend.spacing.y = unit(0.5, \"cm\")\n    )\n)\n\nedge_legend &lt;- cowplot::get_legend(\n  ggplot(relationship_data, aes(x, y, color = Relationship)) +\n    geom_segment(aes(x = 0, xend = 1, yend = y),\n                 arrow = arrow(length = unit(0.2, \"inches\"), type = \"closed\")) +\n    scale_color_discrete(name = \"Relationship\") +\n    theme(\n      legend.background = element_blank(),\n      legend.key = element_blank(),\n      legend.spacing.y = unit(0.2, \"cm\"),\n      legend.text = element_text(margin = margin(r = 15))\n    )\n)\n\n# Combine plot and legends\ncombined_legend &lt;- plot_grid(\n  node_legend, \n  edge_legend, \n  ncol = 1, \n  rel_heights = c(1, 1.5),\n  align = 'v',\n  axis = 'l'\n)\n\n# Combine main plot and legends\nfinal_plot &lt;- plot_grid(\n  plot, \n  combined_legend,\n  rel_widths = c(5, 2),\n  align = 'h',\n  axis = 'tb'\n)\n\n# Display the final plot\nfinal_plot\n\n\n\n\n\n\n\n\nCodelibrary(pacman)\np_load(jsonlite,igraph,ggraph,tidyverse,showtext,ggrepel,cowplot)\n\n# Display summary statistics\ncharacter_summary &lt;- tibble(\n  Alliance = names(table(V(empresses_graph)$Alliance)),\n  Count = as.numeric(table(V(empresses_graph)$Alliance))\n)%&gt;%\n  arrange(desc(Count)) \n\nknitr::kable(character_summary, caption = \"Summary of Character Alliances\")\n\n\nSummary of Character Alliances\n\nAlliance\nCount\n\n\n\n甄嬛阵营\n22\n\n\n皇后阵营\n12\n\n\n华妃阵营\n6\n\n\n皇室成员\n4\n\n\n\n\nCode# Calculate and display top 5 characters by total degree centrality\ntop_characters &lt;- tibble(\n  Character = V(empresses_graph)$Label,\n  InDegree = degree(empresses_graph, mode = \"in\"),\n  OutDegree = degree(empresses_graph, mode = \"out\"),\n  TotalDegree = degree(empresses_graph, mode = \"total\")\n) %&gt;%\n  arrange(desc(TotalDegree)) %&gt;%\n  left_join(.,relation_data$nodes%&gt;%select(Label,Title,Alliance,Bio),by=(c(\"Character\"=\"Label\")))%&gt;%\n  slice_head(n = 5)\n\nknitr::kable(top_characters, caption = \"Top 5 Characters by Total Connections\")\n\n\nTop 5 Characters by Total Connections\n\n\n\n\n\n\n\n\n\n\nCharacter\nInDegree\nOutDegree\nTotalDegree\nTitle\nAlliance\nBio\n\n\n\n甄嬛\n16\n22\n38\n皇贵妃\n甄嬛阵营\n因容貌酷似已逝的纯元皇后被选中，一直蒙受恩宠。对皇帝真心实意。因为在后宫算计中受到影响，一步步变得城府至深，结果却是她的一厢情愿，离开后宫，遇到了果郡王，心心相爱。后来设计回宫。经历了后宫无数的明争暗斗，勾心斗角，害死皇帝，扶养子登位，终于成为太后。\n\n\n雍正\n13\n18\n31\n皇上\n皇室成员\n九龙夺嫡之后终于登上了宝座。在他的眼中任何人都可以成为他巩固江山的棋子。纯元皇后去世多年，痴情的皇帝对她依然念念不忘，于是对酷似纯元皇后的甄嬛愈加宠爱，渐渐超过了华妃。甄嬛回宫，皇上对其极其信任，百依百顺。但最后还是被枕边人算计。\n\n\n宜修\n8\n11\n19\n皇后\n皇后阵营\n纯元皇后的庶出妹妹。外表稳重端庄，但擅于隐忍，城府之深，极爱后位，后宫妃子皆为她的棋子。利用甄嬛打败了华妃，继而将甄嬛赶出宫外。甄嬛回宫后继续步步为营，但最后因害死纯元皇后的事情东窗事发，在太后的力保之下没被废后，但永生禁足于景仁宫，二人死生不复相见，最后新帝登基，心悸而死。\n\n\n允礼\n7\n7\n14\n果亲王\n皇室成员\n果郡王不同于宫中争权夺势的皇室子弟，他庆幸自己并非帝王之身，不必将朝政琐事萦绕于身，他不求娇妻美妾如云，只盼与爱人厮守到老。果郡王爱着甄嬛，却又不能僭越宫廷之礼。他善良、痴情、风流倜傥，终其一生都是在寻求一份爱，奋不顾身地保护一份爱，说他是为爱而生也不为过。\n\n\n年世兰\n6\n7\n13\n正三品贵嫔\n华妃阵营\n大将军的妹妹，仗着身世和宠爱，对其他的妃嫔不择手段，但本性并不坏，因为孩子被打掉而痛恨端妃。甄嬛在其打入冷宫后还不能怀孕的事实告诉她，绝望的华妃撞墙而死。最终她被追封为敦肃皇贵妃。",
    "crumbs": [
      "Book",
      "§1 Introduction to Digital Humanities"
    ]
  },
  {
    "objectID": "book/01-introduction.html#conclusion",
    "href": "book/01-introduction.html#conclusion",
    "title": "§1 Introduction to Digital Humanities",
    "section": "\n3 Conclusion",
    "text": "3 Conclusion\nThese demonstrations showcase just a few of the exciting possibilities that digital humanities offers for exploring and analyzing humanities data. By combining computational methods with traditional humanities scholarship, we can uncover new patterns, ask novel questions, and gain fresh insights into cultural and historical materials.\n\n\n\n\n\n\nFinal Reflection\n\n\n\n\n\nConsider the examples we’ve explored today:\n\nWhich technique (word frequency analysis, sentiment analysis, or network analysis) do you find most intriguing? Why?\nCan you think of a humanities question or topic from your own interests that might benefit from one of these computational approaches?\n\nShare your thoughts with a partner or the class if time allows.",
    "crumbs": [
      "Book",
      "§1 Introduction to Digital Humanities"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Introduction to Digital Humanities! We will cover several key topics in digital humanities and R programming. Each topic will be covered in theoretical/conceptual lectures, followed by practical sessions.\nHere’s a breakdown of our semester:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\n9.9\nOrientation: Introduction to Digital Humanities\n\n\n2-6\n9.14, 9.23, 9.30, 10.7 (holiday), 10.14\nR Programming Basics\n\n\n7-8\n10.21, 10.28\nQuantitative Text Analysis Basics\n\n\n8-9\n10.28, 11.4\nSentiment Analysis\n\n\n10-12\n11.11, 11.18, 11.25\nTopic Modelling\n\n\n13-15\n12.2, 12.9, 12.16\nSocial Network Analysis\n\n\n16\n12.23\nEthics, AI and Future of Digital Humanities\n\n\n17-18\nExam Weeks\nFinal Assessment\n\n\n\n\n\n\n\n\n\nSchedule Subject to Change\n\n\n\nPlease note that this schedule is tentative and may be adjusted as needed throughout the semester to accommodate our learning progress and any unforeseen circumstances."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Introduction to Digital Humanities! We will cover several key topics in digital humanities and R programming. Each topic will be covered in theoretical/conceptual lectures, followed by practical sessions.\nHere’s a breakdown of our semester:\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\n\n\n\n\n1\n9.9\nOrientation: Introduction to Digital Humanities\n\n\n2-6\n9.14, 9.23, 9.30, 10.7 (holiday), 10.14\nR Programming Basics\n\n\n7-8\n10.21, 10.28\nQuantitative Text Analysis Basics\n\n\n8-9\n10.28, 11.4\nSentiment Analysis\n\n\n10-12\n11.11, 11.18, 11.25\nTopic Modelling\n\n\n13-15\n12.2, 12.9, 12.16\nSocial Network Analysis\n\n\n16\n12.23\nEthics, AI and Future of Digital Humanities\n\n\n17-18\nExam Weeks\nFinal Assessment\n\n\n\n\n\n\n\n\n\nSchedule Subject to Change\n\n\n\nPlease note that this schedule is tentative and may be adjusted as needed throughout the semester to accommodate our learning progress and any unforeseen circumstances."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "Syllabus",
    "section": "📊 Assessment",
    "text": "📊 Assessment\nYour final grade will be composed of the following elements:\n\nMonthly Quiz 📝: 30%\n\nRegular assessments to check your understanding of the course material. (微助教/问卷星)\n\nClass Participation and Attendance 🙋‍♀️: 30%\n\nYour active involvement in class discussions and activities is crucial! (微助教/Leaderboard)\n\n\n\n\n\n\n\nImportant\n\n\n\n\nAbsence of one-third of class time, excused or unexcused, will disqualify you from earning credits for the course.\n\n\n\nTerm Paper 📄: 40%\n\n\n\n\n\n\n🎯 Term Project\n\n\n\nA digital humanities project of your choice! You should pick your topic for the term project by week 12. More details will be released later when you have learned R basics."
  },
  {
    "objectID": "syllabus.html#important-pep-talk",
    "href": "syllabus.html#important-pep-talk",
    "title": "Syllabus",
    "section": "🚀 Important Pep Talk!",
    "text": "🚀 Important Pep Talk!\n\n\n\n\n\n\n💪 Setting the Right Mindset\n\n\n\nRemember these key points while you embark on your DH journey:\n\n\n\nEmbrace the Learning Curve\n\nLearning digital tools and methods is like learning a new language—it takes time and practice.\nDon’t be discouraged by initial difficulties; everyone faces challenges when starting out.\nCelebrate small victories and progress along the way.\n\n\n\nFrustration is Part of Growth\n\nFeeling frustrated? Congratulations! It means you’re pushing your boundaries and learning.\nHadley Wickham (Father of modern R syntax) once said: “Everyone gets frustrated… I still get frustrated occasionally when writing R code. It’s just a natural part of programming.”\nTake breaks when needed, but don’t give up. Return with fresh eyes and renewed determination.\n\n\n\nCultivate Curiosity\n\nApproach new concepts with an open and inquisitive mind.\nAsk questions, explore beyond the course material, and let your curiosity drive your learning.\nDigital humanities is a field of endless possibilities—embrace the excitement of discovery.\n\n\n\nBuild Resilience\n\nExpect challenges and setbacks—they’re not roadblocks, but opportunities to problem-solve.\nDevelop a growth mindset: view difficulties as chances to improve, not as personal shortcomings.\nRemember: every expert was once a beginner. Persistence is key to mastery.\n\n\n\n\n\n\n\nRemember\n\n\n\nI promise you can succeed in this class as long as you put your heart to it. Learning new digital skills may seem daunting at first, but with patience, persistence, and the right mindset, you’ll be amazed at what you can achieve. Don’t hesitate to reach out for help when you need it—we’re all in this learning journey together!"
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Monthly Quiz",
    "section": "",
    "text": "Instructions for Students\nWelcome to the monthly quiz! 📝\nPlease read the following instructions carefully:\n\n📋 Enter your full name exactly as it appears in the course registration system (请使用与数北选课系统完全一致的姓名).\n⏱️ Important: After clicking ‘Submit’ at the end of the quiz, you cannot retake or resubmit the quiz.\n🌐 Ensure you have a stable internet connection before starting the quiz.\n🔒 Do not close the browser window during the quiz.\n🆘 If you encounter any technical issues, please contact Meng via WeChat.\n\nGood luck! 🍀\n\n\n\n\nStudent Information\n\nName:  Enter your name exactly as it appears in the course registration system (请填写与数北选课系统一致的姓名).   \n\n\nStart Quiz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformation about the Quiz\n📝 Quiz Format:\n\n10 questions in total\nAll questions are taken from the course textbook\nMultiple choice format\n\n⏰ Quiz Schedule:\n\nThe quiz is currently closed\nIt will be open from \nYou can submit any time during this period\nYou can only submit the quiz once\n\n📚 Content Coverage:\n\n\n\nGood luck and happy studying! 🍀📚"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities",
    "href": "book/slides/s-intro.html#what-is-digital-humanities",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-1",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-1",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨\nWrite down three keywords that come to mind."
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-2",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-2",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨\n🤔 What patterns do you see in the word cloud? Any suprising entries?"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-3",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-3",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-4",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-4",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-5",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-5",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-6",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-6",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-7",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-7",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨"
  },
  {
    "objectID": "book/slides/s-intro.html#what-is-digital-humanities-8",
    "href": "book/slides/s-intro.html#what-is-digital-humanities-8",
    "title": "Introduction to Digital Humanities",
    "section": "What is digital humanities?🤨",
    "text": "What is digital humanities?🤨\n\n“Digital humanities work is done at the intersection of computational methods and humanities materials.”\n– Johanna Drucker, 2021 📚\n\n\n🔀 Intersection of computation and humanities\n🔗 Combines traditional disciplines with digital tools\n🤝 Collaborative and interdisciplinary\n🔄 Iterative and public-facing\n🔓 Open and accessible"
  },
  {
    "objectID": "book/slides/s-intro.html#components-of-dh-projects",
    "href": "book/slides/s-intro.html#components-of-dh-projects",
    "title": "Introduction to Digital Humanities",
    "section": "Components of DH Projects",
    "text": "Components of DH Projects\n\n📁 MATERIALS\n⚙️ PROCESSING\n🎨 PRESENTATION"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-materials",
    "href": "book/slides/s-intro.html#dh-materials",
    "title": "Introduction to Digital Humanities",
    "section": "DH Materials",
    "text": "DH Materials\n\n💾 Digital or digitized humanities sources\n🖼️ Examples: texts, images, audio, video\n📈 Often large-scale datasets"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-processing",
    "href": "book/slides/s-intro.html#dh-processing",
    "title": "Introduction to Digital Humanities",
    "section": "DH Processing",
    "text": "DH Processing\n\n🧮 Computational methods and tools\n📊 Data analysis and modeling\n🤖 Machine learning and AI applications"
  },
  {
    "objectID": "book/slides/s-intro.html#dh-presentation",
    "href": "book/slides/s-intro.html#dh-presentation",
    "title": "Introduction to Digital Humanities",
    "section": "DH Presentation",
    "text": "DH Presentation\n\n📊 Visualizations\n🗄️ Digital archives\n🖱️ Interactive platforms\n🗃️ Databases"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\n\n🍿 Watch a video about the project"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-1",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-1",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nWhat it is\n\n🌐 One of the largest Digital Humanities initiatives worldwide\n🏛️ Stanford-based project with international collaborations\n🚀 Groundbreaking effort in historical data visualization"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-2",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-2",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nWhat it does\n\n🗺️ Maps and analyzes intellectual networks of the 17th-18th century Enlightenment\n✉️ Visualizes over 200,000 letters exchanged by key historical figures\n🔍 Reveals patterns of knowledge exchange across Europe and the Americas"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-3",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-3",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nCan you identify the core components of this DH project? 🧐 (quick learning check!)\n\n📁 MATERIALS\n⚙️ PROCESSING\n🎨 PRESENTATION"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-4",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-4",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\nCan you identify the core components of this DH project? 🧐\n\n📁 MATERIALS: Letters and correspondence of Enlightenment thinkers\n⚙️ PROCESSING: Network analysis, geospatial mapping\n🎨 PRESENTATION: Interactive visualizations of intellectual networks"
  },
  {
    "objectID": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-5",
    "href": "book/slides/s-intro.html#example-mapping-the-republic-of-letters-5",
    "title": "Introduction to Digital Humanities",
    "section": "Example: Mapping the Republic of Letters",
    "text": "Example: Mapping the Republic of Letters\n🧐 How is the project different from a traditional humanities project? Discuss in pairs to identify key differences between traditional humanities and DH."
  },
  {
    "objectID": "book/slides/s-intro.html#digital-humanities-vs-traditional-humanities",
    "href": "book/slides/s-intro.html#digital-humanities-vs-traditional-humanities",
    "title": "Introduction to Digital Humanities",
    "section": "Digital Humanities vs Traditional Humanities",
    "text": "Digital Humanities vs Traditional Humanities\n\n\nTraditional Humanities:\n\n🔎 Limited by human reading capacity\n🧑‍🏫 Often individual research\n📚 Primarily books and articles\n🖋️ Close reading and interpretation\n🏛️ Focus on canonical works\n🗂️ Manual data collection and analysis\n🌐 Geographically limited research scope\n🕰️ Time-intensive archival research\n📊 Limited quantitative analysis\n🎭 Emphasis on qualitative methods\n\n\nDigital Humanities:\n\n📊 Handles large datasets and corpora\n👥 Often team-based, interdisciplinary projects\n🖥️ Diverse digital outputs (visualizations, databases, etc.)\n🔍 Distant reading and pattern recognition\n📜 Inclusion of non-canonical texts and artifacts\n🤖 Automated data collection and processing\n🌍 Global reach through digital archives\n⚡ Rapid processing of vast information\n📈 Advanced statistical and computational analysis\n🔀 Integration of quantitative and qualitative methods"
  },
  {
    "objectID": "book/slides/s-intro.html#reflection",
    "href": "book/slides/s-intro.html#reflection",
    "title": "Introduction to Digital Humanities",
    "section": "Reflection",
    "text": "Reflection\n💡 Does humanities need computational methods?\n🧑 Are we losing the “human” in digital humanities?\n⚖️ How might DH methods transform your area of study?\nDiscuss with a partner, then share with the class."
  },
  {
    "objectID": "book/02-R.html",
    "href": "book/02-R.html",
    "title": "§2 Getting Started With R",
    "section": "",
    "text": "We will do all of your analysis with the open source (and free!) programming language R. We will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Book",
      "§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#r-and-rstudio",
    "href": "book/02-R.html#r-and-rstudio",
    "title": "§2 Getting Started With R",
    "section": "",
    "text": "We will do all of your analysis with the open source (and free!) programming language R. We will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.",
    "crumbs": [
      "Book",
      "§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#online-help",
    "href": "book/02-R.html#online-help",
    "title": "§2 Getting Started With R",
    "section": "2 Online Help",
    "text": "2 Online Help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and Posit Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Mastodon or Twitter/X, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nChatGPT is generally relatively okay with R questions, but it may hallucinate responses—it sometimes imagine packages that don’t exist and then give you answers with them. Think of it as glorified autocomplete. I’d encourage you to read the first few paragraphs here about a few important things to keep in mind when using ChatGPT, though.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work will deal with {ggplot2}, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).",
    "crumbs": [
      "Book",
      "§2 Getting Started With R"
    ]
  },
  {
    "objectID": "book/02-R.html#installation-guide",
    "href": "book/02-R.html#installation-guide",
    "title": "§2 Getting Started With R",
    "section": "3 Installation Guide",
    "text": "3 Installation Guide\n\n3.1 Install R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network) website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows: \n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.4.1) and download it.\n\n\n\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it. \n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\n3.2 Install RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it: \nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\n3.3 FAQs 常见安装问题\n\n打开RStudio，出现弹窗让我选择R的版本（如下图）:  这说明你的R的安装路径无法被RStudio自动找到，需要点击browse按钮手动选择R.exe的位置： 请找到并点击R安装文件夹(如文件名为R 4.4.1的文件夹)中的bin文件夹，即可看到R.exe，选中即可。\n报错消息中说缺少RTools: 请到RTools网站下载并安装RTools https://cran.r-project.org/bin/windows/Rtools/\n\n\n\n3.4 Install tidyverse\nR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter. \nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including {ggplot2}) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed. \nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n\n\n3.5 Install tinytex (Optional, this is for rendering PDF files)\nWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named {tinytex} that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install {tinytex} so you can knit to pretty PDFs:\n\nUse the Packages in panel in RStudio to install {tinytex} like you did above with {tidyverse}. Alternatively, run install.packages(\"tinytex\") in the console.\nRun tinytex::install_tinytex() in the console.\nWait for a bit while R downloads and installs everything you need.\nThe end! You should now be able to knit to PDF.\n\nCredit: this installation guide was adpated from the guide by Dr Andrew Heiss at Georgia State University.",
    "crumbs": [
      "Book",
      "§2 Getting Started With R"
    ]
  },
  {
    "objectID": "backup/exam.html",
    "href": "backup/exam.html",
    "title": "Monthly Quiz",
    "section": "",
    "text": "Instructions for Students\nWelcome to the monthly quiz! 📝 Please read the following instructions carefully:\n\n📋 Enter your name exactly as it appears in the course registration system (请填写与数北选课系统一致的姓名).\n📅 Select your class day from the dropdown menu.\n⏱️ You cannot pause or restart the quiz once it has begun.\n🌐 Ensure you have a stable internet connection before starting.\n🆘 If you encounter any technical issues, please contact your instructor immediately.\n\nGood luck! 🍀\n\n\n\n\nStudent Information\n\nName:  Enter your name exactly as it appears in the course registration system (请填写与数北选课系统一致的姓名). \n\n\nStart Quiz\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow Analysis"
  },
  {
    "objectID": "book/index.html",
    "href": "book/index.html",
    "title": "Preface",
    "section": "",
    "text": "This is the textbook for the course Introduction to Digital Humanities taught by Meng Liu, at the School of English and International Studies, Beijing Foreign Studies University.\nIntroduction to Digital Humanities explores the intersection of humanities scholarship and digital methodologies. Students will learn how computational techniques can enhance humanities research, covering key concepts in digital humanities, modern R programming, and various analytical methods including quantitative text analysis, topic modeling, sentiment analysis, and social network analysis. Through hands-on application of digital tools to humanities data, students will develop both technical skills and critical thinking abilities. The course progresses from foundational programming to advanced analytical techniques, concluding with discussions on ethics and AI in humanities scholarship. By the end of the course, students will be equipped with a versatile toolkit for innovative, data-driven humanities research, suitable for those seeking to expand their methodological approach to cultural studies.\nThe content of the book is updated on a regular basis as the course progresses.",
    "crumbs": [
      "Book",
      "Preface"
    ]
  },
  {
    "objectID": "book/index.html#introduction-and-welcome-slides",
    "href": "book/index.html#introduction-and-welcome-slides",
    "title": "Preface",
    "section": "Introduction and Welcome Slides",
    "text": "Introduction and Welcome Slides",
    "crumbs": [
      "Book",
      "Preface"
    ]
  },
  {
    "objectID": "book/slides/s-preface.html#course-overview",
    "href": "book/slides/s-preface.html#course-overview",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\n🎒 Course design:\n\nWith a more pratical focus on DH methods using R\nLectures followed by practical sessions with R\nBring your laptop 💻\n\n🏠 Course platform:\n\n💬 Wechat: notifications + live chats\n🛜 Course website: course materials/code scripts"
  },
  {
    "objectID": "book/slides/s-preface.html#why-focus-on-dh-methods",
    "href": "book/slides/s-preface.html#why-focus-on-dh-methods",
    "title": "Introduction to Digital Humanities",
    "section": "Why Focus on DH Methods?",
    "text": "Why Focus on DH Methods?\n\n🌉 Common ground: Methods bridge diverse humanities disciplines\n🔧 Transferable skills: Applicable across various research areas\n🚀 Future-proofing: Open the door to coding/programming to prepare for the AI age\n🧠 Critical thinking: Enhance analytical skills for all humanities\n💡 Computational thinking: Develop a sense of data and computational thinking"
  },
  {
    "objectID": "book/slides/s-preface.html#course-overview-1",
    "href": "book/slides/s-preface.html#course-overview-1",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\n🎯 Learning outcomes:\n\nUnderstand key DH concepts and methodologies\nDevelop basic programming skills in R\nApply digital methods to humanities research questions\nCritically evaluate DH projects and their implications"
  },
  {
    "objectID": "book/slides/s-preface.html#getting-ready",
    "href": "book/slides/s-preface.html#getting-ready",
    "title": "Introduction to Digital Humanities",
    "section": "Getting Ready",
    "text": "Getting Ready\n\n🏠 Tour the website\n💬 Create wechat group\n🤳 Join 微助教班级"
  },
  {
    "objectID": "book/slides/s-preface.html#about-you",
    "href": "book/slides/s-preface.html#about-you",
    "title": "Introduction to Digital Humanities",
    "section": "About You",
    "text": "About You"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Digital Humanities",
    "section": "",
    "text": "Introduction to Digital Humanitites 数字人文导论 • 2024 Fall 秋\nSEIS 英语学院 • BFSU 北京外国语大学\n\n\n\n\n\nDr Meng Liu 刘梦\n\n🏢 Office: SEIS 202\n🕰️ Office Hours: Wednesday 3:20-4:20 pm | SEIS 202\n📧 Email: mengliu@bfsu.edu.cn\n🌐 Website: mengliu.info\n\n\n\n\n\n🗓️ Class Days/Times: Monday/Wednesday 13:30 - 15:05\n🏛️ Format: In-person\n📍 Location: 逸夫二阶\n\n\n\n\n\n💬 Wechat (group)\n📧 Email\n\n\n\n\n\n\n\n平台1：微信\n\n课堂互动：微助教\n课后沟通及消息发布：微信群\n\n平台2：课程网站\n\n课堂资料信息\n\n\n\n\n\n\nAnonymous Feedback Form"
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Introduction to Digital Humanities",
    "section": "",
    "text": "Introduction to Digital Humanitites 数字人文导论 • 2024 Fall 秋\nSEIS 英语学院 • BFSU 北京外国语大学\n\n\n\n\n\nDr Meng Liu 刘梦\n\n🏢 Office: SEIS 202\n🕰️ Office Hours: Wednesday 3:20-4:20 pm | SEIS 202\n📧 Email: mengliu@bfsu.edu.cn\n🌐 Website: mengliu.info\n\n\n\n\n\n🗓️ Class Days/Times: Monday/Wednesday 13:30 - 15:05\n🏛️ Format: In-person\n📍 Location: 逸夫二阶\n\n\n\n\n\n💬 Wechat (group)\n📧 Email\n\n\n\n\n\n\n\n平台1：微信\n\n课堂互动：微助教\n课后沟通及消息发布：微信群\n\n平台2：课程网站\n\n课堂资料信息\n\n\n\n\n\n\nAnonymous Feedback Form"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Introduction to Digital Humanities",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n🌱 Foundations\n\nDigital Humanities Overview\n\n\n\n🛠️ Core DH Techniques\n\nR Programming Basics\nQuantitative Text Analysis\n\n\n\n🚀 Specialized Topics\n\nTopic Modelling\nSentiment Analysis\nSocial Network Analysis"
  },
  {
    "objectID": "toolbox.html#poker-card-ballot-drawer",
    "href": "toolbox.html#poker-card-ballot-drawer",
    "title": "Toybox",
    "section": "🃏 Poker Card Ballot Drawer",
    "text": "🃏 Poker Card Ballot Drawer"
  },
  {
    "objectID": "toolbox.html#real-time-feedback-system",
    "href": "toolbox.html#real-time-feedback-system",
    "title": "Toybox",
    "section": "🚥 Real-Time Feedback System",
    "text": "🚥 Real-Time Feedback System\n\nReal-Time Personal Status ✅⏳🆘\nThe floating traffic light 🚥 in the bottom-right corner is your personal status indicator. Here’s how to use it:\n\nHover over the traffic light icon to expand it.\nEnter your name when using it for the first time. You only need to input it once. 请按数北选课系统的姓名填写\nClick on one of the three status icons to update your status:\n\n✅ “Got it! 😎” - You’re understanding and progressing well.\n⏳ “Hold on… 🤔” - You need a moment to process or feel a little bit confused.\n🆘 “Stuck…Help! 😵‍💫” - You’re stuck and need assistance.\n\n\nRemember to update it when your status changes!\n\n\nReal-Time Class Status Overview 📊\nThe floating chart 📊 in the bottom-left corner shows the overall class status:\n\nHover over the chart icon to expand it.\nIt displays the percentage of students in each status.\nThe bars and percentages update automatically every 10 seconds during class time.\nYou can see when the data was last updated at the bottom of the chart.\n\nThis tool helps all of us understand the overall class progress in real-time."
  },
  {
    "objectID": "toolbox.html#quiz-analysis",
    "href": "toolbox.html#quiz-analysis",
    "title": "Toybox",
    "section": "📊 Quiz Analysis",
    "text": "📊 Quiz Analysis\n\n\nQuiz Analysis Login\n\n Admin Login\n\n\n\nQuiz Analysis Controls\n\nRun Quiz Analysis Hide Quiz Analysis Show Missing Quiz Takers Hide Missing Quiz Takers"
  },
  {
    "objectID": "book/03-dice.html",
    "href": "book/03-dice.html",
    "title": "§3 Rolling the Dice 🎲",
    "section": "",
    "text": "Welcome to your first hands-on project with R! We’ll create a virtual pair of dice to learn fundamental R concepts. Just as historians analyze artifacts or literary scholars examine texts, we’ll be examining the building blocks of data analysis.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#understanding-the-rstudio-interface",
    "href": "book/03-dice.html#understanding-the-rstudio-interface",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n1.1 Understanding the RStudio Interface 🖥️",
    "text": "1.1 Understanding the RStudio Interface 🖥️\nWhen you open RStudio, you’ll see four main panes:\n\nConsole (usually bottom-left): This is where you type R commands and see results. Think of it as a conversation with R - you ask questions, and R answers.\nSource (usually top-left): Here you write and edit R scripts. It’s like a digital notebook for your code.\nEnvironment (usually top-right): This shows data and objects in R’s memory. Imagine it as a shelf where R stores all the information you’ve given it.\nFiles/Plots/Packages/Help (usually bottom-right): This multi-purpose pane is like a Swiss Army knife, offering file management, visualizations, add-ons, and documentation.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#running-r-code",
    "href": "book/03-dice.html#running-r-code",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n1.2 Running R Code 🏃‍♂️",
    "text": "1.2 Running R Code 🏃‍♂️\nThere are several ways to execute R code in RStudio:\n\n\nUsing the Console:\n\nType code directly into the Console and press Enter.\n\n\n\nUsing the Source Editor:\n\nWrite code in the Source Editor.\nRun a single line: Place cursor on the line and press Ctrl+Enter (Cmd+Enter on Mac).\nRun multiple lines: Highlight lines and press Ctrl+Enter (Cmd+Enter on Mac).\nRun entire script: Click “Run” button or press Ctrl+Shift+Enter (Cmd+Shift+Enter on Mac).\n\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\n\n\nWriting code in the Source Editor allows you to save, edit, and rerun your entire analysis easily.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check",
    "href": "book/03-dice.html#learning-check",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n1.3 Learning Check 🏁",
    "text": "1.3 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding",
    "href": "book/03-dice.html#hands-on-coding",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n1.4 Hands-On Coding 💻",
    "text": "1.4 Hands-On Coding 💻\nTry the following:\n\nOpen RStudio and create a new R script named “my_first_script.R”\nIn the script, write a simple calculation (e.g., 5 * 7)\nRun this calculation using different methods:\n\nConsole\nRunning a single line\nRunning the entire script\n\n\nObserve where the result appears in the RStudio interface\n\n\n# In my_first_script.R\n5 * 7\n\n# In Console\n&gt; 5 * 7\n[1] 35",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#creating-a-die-as-an-object",
    "href": "book/03-dice.html#creating-a-die-as-an-object",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n2.1 Creating a Die as an Object 🎲",
    "text": "2.1 Creating a Die as an Object 🎲\nLet’s start by creating a single die:\n\ndie &lt;- 1:6\ndie\n\n[1] 1 2 3 4 5 6\n\n\n\n\n\n\n\n\nKey Concepts\n\n\n\n\n\n\n\nVector: A basic container that can hold multiple items of the same type.\n\n\n\n\n\n\n\n\n\n\nThe Colon Operator\n\n\n\n\n\nThe colon operator (:) returns every integer between two integers. It’s an easy way to create a sequence of numbers.\n\n\n\n🙋🏻‍♀️ What if I want to create a vector that’s not a sequence of numbers?\n\n2.1.1 Combining Data: c()\n\nIn R, c() stands for “combine” or “concatenate”. It’s used to create vectors, which are sequences of data elements of the same type. For example:\n\n# Creating a vector of numbers\nnumbers &lt;- c(2, 4, 6, 8, 10)\nnumbers\n\n[1]  2  4  6  8 10\n\n# Creating a vector of characters (text)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\nfruits\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n# Creating a vector of logical values\nlogical_values &lt;- c(TRUE, FALSE, TRUE, TRUE)\nlogical_values\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n# You can also mix data types, but R will convert them to a single type\nmixed &lt;- c(1, \"two\", TRUE)\nmixed # R converts all to characters\n\n[1] \"1\"    \"two\"  \"TRUE\"\n\n\nThe c() function is versatile and essential for creating custom datasets in R. You can use it to make lists of any kind of data you’re working with in your research.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#manipulating-objects",
    "href": "book/03-dice.html#manipulating-objects",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n2.2 Manipulating Objects",
    "text": "2.2 Manipulating Objects\nYou can perform various operations on your die object:\n\ndie - 1\n\n[1] 0 1 2 3 4 5\n\ndie * 2\n\n[1]  2  4  6  8 10 12\n\ndie / 2\n\n[1] 0.5 1.0 1.5 2.0 2.5 3.0\n\n\n🤨 What do you notice about the results of the above operations?\n\n\n\n\n\n\nVectorized Operations in R\n\n\n\n\n\nR performs these operations element-wise on the die object because it uses vectorized operations. This means:\n\nEach element in the die vector is individually operated on.\nThe operation is applied to every element simultaneously.\nA new vector of the same length is returned as the result.\n\nThis vectorized approach makes R efficient for handling data and allows for concise, readable code when working with vectors or larger datasets.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-1",
    "href": "book/03-dice.html#learning-check-1",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n2.3 Learning Check 🏁",
    "text": "2.3 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-1",
    "href": "book/03-dice.html#hands-on-coding-1",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n2.4 Hands-On Coding 💻",
    "text": "2.4 Hands-On Coding 💻\nTry the following:\n\nCreate an object called favorite_number and assign it your favorite number.\nCreate a vector called lucky_numbers with 3-5 numbers you consider lucky.\nUse the length() function to find out how many lucky numbers you have.\nCreate a new vector called unlucky_numbers by subtracting 1 from each of your lucky_numbers.\nCreate a vector called all_numbers that combines your lucky_numbers and unlucky_numbers.\n\n\n\n# 1. Create favorite_number\nfavorite_number &lt;- 7\n\n# 2. Create lucky_numbers\nlucky_numbers &lt;- c(3, 7, 13)\n\n# 3. Use length() function\nlength(lucky_numbers)\n\n[1] 3\n\n# 4. Create unlucky_numbers\nunlucky_numbers &lt;- lucky_numbers - 1\n\n# 5. Combine vectors\nall_numbers &lt;- c(lucky_numbers, unlucky_numbers)\nprint(all_numbers)\n\n[1]  3  7 13  2  6 12\n\n\n\n\n\n\n\n\nPrinting in R\n\n\n\n\n\nIn R, using print() is often optional when working in the console. Simply typing the object name (e.g., all_numbers) will automatically display its contents. However, print() is useful in functions or scripts where you want to ensure output is displayed. It’s also helpful for explicitly showing your intent to output a value.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#key-concepts-of-functions-in-r",
    "href": "book/03-dice.html#key-concepts-of-functions-in-r",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n3.1 Key Concepts of Functions in R",
    "text": "3.1 Key Concepts of Functions in R\n\nPurpose: Functions are reusable blocks of code that perform a specific task.\n\nStructure: A function typically has three parts:\n\n\nName: What you call the function (e.g., sum, mean, plot)\n\nArguments: Input values the function needs to do its job\n\nBody: The code that defines what the function does\n\nHere’s the generic structure of a function in R:\n\nname_of_function &lt;- function(argument1, argument2, ...) {\n  # Body of the function\n  # Perform operations using the arguments\n\n  return(result)  # Output of the function\n}\n\nIn this structure:\n\n\nname_of_function is where you specify the function’s name\n\nfunction() keyword defines it as a function\n\n(argument1, argument2, ...) are the inputs the function accepts\nThe code between { and } is the body of the function\n\nreturn() specifies what the function outputs\n\nNow, let’s look at a specific example:\n\ndouble_value &lt;- function(x) {\n  result &lt;- x * 2\n  return(result)\n}\n\nIn this example:\n\nThe function name is double_value\n\nIt has one argument: x\n\nThe body is the code between the curly braces { }\n\nThe return() statement specifies what the function outputs\n\nYou can use this function like this:\n\ndoubled_value &lt;- double_value(5)\ndoubled_value\n\n[1] 10\n\n\n\nInput and Output: Functions often take input (arguments) and return output (results).\nBuilt-in vs. Custom: R has many built-in functions, but you can also create your own. You can also set default values for the arguments so that the function can still run without input.\n\nSyntax: To use a function, type its name followed by parentheses containing any arguments:\nfunction_name(argument1, argument2, ...)\n\nDocumentation: You can learn about a function’s use and arguments with ?function_name in the console.\n\nRemember, functions are powerful tools that help you organize and streamline your code, making your analysis more efficient and readable.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#using-built-in-functions",
    "href": "book/03-dice.html#using-built-in-functions",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n3.2 Using Built-in Functions",
    "text": "3.2 Using Built-in Functions\nR comes with many built-in functions that you can use right away. These are like the basic tools that come in a starter toolkit. Let’s look at a few examples:\n\n# The round() function rounds numbers\nround(3.14159)\n\n[1] 3\n\n# The sum() function adds numbers together\nsum(1, 2, 3, 4, 5)\n\n[1] 15\n\n# The length() function tells you how many items are in a list or vector\nlength(die)\n\n[1] 6\n\n\nUsing a function is straightforward. You write the name of the function followed by parentheses. Inside the parentheses, you put the information (called “arguments”) that the function needs to do its job.\n\n\n\n\n\n\nComments in R\n\n\n\n\n\nDid you notice the # … part in the above code chunk?\nIn R, the # symbol is used to create comments. Anything after a # on a line is ignored by R when running the code. Comments are useful for:\n\nExplaining what your code does\nTemporarily disabling code without deleting it\nOrganizing your script into sections\n\n\n\n\n\n3.2.1 Simulating Randomness: sample()\n\nTo simulate rolling a die, we need a function that can randomly select a number. The sample() function is perfect for this task. It’s like reaching into a bag and pulling out a random item.\n\n# Roll the die once\nsample(x = die, size = 1)\n\n[1] 5\n\n# Roll the die again\nsample(x = die, size = 1)\n\n[1] 4\n\n\nIn this case, x = die tells the function what to choose from (our die object), and size = 1 says to pick one number.\n\n\n\n\n\n\nSampling With Replacement\n\n\n\n\n\nBy default, sample() doesn’t put the number back in the “bag” after selecting it. For dice rolling, we want to allow the same number to be selected multiple times. Use replace = TRUE in the sample() function to achieve this.\n\n\n\n\n# Roll two dice\nsample(die, size = 2, replace = TRUE)\n\n[1] 3 4",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#writing-your-own-functions",
    "href": "book/03-dice.html#writing-your-own-functions",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n3.3 Writing Your Own Functions",
    "text": "3.3 Writing Your Own Functions\nLet’s create a function to roll two dice:\n\nroll &lt;- function() {\n  dice &lt;- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\nLet’s break this down:\n\n\nroll &lt;- function() { ... } creates a new function named roll.\nInside the curly braces { } is the “body” of the function - the instructions for what it should do.\n\ndice &lt;- sample(die, size = 2, replace = TRUE) rolls two dice.\n\nsum(dice) adds up the numbers rolled.\n\nNow, whenever you want to roll two dice and get the sum, you can simply use:\n\nroll()\n\n[1] 9\n\nroll()\n\n[1] 6",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-2",
    "href": "book/03-dice.html#learning-check-2",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n3.4 Learning Check 🏁",
    "text": "3.4 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-2",
    "href": "book/03-dice.html#hands-on-coding-2",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n3.5 Hands-On Coding 💻",
    "text": "3.5 Hands-On Coding 💻\nLet’s apply what we’ve learned to simulate flipping a coin. This is similar to rolling a die, but with only two possible outcomes.\n\nCreate an object called coin that represents the two sides of a coin (use the numbers 1 and 2).\nUse the sample() function to simulate flipping the coin once.\nCreate a function called flip_coin() that simulates flipping a coin and returns the result.\nModify your flip_coin() function to have an argument named n_flips so that the user could specify the number of time the coin is flipped.\n\n\n\n# 1. Create coin object\ncoin &lt;- 1:2\n# 2. Flip coin once using sample()\nsample(coin, 1)\n\n[1] 1\n\n# 3. Create flip_coin() function\nflip_coin &lt;- function() {\nsample(coin, size = 1)\n}\n# 4. Modify flip_coin() to have a n_flips argument\nflip_coin &lt;- function(n_flips) {\n  sample(coin, size = n_flips, replace = TRUE)\n}",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#why-use-quarto",
    "href": "book/03-dice.html#why-use-quarto",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n4.1 Why Use Quarto?",
    "text": "4.1 Why Use Quarto?\n\n\nReproducibility: Quarto documents contain both code and narrative that make it easy to reproduce your analyses.\n\nProfessional Presentation: Create polished, professional-looking documents.\n\nFlexibility: Output to various formats like HTML, PDF, or slideshows.\n\n\n\n\n\n\n\nThe Importance of Reproducibility\n\n\n\n\n\nIn the humanities, as in all academic fields, being able to reproduce the analytical process is crucial. When you write your R code in scripts, you’re creating a record of every step in your analysis. This is like creating detailed footnotes or citations in a research paper - it allows others (or future you!) to follow your work exactly.\nUsing scripts also makes it easy to make changes and rerun your entire analysis, which is much more efficient than trying to remember and retype everything in the console.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#quarto-document",
    "href": "book/03-dice.html#quarto-document",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n4.2 Quarto Document",
    "text": "4.2 Quarto Document\nA Quarto document has two main parts: - A header at the top (called YAML header) - The main content (text and code)\nBelow is an example:\nyaml\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\nformat: html\n---\nThis is where you write your content.\n\nYou can use markdown to format your text or use the \"visual editing mode\" to have an experience similar to Microsoft Word editor.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#essential-quarto-concepts",
    "href": "book/03-dice.html#essential-quarto-concepts",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n4.3 Essential Quarto Concepts",
    "text": "4.3 Essential Quarto Concepts\n\n4.3.1 Editing Quarto Files: Source vs. Visual Editor\nQuarto documents can be edited in two modes: Source and Visual.\n\n\nSource Editor: This mode shows you the raw Markdown and YAML content of your document. It’s great for:\n\nPrecise control over your document’s structure\nEditing code chunks directly\nWorking with advanced Quarto features\n\n\n\nVisual Editor: This mode provides a WYSIWYG (What You See Is What You Get) interface, similar to word processors like Microsoft Word. It’s useful for:\n\nFormatting text without knowing Markdown syntax\nEasily inserting tables, images, and other elements\nCollaborating with team members who may not be familiar with Markdown\n\n\n\n\n\n\n\n\n\nMarkdown Basics\n\n\n\n\n\nMarkdown is a lightweight markup language used in Quarto documents. Here are some basic syntax examples:\n\n\n# Heading 1, ## Heading 2, ### Heading 3, etc. for headings\n\n*italic* or _italic_ for italic text\n\n\n**bold** or __bold__ for bold text\n\n\n- item for unordered lists\n\n1. item for ordered lists\n\n[link text](URL) for links\n\n\n![alt text](image.jpg) for images\n\nFor a more comprehensive guide, check out Quarto’s Markdown Basics.\n\n\n\n\n4.3.2 Rendering\nRendering is the process of converting your Quarto document into its final output format (e.g., HTML, PDF, or Word). To render your document:\n\nClick the “Render” button in RStudio, or\nUse the keyboard shortcut Ctrl+Shift+K (Cmd+Shift+K on Mac)\n\nWhen you render, Quarto executes all the code in your document and combines the results with your text to create the final output.\n\n4.3.3 (Optional) Code Chunk Options\nCode chunks in Quarto can be customized using options. These options control how the code is executed and displayed. Here are some common options:\n```{r}\n#| label: chunk-name\n#| echo: true\n#| eval: true\n#| warning: false\n#| message: false\n\n# Your R code here\n```\n\n\nlabel: Gives the chunk a unique name\n\necho: Controls whether the code is displayed in the output (true/false)\n\neval: Determines if the code should be executed (true/false)\n\nwarning: Shows or hides warnings (true/false)\n\nmessage: Shows or hides messages (true/false)\n\nYou can set these options globally in the YAML header or for individual chunks.\n\n\n\n\n\n\nPro Tip\n\n\n\n\n\nUse code chunk options to control what your readers see. For example, you might hide code for complex calculations but show the results.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#learning-check-3",
    "href": "book/03-dice.html#learning-check-3",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n4.4 Learning Check 🏁",
    "text": "4.4 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/03-dice.html#hands-on-coding-3",
    "href": "book/03-dice.html#hands-on-coding-3",
    "title": "§3 Rolling the Dice 🎲",
    "section": "\n4.5 Hands-On Coding 💻",
    "text": "4.5 Hands-On Coding 💻\nLet’s practice creating a simple Quarto document about our coin flipping simulation:\n\nCreate a new Quarto document in RStudio.\nAdd a title “Coin Flipping Simulation” and your name as the author.\nWrite a brief introduction about coin flipping simulations.\nInsert a code chunk that defines your coin object and flip_coin() function.\nAdd another code chunk that uses your flip_coin() function to simulate flipping a coin 10 times.\nAdd some text explaining the results.\nRender it as an html file.\n\n\n---\ntitle: \"Coin Flipping Simulation\"\nauthor: \"Your Name\"\nformat: html\n---\n# Introduction\nIn this document, we'll simulate flipping coins using R functions we've created.\n\n# Defining Our Coin and Flip Function\nFirst, let's define our coin and create a function to flip it:\n\n\ncoin &lt;- 1:2   \n\nflip_coin &lt;- function(n_flips) {\n  sample(coin, size = n_flips, replace = TRUE)\n}\n\n# Flipping a Coin 10 Times\nNow, let's flip our coin 10 times:\n\nten_flips &lt;- flip_coin(10)\nprint(ten_flips)\n\n [1] 2 1 1 1 2 2 2 2 2 1\n\n\nThis simulates flipping a coin 10 times. Each 1 represents heads, and each 2 represents tails. We then count how many times we got heads.\nCongratulations! 🎉 You’ve just created your first Quarto document combining narrative text and code analysis. This is an important step in your journey of digital humanities, as it allows you to present your research and analysis in a clear, reproducible format. As you continue to explore, you’ll find that Quarto is a powerful tool for integrating your humanities insights with computational methods.",
    "crumbs": [
      "Book",
      "§3 Rolling the Dice 🎲"
    ]
  },
  {
    "objectID": "book/06-qta.html",
    "href": "book/06-qta.html",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "",
    "text": "Welcome to the world of quantitative text analysis! In this chapter, we’ll explore the fundamental concepts and techniques used to prepare text data for analysis. We’ll use a collection of classic novels as our dataset to demonstrate these concepts.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#qta-in-digital-humanities",
    "href": "book/06-qta.html#qta-in-digital-humanities",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n1.1 QTA in Digital Humanities",
    "text": "1.1 QTA in Digital Humanities\nIn Digital Humanities, QTA offers powerful tools for exploring large text corpora:\n\nScale: Analyze vast collections of texts, revealing patterns across time periods, genres, or authors.\nDistant Reading: Observe broader patterns in literature and cultural production.\nHypothesis Testing: Empirically test literary and cultural theories across large datasets.\nDiscovery: Reveal unexpected patterns or connections, sparking new research questions.\nInterdisciplinary: Combine methods from linguistics, computer science, and statistics with humanistic inquiry.\nVisualization: Present textual data in new, visually interpretable ways.\n\nQTA complements traditional close reading, offering Digital Humanities scholars new perspectives on cultural, historical, and literary phenomena.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#qta-workflow",
    "href": "book/06-qta.html#qta-workflow",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n1.2 QTA Workflow",
    "text": "1.2 QTA Workflow\n\nThe Quantitative Text Analysis (QTA) workflow illustrates the systematic process of analyzing textual data using computational methods. This workflow can be divided into five main stages:\n\n\nData Acquisition:\n\nData retrieval from various sources\nWeb scraping techniques\nSocial media API integration\n…\n\n\n\nText Preprocessing and Standardization:\n\n\nTidy structuring: Organizing data into a consistent format\n\nNoise removal: Eliminating capitalization, special characters, punctuation, and numbers\n\nTokenization: Breaking text into individual units (words, sentences)\n\nStopwords removal: Eliminating common words with little semantic content\n\nStemming or lemmatization: Reducing words to their root forms\n\nPOS (Part-of-Speech) tagging: Labeling words with their grammatical categories\n\nNamed Entity Recognition (NER): Identifying and classifying named entities in text\n…\n\n\n\nText Representation:\n\n\nCount-based methods:\n\nBag-of-words representation\nTerm frequency-inverse document frequency (TF-IDF)\n…\n\n\n\nContext-based methods:\n\nN-grams analysis\nCo-occurrence matrix\nWord embeddings (e.g., Word2Vec, GloVe)\n…\n\n\n\n\n\nData Analysis and Modeling:\n\n\nSentiment Analysis: Determining the emotional tone of text\n\nTopic Modeling: Discovering abstract topics in a collection of documents\n\nSocial Network Analysis: Analyzing relationships and structures in text data\n…\n\n\n\nVisualization & Reporting:\n\nVisualizing sentiment trends over time or across categories\nCreating network graphs to represent relationships in text data\nDisplaying topic distributions and their evolution\nSummarizing findings and insights in reports or publications\n…\n\n\n\nThis workflow emphasizes the importance of a structured approach to text analysis, from raw data to final insights. Each stage builds upon the previous one, with the flexibility to iterate or adjust methods based on the specific requirements of the analysis and research questions. The process can be iterative, with researchers often moving back and forth between stages as needed to refine their analysis and results.\n\n\n\n\n\n\nInstall Package Manager pacman\n\n\n\n\n\nWe will be using some new packages that you probably haven’t installed. To streamline the process of package installation, let’s introduce a helpful tool for managing R packages: pacman. The pacman package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.\nKey features of pacman:\n\nIt combines the functionality of install.packages() and library() into a single function p_load().\nIt automatically installs packages if they’re not already installed.\nIt can load multiple packages with a single line of code.\n\nLet’s install pacman:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\n\nOnce we load the pacman library, we can use p_load() to efficiently load (and install if necessary) the packages we’ll need:\n\nlibrary(pacman)\np_load(tidyverse, tidytext)\n\nThis single line will ensure all the packages we need are installed and loaded, streamlining our setup process.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#understanding-text-preprocessing",
    "href": "book/06-qta.html#understanding-text-preprocessing",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.1 Understanding Text Preprocessing",
    "text": "2.1 Understanding Text Preprocessing\nText preprocessing is the crucial first step in quantitative text analysis. It involves cleaning and standardizing raw text data to make it suitable for computational analysis.\nWhy is it important?\n\nImproves data quality and consistency\nReduces noise and irrelevant information\nEnhances the accuracy of subsequent analyses\nMakes text data more manageable for computational processing\n\nFundamental considerations:\n\n\nPurpose: The preprocessing steps you choose should align with your research questions and analysis goals.\n\nLanguage: Different languages may require specific preprocessing techniques.\n\nDomain: The nature of your texts (e.g., literary, social media, historical) may influence preprocessing decisions.\n\nInformation preservation: Be cautious not to remove potentially important information during preprocessing.\n\nReproducibility: Document your preprocessing steps to ensure your analysis can be replicated.\n\nRemember, there’s no one-size-fits-all approach to preprocessing. The techniques you apply should be carefully considered based on your specific research context.\nTo showcase the various techniques for text preprocessing, let’s first create a mock dataset:\n\nmock_data &lt;- tibble(\n  text = c(\n    \"The Quick Brown Fox Jumps Over the Lazy Dog! Data Science meets Cultural Studies.\",\n    \"Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literature & History\",\n    \"R Programming for Text Analysis - Chapter 3. Machine Learning for Textual Analysis\",\n    \"NLP techniques & their applications in DH research; Computational Methods in Humanities Research?\",\n    \"20+ ways to visualize data 📊: graphs, charts, and more! Digital Archives and Text Mining Techniques.\"\n  )\n)\n\nLet’s take a closer look at our mock dataset:\n\nprint(mock_data)\n\n# A tibble: 5 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 The Quick Brown Fox Jumps Over the Lazy Dog! Data Science meets Cultural Stud…\n2 Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literat…\n3 R Programming for Text Analysis - Chapter 3. Machine Learning for Textual Ana…\n4 NLP techniques & their applications in DH research; Computational Methods in …\n5 20+ ways to visualize data 📊: graphs, charts, and more! Digital Archives and…\n\n\n\n\n\n\n\n\nReflection 🤔\n\n\n\n\n\nExamine the mock dataset above and reflect on the following questions:\n\nWhat characteristics or elements do you notice that might need preprocessing for effective text analysis?\nWhat challenges might these elements pose for text analysis? How might preprocessing help address these challenges?\n\n\n\n\n\nClick to Reveal Some Insights\n\nCapitalization: Words are inconsistently capitalized (e.g., “The Quick Brown Fox” vs. “Data Science”). This could lead to treating identical words as different entities.\nPunctuation: Various punctuation marks are present, including periods, exclamation marks, colons, and semicolons. These might interfere with word tokenization and analysis.\nNumbers: Some entries contain numbers (e.g., “101”, “2024”, “3”, “20+”). Depending on the analysis goals, these might need to be removed or treated specially.\nSpecial Characters: There are ampersands (&) and hyphens (-) which might need special handling.\nSentence Structure: Each row contains multiple sentences. For sentence-level analysis, we might need to split these.\nAbbreviations: “NLP” and “DH” are present. We might need to decide whether to expand these or treat them as single tokens.\nStop Words: Common words like “the”, “and”, “for” are present. These might not contribute much meaning to the analysis.\n\nThese observations highlight the need for various preprocessing steps, including: - Converting text to lowercase for consistency - Removing or standardizing punctuation - Handling numbers and special characters - Sentence or word tokenization - Removing stop words\nBy addressing these elements through preprocessing, we can prepare our text data for more effective and accurate analysis.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#understanding-regular-expressions",
    "href": "book/06-qta.html#understanding-regular-expressions",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.2 Understanding Regular Expressions",
    "text": "2.2 Understanding Regular Expressions\nBefore we dive into analyzing our mock data, let’s explore a powerful tool in text analysis: Regular Expressions.\nHave you ever wondered how computer programs like Excel or Word can find the exact word or phrase you’re searching for? Or how they can replace all instances of a word throughout a document in just seconds? These everyday text operations are powered by a concept called pattern matching, and regular expressions take this idea to a whole new level.\nRegular Expressions, often called regex, are like a special language for describing patterns in text. Imagine you’re a librarian with a magical magnifying glass that can find not just specific words, but patterns in books.\n\n2.2.1 Key Concepts\n\n\nPattern Matching: Instead of searching for exact words, regex lets you search for patterns. For example, you could search for:\n\nAll words that start with “pre”\nAny sequence of five letters\nAll dates in a specific format\n\n\n\nText Processing: Once you find these patterns, you can do things like:\n\nHighlight them\nReplace them with something else\nExtract them for further study\n\n\n\n2.2.2 Examples of Regex\nTo give you a better idea of what regular expressions look like and how they work, let’s look at an example:\n\np_load(stringr)\n\n# Sample text\ntext &lt;- \"Jane Austen wrote Pride and Prejudice. Elizabeth Bennet is the protagonist.\"\n\n# Regex pattern for capitalized words\npattern &lt;- \"\\\\b[A-Z][a-z]+\\\\b\"\n\n# Find all matches\nmatches &lt;- str_extract_all(text, pattern)\n\n# Print the matches\nprint(matches)\n\n[[1]]\n[1] \"Jane\"      \"Austen\"    \"Pride\"     \"Prejudice\" \"Elizabeth\" \"Bennet\"   \n\n# To see which words were matched in context\nstr_view(text, pattern)\n\n[1] │ &lt;Jane&gt; &lt;Austen&gt; wrote &lt;Pride&gt; and &lt;Prejudice&gt;. &lt;Elizabeth&gt; &lt;Bennet&gt; is the protagonist.\n\n\n\n\n\n\n\n\nRegex breakdown\n\n\n\n\n\nLet’s break down the regex pattern \\\\b[A-Z][a-z]+\\\\b:\n\n\\\\b: This represents a word boundary. In R, we need to escape the backslash, so we use two. It ensures we’re matching whole words, not parts of words.\n[A-Z]: This character class matches any single uppercase letter from A to Z.\n\n[a-z]+: This matches one or more lowercase letters.\n\n\n[a-z] is a character class that matches any single lowercase letter.\nThe + quantifier means “one or more” of the preceding element.\n\n\n\\\\b: Another word boundary to end the match.\n\nSo, this pattern matches: - Words that start with a capital letter (like names or the first word of a sentence) - Followed by one or more lowercase letters - As whole words, not parts of larger words\nIt won’t match: - ALL CAPS words - words with numbers or symbols - Single-letter capitalized words like “I” or “A”\nThis pattern is useful for finding proper nouns in the middle of sentences, like names of people or places.\n\n\n\n\n2.2.3 POSIX Character Classes: A Friendly Starting Point\nYou may find that regex can be quite hard to read for humans. POSIX character classes are pre-defined sets of characters that make regex more accessible and portable across different systems. They simplify regex patterns and address some common challenges in text processing:\n\nSimplification: POSIX classes provide easy-to-remember shorthand for common character groups. Instead of writing [A-Za-z] to match any letter, you can use [:alpha:].\nConsistency: They ensure consistent behavior across different operating systems and programming languages. For example, [A-Z] might behave differently in some contexts depending on the locale settings, but [:upper:] is always consistent.\nInternationalization: POSIX classes can handle characters beyond the ASCII range, making them useful for working with texts in various languages.\nReadability: They make regex patterns more readable and self-explanatory, which is especially helpful when sharing code or working in teams.\n\nHere are some useful POSIX character classes:\n\n\n[:alpha:]: Matches any alphabetic character (equivalent to [A-Za-z] in English texts)\n\n[:digit:]: Matches any digit (equivalent to [0-9])\n\n[:lower:]: Matches any lowercase letter\n\n[:upper:]: Matches any uppercase letter\n\n[:punct:]: Matches any punctuation character\n\n[:space:]: Matches any whitespace character (spaces, tabs, newlines)\n\nBy using these classes, you can create more robust and readable regex patterns. For example, instead of [A-Za-z0-9] to match any alphanumeric character, you could use [[:alpha:][:digit:]], which is clearer in its intent and works across different language settings.\nExamples in Humanities Context\n\nFinding capitalized words:\n\nPattern: [[:upper:]][[:lower:]]+\n\nThis would match words like “Shakespeare”, “London”, “Renaissance”\n\n\nIdentifying years:\n\nPattern: [[:digit:]]{4}\n\nThis would match years like “1564”, “1616”, “2023”\n\n\nLocating punctuation:\n\nPattern: [[:punct:]]\n\nThis would find all punctuation marks in a text\n\n\n\nRemember, regex is a tool that becomes more useful as you practice. Start simple, and you’ll gradually be able to create more complex patterns for your research needs!",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#learning-check",
    "href": "book/06-qta.html#learning-check",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.3 Learning Check 🏁",
    "text": "2.3 Learning Check 🏁\n\n\n\nNow let’s proceed to preprocess the mock dataset!",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#tidy-structuring",
    "href": "book/06-qta.html#tidy-structuring",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.4 Tidy Structuring",
    "text": "2.4 Tidy Structuring\nThe first step in our preprocessing pipeline is to ensure our data is in a tidy format. Remember a tidy format means one observation per row and the unit of the observation, whether it is a sentence, or a word, varies from project to project depending on the nature of your study and research questions. For this example, let’s assume we want one sentence per row. We’ll use separate_rows() with a regular expression (regex) pattern to achieve this:\n\ntidy_data &lt;- mock_data %&gt;%\n  separate_rows(text, sep = \"(?&lt;=[.!?])\\\\s+(?=[A-Z])\")\n\nprint(tidy_data)\n\n# A tibble: 8 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 The Quick Brown Fox Jumps Over the Lazy Dog!                                  \n2 Data Science meets Cultural Studies.                                          \n3 Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literat…\n4 R Programming for Text Analysis - Chapter 3.                                  \n5 Machine Learning for Textual Analysis                                         \n6 NLP techniques & their applications in DH research; Computational Methods in …\n7 20+ ways to visualize data 📊: graphs, charts, and more!                      \n8 Digital Archives and Text Mining Techniques.                                  \n\n\n\n\n\n\n\n\nUnderstanding separate_rows()\n\n\n\n\n\n\n\nseparate_rows() Function:\n\nPart of the tidyr package in tidyverse\nSeparates a column into multiple rows based on a delimiter\nSyntax: separate_rows(data, column, sep = delimiter)\n\n\n\nRegular Expression (Regex) Pattern:\n\n\n(?&lt;=[.!?]): Positive lookbehind, matches a position after a period, exclamation mark, or question mark\n\n\\\\s+: Matches one or more whitespace characters\n\n(?=[A-Z]): Positive lookahead, matches a position before an uppercase letter\nCombined, this pattern splits the text at sentence boundaries\n\n\nHow it works:\n\n\nseparate_rows() applies the regex pattern to split the ‘text’ column\nEach resulting sentence becomes a new row in the dataframe\nThe original row’s other columns (if any) are duplicated for each new sentence row",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#noise-removal",
    "href": "book/06-qta.html#noise-removal",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.5 Noise Removal",
    "text": "2.5 Noise Removal\n\n2.5.1 Capitalization Removal\nConverting text to lowercase is a common preprocessing step that helps standardize the text and reduce the vocabulary size. Let’s apply this to our tidy data:\n\nlowercase_data &lt;- tidy_data %&gt;%\n  mutate(text = tolower(text))\n\nprint(lowercase_data)\n\n# A tibble: 8 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 the quick brown fox jumps over the lazy dog!                                  \n2 data science meets cultural studies.                                          \n3 digital humanities 101: an introduction (2024); exploring big data in literat…\n4 r programming for text analysis - chapter 3.                                  \n5 machine learning for textual analysis                                         \n6 nlp techniques & their applications in dh research; computational methods in …\n7 20+ ways to visualize data 📊: graphs, charts, and more!                      \n8 digital archives and text mining techniques.                                  \n\n\n\n2.5.2 Punctuation and Special Character Removal\nRemoving punctuation and special characters can help focus on the words themselves. We’ll use a regular expression to remove these:\n\nclean_data &lt;- lowercase_data %&gt;%\n  # Remove punctuation\n  mutate(text = str_replace_all(text, \"[[:punct:]]\", \"\")) %&gt;%\n  # Remove special characters (including emojis)\n  mutate(text = str_replace_all(text, \"[^[:alnum:][:space:]]\", \"\"))\n\nprint(clean_data)\n\n# A tibble: 8 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 the quick brown fox jumps over the lazy dog                                   \n2 data science meets cultural studies                                           \n3 digital humanities 101 an introduction 2024 exploring big data in literature …\n4 r programming for text analysis  chapter 3                                    \n5 machine learning for textual analysis                                         \n6 nlp techniques  their applications in dh research computational methods in hu…\n7 20 ways to visualize data  graphs charts and more                             \n8 digital archives and text mining techniques                                   \n\n\n\n2.5.3 Numbers Removal\nDepending on your analysis goals, you might want to remove numbers. Here’s how to do that:\n\nno_numbers_data &lt;- clean_data %&gt;%\n  mutate(text = str_replace_all(text, \"\\\\d+\", \"\"))\n\nprint(no_numbers_data)\n\n# A tibble: 8 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 \"the quick brown fox jumps over the lazy dog\"                                 \n2 \"data science meets cultural studies\"                                         \n3 \"digital humanities  an introduction  exploring big data in literature  histo…\n4 \"r programming for text analysis  chapter \"                                   \n5 \"machine learning for textual analysis\"                                       \n6 \"nlp techniques  their applications in dh research computational methods in h…\n7 \" ways to visualize data  graphs charts and more\"                             \n8 \"digital archives and text mining techniques\"",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#tokenization",
    "href": "book/06-qta.html#tokenization",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.6 Tokenization",
    "text": "2.6 Tokenization\n\n\n\n\n\n\nWhat is a token?\n\n\n\n\n\nIn text analysis, a “token” is the smallest unit of text that we analyze. Most commonly, a token is a single word, but it could also be a character, a punctuation mark, or even a phrase, depending on our analysis needs.\nFor example, in the sentence “The quick brown fox”, each word (“The”, “quick”, “brown”, “fox”) could be considered a token.\n\n\n\n\n\n\n\n\n\nWhat is tokenization?\n\n\n\n\n\nTokenization is the process of breaking down a piece of text into its individual tokens. It’s like taking a sentence and splitting it into a list of words.\nWhy do we do this? Computers can’t understand text the way humans do. By breaking text into tokens, we create a format that’s easier for computers to process and analyze.\n\n\n\nWe’ll use the unnest_tokens() function from the tidytext package:\n\np_load(tidytext)\n\ntokenized_data &lt;- no_numbers_data %&gt;%\n  unnest_tokens(output = word, input = text, token = \"words\")\n\nprint(tokenized_data)\n\n# A tibble: 61 × 1\n   word \n   &lt;chr&gt;\n 1 the  \n 2 quick\n 3 brown\n 4 fox  \n 5 jumps\n 6 over \n 7 the  \n 8 lazy \n 9 dog  \n10 data \n# ℹ 51 more rows\n\n\n\n\n\n\n\n\nunnest_tokens() function\n\n\n\n\n\nThe unnest_tokens() function from the tidytext package is a powerful tool for text analysis:\n\nPurpose: Splits a column into tokens, creating a one-token-per-row structure.\n\nKey Arguments:\n\noutput: Name of the new column for tokens\ninput: Column to be tokenized\ntoken: Tokenization unit (e.g., “words”, “sentences”, “ngrams”)\n\nThe unnest_tokens() is built on the tokenizers package, which means there are a lot of arguments you can tweak to perform multiple operations at once. Below are some common features that you may want to pay attention to:\nFeatures:\n\nConvert to lowercase by default (to_lower = TRUE)\nStrip punctuations by default (strip_punct = TRUE)\nPreserves numbers by default (strip_numeric = FALSE)\nAutomaticlaly removes extra white space",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#stopwords-removal",
    "href": "book/06-qta.html#stopwords-removal",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.7 Stopwords Removal",
    "text": "2.7 Stopwords Removal\n\n\n\n\n\n\nWhat are stopwords?\n\n\n\n\n\nStopwords are common words that are often removed from text analysis because they typically don’t carry much meaning on their own (i.e., “filler” words in language). These are words like “the”, “is”, “at”, “which”, and “on”.\nIn text analysis, removing stopwords helps us focus on the words that are more likely to carry the important content or meaning of the text. It’s like distilling a sentence down to its key concepts.\n\n\n\nNow, let’s remove stopwords from our tokenized data:\n\ndata(stop_words)\n\ndata_without_stopwords &lt;- tokenized_data %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\nanti_join() Function\n\n\n\n\n\nThe anti_join() function is a way to remove items from one list based on another list. Here is how it works:\n\n\nImagine you have two lists:\n\nList A: Your main list (in this case, our tokenized keywords)\nList B: A list of items you want to remove (in this case, stopwords)\n\n\nanti_join() goes through List A and removes any item that appears in List B.\nThe result is a new list that contains only the items from List A that were not in List B.\n\nIn our context:\n\nList A is our tokenized keywords\nList B is the list of stopwords\nThe result is our keywords with all the stopwords removed\n\nIt’s like having a big box of Lego bricks (your keywords) and a list of colors you don’t want (stopwords). anti_join() helps you quickly remove all the Lego bricks of those unwanted colors, leaving you with only the colors you are interested in.\nThis function is particularly useful for cleaning text data, as it allows us to efficiently remove common words that might not be relevant to our analysis.\n\n\n\n\n\n\n\n\n\nCaution with Stopword Removal\n\n\n\n\n\nWhile stopword removal is a common preprocessing step, it’s important to consider its implications:\n\nContext Matters: Words considered “stop words” in one context might be meaningful in another. For example, “the” in “The Who” (band name) carries meaning.\nNegations: Removing words like “not” can invert the meaning of surrounding words, potentially affecting sentiment analysis.\nPhrase Meaning: Some phrases lose their meaning without stop words. “To be or not to be” becomes simply “be be” after stopword removal.\nLanguage Specificity: Stop word lists are language-specific. Ensure you’re using an appropriate list for your text’s language.\nResearch Questions: Your specific research questions should guide whether and how you remove stop words. Some analyses might benefit from keeping certain stop words.\n\n\n\n\n\n\n\n\n\n\nCustomizing Stopword Lists (Advanced)\n\n\n\n\n\nThe stop_words dataset in the tidytext package contains stop words from three lexicons: “onix”, “SMART”, and “snowball”. You can customize your stopword removal by:\n\n\nUsing only one lexicon:\n\nstop_words %&gt;% filter(lexicon == \"snowball\")\n\n# A tibble: 174 × 2\n   word      lexicon \n   &lt;chr&gt;     &lt;chr&gt;   \n 1 i         snowball\n 2 me        snowball\n 3 my        snowball\n 4 myself    snowball\n 5 we        snowball\n 6 our       snowball\n 7 ours      snowball\n 8 ourselves snowball\n 9 you       snowball\n10 your      snowball\n# ℹ 164 more rows\n\n\n\n\nAdding or removing words from the list:\n\ncustom_stop_words &lt;- stop_words %&gt;%\n  add_row(word = \"custom_word\", lexicon = \"custom\")\n\n\nCreating your own domain-specific stop word list based on your corpus and research needs.\n\n\n\n\nRemember, the goal of stopword removal is to reduce noise in the data and focus on meaningful content. However, what constitutes “meaningful” can vary depending on your specific analysis goals and the nature of your text data.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#stemming-and-lemmatization",
    "href": "book/06-qta.html#stemming-and-lemmatization",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.8 Stemming and Lemmatization",
    "text": "2.8 Stemming and Lemmatization\nStemming and lemmatization are text normalization techniques used to reduce words to their base or root form. This process helps in grouping together different inflected forms of a word, which can be useful for various text analysis tasks.\n\n2.8.1 What are Stemming and Lemmatization?\n\n\nStemming is a simple, rule-based process of removing the ends of words (suffixes) to reduce them to their base form. For example:\n\n“running” → “run”\n“cats” → “cat”\n“better” → “better” (remains unchanged)\n\n\n\nLemmatization is a more sophisticated approach that considers the context and part of speech of a word to determine its base form (lemma). For example:\n\n“running” → “run”\n“better” → “good”\n“are” → “be”\n\n\n\n2.8.2 When to Use Them?\n\nUse stemming when:\n\nYou need a quick, computationally efficient method.\nThe meaning of the stem is clear enough for your analysis.\nYou’re working with a large dataset and processing speed is crucial.\n\n\nUse lemmatization when:\n\nYou need more accurate, dictionary-based results.\nThe precise meaning of the word is important for your analysis.\nYou have the computational resources to handle a more intensive process.\n\n\n\n2.8.3 Applying Stemming\nIn this example, we’ll apply stemming using the SnowballC package, which implements the Porter stemming algorithm:\n\np_load(SnowballC)\n\nstemmed_data &lt;- data_without_stopwords %&gt;%\n  mutate(stem = wordStem(word))\n\nprint(stemmed_data)\n\n# A tibble: 44 × 2\n   word     stem  \n   &lt;chr&gt;    &lt;chr&gt; \n 1 quick    quick \n 2 brown    brown \n 3 fox      fox   \n 4 jumps    jump  \n 5 lazy     lazi  \n 6 dog      dog   \n 7 data     data  \n 8 science  scienc\n 9 meets    meet  \n10 cultural cultur\n# ℹ 34 more rows\n\n\n\n\n\n\n\n\nLemmatization Alternative\n\n\n\n\n\nFor lemmatization, you can use the textstem package:\n\np_load(textstem)\n\nlemmatized_data &lt;- data_without_stopwords %&gt;%\n  mutate(lemma = lemmatize_words(word))\n\nprint(lemmatized_data)\n\n# A tibble: 44 × 2\n   word     lemma   \n   &lt;chr&gt;    &lt;chr&gt;   \n 1 quick    quick   \n 2 brown    brown   \n 3 fox      fox     \n 4 jumps    jump    \n 5 lazy     lazy    \n 6 dog      dog     \n 7 data     datum   \n 8 science  science \n 9 meets    meet    \n10 cultural cultural\n# ℹ 34 more rows\n\n\nLemmatization often produces more intuitive results but can be slower for large datasets.\n\n\n\n\n2.8.4 Stemming vs. Lemmatization\nIn humanities research, the choice between stemming and lemmatization can significantly impact your analysis:\n\nPreserving Meaning: Lemmatization often preserves meaning better, which can be crucial for literary analysis or historical research.\nHandling Irregular Forms: Lemmatization is better at handling irregular forms (e.g., “better” → “good”), which is common in many languages and especially important in analyzing older texts.\nComputational Resources: If you’re working with vast corpora, stemming might be more practical due to its speed.\nLanguage Specificity: For languages other than English, or for multilingual corpora, lemmatization often provides better results as it’s more adaptable to language-specific rules.\nResearch Questions: Consider your research questions. If distinguishing between, say, “democratic” and “democracy” is crucial, lemmatization might be more appropriate.\n\nRemember, the choice of preprocessing steps, including whether to use stemming or lemmatization, should be guided by your specific research questions and the nature of your text data. It’s often helpful to experiment with different approaches and see which yields the most meaningful results for your particular study.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#hands-on-coding",
    "href": "book/06-qta.html#hands-on-coding",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n2.9 Hands-On Coding 💻",
    "text": "2.9 Hands-On Coding 💻\nNow, let’s apply our text preprocessing skills to a real dataset. We’ll use the answers submitted by you in response to the “what is digital humanities? write down three keywords that come to mind” question. Our goal is to clean and prepare this text data for analysis.\nFirst, let’s download the dataset:\nDownload DH Keywords dataset\nSave it in the folder name data in your working directory.\nWe can then load the csv file into RStudio:\n\np_load(tidyverse)\n\n# Load the dataset\ndh_keywords &lt;- read_csv(\"data/dh_keywords.csv\", col_names = FALSE)\n\nRows: 180 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(dh_keywords) &lt;- \"keyword\"\n\n# Display the first few rows\nhead(dh_keywords)\n\n# A tibble: 6 × 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 AI     \n4 ai     \n5 AI     \n6 ai     \n\n\n\n2.9.1 Exercise 1: Planning Preprocessing Steps\nBefore we start coding, let’s think about what steps we need to take to clean and standardize our DH keywords dataset, and in what order these steps should be performed.\nLook at the first few rows of our dataset:\n\nhead(dh_keywords)\n\n# A tibble: 6 × 1\n  keyword\n  &lt;chr&gt;  \n1 math   \n2 chatgpt\n3 AI     \n4 ai     \n5 AI     \n6 ai     \n\n\nNow, consider the following questions:\n\nWhat issues do you notice in the data that need to be addressed?\nWhat preprocessing steps would you take to clean this data?\nIn what order should these steps be performed, and why?\n\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nNotice that many rows contain more than one word. We may need to either separate rows so that there is only one entry per row or tokenize the data so that there is only one word per row.\n\n\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nAfter tokenization, consider issues like inconsistent capitalization, punctuation, extra whitespace, and potentially empty entries.\n\n\n\n\n\nA possible preprocessing plan could be:\n\nTokenize the data so that we have one word per row.\n\nThis ensures we’re working with individual words.\n\n\nConvert all text to lowercase\n\nThis ensures consistency in our text data\n\n\nRemove all punctuation\n\nThis focuses our analysis on the words themselves\n\n\nTrim leading and trailing whitespace and replace multiple spaces with a single space\n\nThis cleans up any formatting inconsistencies\n\n\nRemove stop words\n\nThis eliminates common words that don’t carry much meaning in analysis\n\n\nRemove any empty entries that might result from the above steps\n\nThis ensures we’re only working with meaningful data\n\n\n\nThis order ensures that we first isolate individual keywords, then standardize the text (lowercase) before removing elements (punctuation), clean up spacing issues, and finally remove any entries that became empty as a result of our preprocessing.\n\n\n2.9.2 Exercise 2: Implementing the Preprocessing Steps\nNow that we have a plan, let’s implement these preprocessing steps in R. Use the tidyr, dplyr, and stringr packages to clean the dh_keywords dataset according to your plan.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse unnest_tokens() from tidytext to tokenize the data into individual words. Remember, by default this will take care of the lowercasing and punctuation removal for us.\n\n\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nUse anti_join() with a stop words dataset to remove common words.\n\n\n\n\n\n\n\n\n\n\n\nHint 3\n\n\n\n\n\nUse filter() with nchar() to remove empty entries.\n\n\n\n\n\n\np_load(tidytext)\n\ndh_keywords_clean &lt;- dh_keywords %&gt;%\n  unnest_tokens(output = word, input = keyword) %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(_ &gt; _)\n\n# Display the final result and the number of rows\nprint(dh_keywords_clean)\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_clean))\n\n\n\n\np_load(tidytext)\n\ndh_keywords_clean &lt;- dh_keywords %&gt;%\n  unnest_tokens(output = word, input = keyword) %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(nchar(word) &gt; 0)\n\nJoining with `by = join_by(word)`\n\n# Display the final result and the number of rows\nprint(dh_keywords_clean)\n\n# A tibble: 405 × 1\n   word   \n   &lt;chr&gt;  \n 1 math   \n 2 chatgpt\n 3 ai     \n 4 ai     \n 5 ai     \n 6 ai     \n 7 coding \n 8 ai     \n 9 ai     \n10 data   \n# ℹ 395 more rows\n\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_clean))\n\n[1] \"Number of keywords after preprocessing: 405\"",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#bag-of-words-bow",
    "href": "book/06-qta.html#bag-of-words-bow",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n3.1 Bag-of-Words (BoW)",
    "text": "3.1 Bag-of-Words (BoW)\nThe Bag-of-Words model is a simple yet powerful way to represent text data. It treats each document (unit of analysis) as an unordered collection of words, disregarding grammar and word order but keeping track of word frequency. \n\n\n\n\n\n\nKey Features of Bag-of-Words\n\n\n\n\n\n\nCounts how many times each word appears in a text or document\nCreates a list of all unique words used across all documents\nTreats each unique word as a separate piece of information\nIgnores word order or grammar and only represent how often words appear\n\nNote: In text analysis, we often use the term “document” to refer to any unit of text we’re analyzing. This could be a book, an article, a tweet, or even a single sentence, depending on our research goals.\n\n\n\nLet’s look at a simple example to understand how BoW works:\n\nlibrary(tidytext)\nlibrary(dplyr)\n\n# Sample documents\ndocs &lt;- c(\n  \"The cat sat on the mat\",\n  \"The dog chased the cat\",\n  \"The mat was red\"\n)\n\n# Create a tibble\ntext_df &lt;- tibble(doc_id = 1:3, text = docs)\n\n# Tokenize and count words\nbow_representation &lt;- text_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  count(doc_id, word, sort = TRUE)\n\n# Display the BoW representation\nprint(bow_representation)\n\n# A tibble: 13 × 3\n   doc_id word       n\n    &lt;int&gt; &lt;chr&gt;  &lt;int&gt;\n 1      1 the        2\n 2      2 the        2\n 3      1 cat        1\n 4      1 mat        1\n 5      1 on         1\n 6      1 sat        1\n 7      2 cat        1\n 8      2 chased     1\n 9      2 dog        1\n10      3 mat        1\n11      3 red        1\n12      3 the        1\n13      3 was        1\n\n\nIn this representation: - Each row represents a word in a specific document - ‘doc_id’ identifies the document - ‘word’ is the tokenized word - ‘n’ represents the count of that word in the document\nThis BoW representation allows us to see the frequency of each word in each document, forming the basis for various text analysis techniques.\n\n\n\n\n\n\ncount() function\n\n\n\n\n\nThe count() function from dplyr is a powerful tool for summarizing data:\n\nPurpose: Counts the number of rows with each unique combination of variables.\nBasic usage: count(data, variable_to_count)\n\nKey features:\n\nAutomatically arranges results in descending order with sort = TRUE\n\nCan count multiple variables at once: count(var1, var2)\n\nAllows custom naming of the count column: count(var, name = \"my_count\")\n\n\n\nTip: Combine with slice_head() or slice_max() to focus on top results\n\n\n\n\n\n\n\n\n\n\nAdvantages and Limitations of BoW\n\n\n\n\n\nAdvantages: - Simple and intuitive - Computationally efficient - Effective for many text classification tasks\nLimitations: - Loses word order information - Can result in high-dimensional, sparse vectors - Doesn’t capture semantics or context - Ignores word relationships and meanings\n\n\n\nDespite its limitations, the Bag-of-Words model remains a fundamental technique in text analysis, often serving as a starting point for more complex analyses or as a baseline for comparing more sophisticated models.\nNow, let’s generate the BoW representation of our preprocessed DH Keywords data.\n\nword_frequencies &lt;- dh_keywords_clean %&gt;%\n  count(word, sort = TRUE)\n\nword_frequencies\n\n# A tibble: 120 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 data           61\n 2 ai             50\n 3 coding         34\n 4 analysis       18\n 5 technology     17\n 6 science        14\n 7 computer       13\n 8 programming    11\n 9 statistics      9\n10 ethics          7\n# ℹ 110 more rows",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#word-clouds",
    "href": "book/06-qta.html#word-clouds",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n4.1 Word Clouds",
    "text": "4.1 Word Clouds\nWord clouds are a popular way to visualize the most frequent words in a text, with the size of each word proportional to its frequency.\n\np_load(wordcloud2)\n\n# Create a word cloud\nwordcloud2(data = word_frequencies %&gt;% slice_head(n = 50), size = 0.5)\n\n\n\n\n\nAlternatively, you can also use the following code to tweak the style of the plot:\n\n# Make it more aesthetically pleasing\np_load(RColorBrewer)\n\n# Create a color palette\ncolor_palette &lt;- brewer.pal(8, \"Dark2\")\n\nwordcloud2(\n  data = word_frequencies %&gt;% slice_head(n = 50),  # Use top 50 most frequent words\n  size = 0.6,                    # Increase text size for better readability\n  color = rep_len(color_palette, 50),  # Apply color palette to words\n  backgroundColor = \"white\",     # Set background color to white\n  rotateRatio = 0.3,             # Reduce word rotation for cleaner look\n  shape = \"circle\",              # Set overall shape of the word cloud\n  fontFamily = \"Arial\",          # Use Arial font for consistency\n  fontWeight = \"bold\",           # Make text bold for emphasis\n  minRotation = -pi/6,           # Set minimum rotation angle (30 degrees left)\n  maxRotation = pi/6             # Set maximum rotation angle (30 degrees right)\n)",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#bar-charts-of-word-frequency",
    "href": "book/06-qta.html#bar-charts-of-word-frequency",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n4.2 Bar Charts of Word Frequency",
    "text": "4.2 Bar Charts of Word Frequency\nBar charts offer a more precise way to visualize word frequencies, especially for comparing the most common words.\n\np_load(ggplot2)\n\nword_frequencies %&gt;%\n  slice_head(n = 20) %&gt;%\n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col(fill=\"Steelblue\") + # Color all bars in blue\n  coord_flip() +\n  theme_minimal()+\n  theme(legend.position = \"none\") +\n  labs(x = \"Word\", y = \"Frequency\", title = \"Top 10 Most Frequent DH Keywords\")\n\n\n\n\n\n\n\nNow, what problem do you notice in the above analysis? You may notice that the top keywords/words are perhaps too decontexualized. For instance, the “science” and “computer” entries are perhaps originally “computer science”.\nHow do we add a bit more contextual information to this?",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#n-grams",
    "href": "book/06-qta.html#n-grams",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n4.3 N-grams",
    "text": "4.3 N-grams\nN-grams are sequences of n items from a given text. These items can be words, characters, or even syllables. N-grams help capture phrases and word associations, providing context that single words might miss. This approach can address some of the limitations we observed with the Bag-of-Words model, particularly its inability to preserve word order and capture multi-word concepts.\n\n\n\n\n\n\n\nUnderstanding N-grams\n\n\n\n\n\n\nUnigrams (n=1): Single words, e.g., “digital”, “humanities”\nBigrams (n=2): Two consecutive words, e.g., “digital humanities”\nTrigrams (n=3): Three consecutive words, e.g., “natural language processing”\nAnd so on for larger n…\n\nN-grams preserve word order, which can be crucial for understanding meaning and context in text analysis.\n\n\n\nBy using n-grams, we can potentially recover some of the multi-word terms that were split in our earlier analysis, such as “computer science” or “artificial intelligence”.\n\nbigrams &lt;- dh_keywords %&gt;%\n  unnest_ngrams(bigram, keyword, n = 2)\n\n# Display the most common bigrams\nbigrams %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  filter(nchar(bigram) &gt; 0) %&gt;%\n  head(n = 10)\n\n# A tibble: 10 × 2\n   bigram               n\n   &lt;chr&gt;            &lt;int&gt;\n 1 ai coding            6\n 2 coding data          6\n 3 data ai              6\n 4 data coding          6\n 5 computer science     5\n 6 data analysis        5\n 7 ai data              4\n 8 social science       4\n 9 analysis coding      3\n10 coding analysis      3\n\n\n:::\n\n\n\n\n\n\nConsidering Multiple N-gram Sizes\n\n\n\n\n\nResearchers often use a combination of n-gram sizes (e.g., unigrams, bigrams, and trigrams) for several reasons:\n\n\nComprehensive Coverage: Different n-gram sizes capture different aspects of language.\n\nBalancing Specificity and Generality: Larger n-grams are more specific but occur less frequently.\n\nHandling Varied Phrases: Some meaningful phrases are two words, others three or more.\n\nHowever, using multiple n-gram sizes also has challenges:\n\n\nIncreased Complexity: More data to process and analyze.\n\nOverlapping Information: Bigrams and trigrams may contain redundant information.\n\nSparse Data: Larger n-grams may occur very infrequently, leading to sparse data issues.\n\nThe choice depends on your research questions and the nature of your texts.\n\n\n\nHere’s an example of how we might generate n-grams of different sizes:\n\np_load(tidytext)\n\nmixed_ngrams = dh_keywords %&gt;%\n  unnest_ngrams(ngrams, keyword, n = 3, n_min = 1)\n\n\n# Display the top 5 of each\nmixed_ngrams %&gt;%\n  count(ngrams, sort = TRUE) %&gt;%\n  mutate(keyword_len = str_count(ngrams, \"\\\\S+\")) %&gt;%\n  group_by(keyword_len) %&gt;%\n  slice_max(n, n = 5) \n\nThis approach allows us to see the most common phrases of different lengths in our dataset, providing a more comprehensive view of the language used in digital humanities keywords.",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  },
  {
    "objectID": "book/06-qta.html#learning-check-1",
    "href": "book/06-qta.html#learning-check-1",
    "title": "§6 Quantitative Text Analysis Basics 📊",
    "section": "\n4.4 Learning Check 🏁",
    "text": "4.4 Learning Check 🏁",
    "crumbs": [
      "Book",
      "§6 Quantitative Text Analysis Basics 📊"
    ]
  }
]