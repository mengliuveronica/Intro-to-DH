---
title: "¬ß6 Quantitative Text Analysis Basics üìä"
format:
  html:
    toc: true
    code-fold: false
    code-link: true
    highlight-style: github
    include-in-header:
      text: |
        <script src="quiz.js"></script>
        <script src="content-switch.js"></script>
filters: 
 - collapse-callout.lua
execute:
  freeze: auto
---

```{r setup, include=FALSE}
# Load required packages
library(here)
library(readr)

# Function to find the project root
find_root <- function() {
  if (file.exists("_quarto.yml")) {
    return(".")
  } else if (file.exists("../_quarto.yml")) {
    return("..")
  } else {
    stop("Cannot find _quarto.yml in current or parent directory")
  }
}

# Set the project root directory
root <- find_root()
here::i_am(file.path(root, "_quarto.yml"))

# Print current working directory and project root for verification
print(getwd())
print(here())
print(list.files(here()))

# If we're in the 'book' subdirectory, set the working directory accordingly
if (basename(getwd()) == "book") {
  knitr::opts_knit$set(root.dir = here())
} else {
  knitr::opts_knit$set(root.dir = here("book"))
}

# Verify the contents of the current working directory
print(list.files())
```

Welcome to the world of quantitative text analysis! In this chapter, we'll explore the fundamental concepts and techniques used to prepare text data for analysis. We'll use a collection of classic novels as our dataset to demonstrate these concepts.

::: {.callout-note}
## Learning Objectives
- üìñ Understand the basics of text analysis and its applications in Digital Humanities
- üßπ Learn essential text preprocessing techniques
- üî§ Explore basic text analysis concepts like tokenization and word frequency
- üìä Visualize text data using simple techniques like word clouds
:::

# What is Quantitative Text Analysis? ü§î

Quantitative text analysis (QTA) is a method of analyzing large volumes of text data using computational techniques. It allows researchers to extract meaningful patterns, themes, and insights from textual data that would be difficult or impossible to analyze manually.

## QTA in Digital Humanities

In Digital Humanities, QTA offers powerful tools for exploring large text corpora:

1. **Scale**: Analyze vast collections of texts, revealing patterns across time periods, genres, or authors.

2. **Distant Reading**: Observe broader patterns in literature and cultural production.

3. **Hypothesis Testing**: Empirically test literary and cultural theories across large datasets.

4. **Discovery**: Reveal unexpected patterns or connections, sparking new research questions.

5. **Interdisciplinary**: Combine methods from linguistics, computer science, and statistics with humanistic inquiry.

6. **Visualization**: Present textual data in new, visually interpretable ways.

QTA complements traditional close reading, offering Digital Humanities scholars new perspectives on cultural, historical, and literary phenomena.

## QTA Workflow

![](images/QTA%20workflow.png)


The Quantitative Text Analysis (QTA) workflow illustrates the systematic process of analyzing textual data using computational methods. This workflow can be divided into five main stages:

1. **Data Acquisition**:

   - Data retrieval from various sources
   - Web scraping techniques
   - Social media API integration
   - ...

2. **Text Preprocessing and Standardization**:

   - **Tidy structuring**: Organizing data into a consistent format
   - **Noise removal**: Eliminating capitalization, special characters, punctuation, and numbers
   - **Tokenization**: Breaking text into individual units (words, sentences)
   - **Stopwords removal**: Eliminating common words with little semantic content
   - **Stemming or lemmatization**: Reducing words to their root forms
   - **POS (Part-of-Speech) tagging**: Labeling words with their grammatical categories
   - **Named Entity Recognition (NER)**: Identifying and classifying named entities in text
   - ...

3. **Text Representation**:

   - **Count-based methods**:
     * Bag-of-words representation
     * Term frequency-inverse document frequency (TF-IDF)
     * ...
   - **Context-based methods**:
     * N-grams analysis
     * Co-occurrence matrix
     * Word embeddings (e.g., Word2Vec, GloVe)
     * ...

4. **Data Analysis and Modeling**:

   - **Sentiment Analysis**: Determining the emotional tone of text
   - **Topic Modeling**: Discovering abstract topics in a collection of documents
   - **Social Network Analysis**: Analyzing relationships and structures in text data
   - ...

5. **Visualization & Reporting**:

   - Visualizing sentiment trends over time or across categories
   - Creating network graphs to represent relationships in text data
   - Displaying topic distributions and their evolution
   - Summarizing findings and insights in reports or publications
   - ...

This workflow emphasizes the importance of a structured approach to text analysis, from raw data to final insights. Each stage builds upon the previous one, with the flexibility to iterate or adjust methods based on the specific requirements of the analysis and research questions. The process can be iterative, with researchers often moving back and forth between stages as needed to refine their analysis and results.



:::{.callout-tip}
## Install Package Manager `pacman`

We will be using some new packages that you probably haven't installed. To streamline the process of package installation, let's introduce a helpful tool for managing R packages: `pacman`. The `pacman` package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.

Key features of `pacman`:

1. It combines the functionality of `install.packages()` and `library()` into a single function `p_load()`.
2. It automatically installs packages if they're not already installed.
3. It can load multiple packages with a single line of code.

Let's install `pacman`:

```{r}
if (!require("pacman")) install.packages("pacman")

```

Once we load the `pacman` library, we can use `p_load()` to efficiently load (and install if necessary) the packages we'll need:

```{r}
library(pacman)
p_load(tidyverse, tidytext)
```

This single line will ensure all the packages we need are installed and loaded, streamlining our setup process.
:::



# Text Preprocessing and Standardization Techniques üßπ

![](images/preprocessing.png)

## Understanding Text Preprocessing

Text preprocessing is the crucial first step in quantitative text analysis. It involves cleaning and standardizing raw text data to make it suitable for computational analysis.

**Why is it important?**

- Improves data quality and consistency
- Reduces noise and irrelevant information
- Enhances the accuracy of subsequent analyses
- Makes text data more manageable for computational processing

**Fundamental considerations:**

1. **Purpose**: The preprocessing steps you choose should align with your research questions and analysis goals.
2. **Language**: Different languages may require specific preprocessing techniques.
3. **Domain**: The nature of your texts (e.g., literary, social media, historical) may influence preprocessing decisions.
4. **Information preservation**: Be cautious not to remove potentially important information during preprocessing.
5. **Reproducibility**: Document your preprocessing steps to ensure your analysis can be replicated.

Remember, there's no one-size-fits-all approach to preprocessing. The techniques you apply should be carefully considered based on your specific research context.


To showcase the various techniques for text preprocessing, let's first create a mock dataset:

```{r}
mock_data <- tibble(
  text = c(
    "The Quick Brown Fox Jumps Over the Lazy Dog! Data Science meets Cultural Studies.",
    "Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literature & History",
    "R Programming for Text Analysis - Chapter 3. Machine Learning for Textual Analysis",
    "NLP techniques & their applications in DH research; Computational Methods in Humanities Research?",
    "20+ ways to visualize data üìä: graphs, charts, and more! Digital Archives and Text Mining Techniques."
  )
)

```

Let's take a closer look at our mock dataset:

```{r}
print(mock_data)
```

::: {.callout-note}
## Reflection ü§î

Examine the mock dataset above and reflect on the following questions:

- What characteristics or elements do you notice that might need preprocessing for effective text analysis? 

- What challenges might these elements pose for text analysis? How might preprocessing help address these challenges?
:::

<details>
<summary>Click to Reveal Some Insights</summary>

1. **Capitalization**: Words are inconsistently capitalized (e.g., "The Quick Brown Fox" vs. "Data Science"). This could lead to treating identical words as different entities.

2. **Punctuation**: Various punctuation marks are present, including periods, exclamation marks, colons, and semicolons. These might interfere with word tokenization and analysis.

3. **Numbers**: Some entries contain numbers (e.g., "101", "2024", "3", "20+"). Depending on the analysis goals, these might need to be removed or treated specially.

4. **Special Characters**: There are ampersands (&) and hyphens (-) which might need special handling.

5. **Sentence Structure**: Each row contains multiple sentences. For sentence-level analysis, we might need to split these.

6. **Abbreviations**: "NLP" and "DH" are present. We might need to decide whether to expand these or treat them as single tokens.

7. **Stop Words**: Common words like "the", "and", "for" are present. These might not contribute much meaning to the analysis.

These observations highlight the need for various preprocessing steps, including:
- Converting text to lowercase for consistency
- Removing or standardizing punctuation
- Handling numbers and special characters
- Sentence or word tokenization
- Removing stop words

By addressing these elements through preprocessing, we can prepare our text data for more effective and accurate analysis.
</details>

## Understanding Regular Expressions

Before we dive into analyzing our mock data, let's explore a powerful tool in text analysis: **Regular Expressions**. 

Have you ever wondered how computer programs like Excel or Word can find the exact word or phrase you're searching for? Or how they can replace all instances of a word throughout a document in just seconds? These everyday text operations are powered by a concept called **pattern matching**, and regular expressions take this idea to a whole new level.

Regular Expressions, often called **regex**, are like a special language for describing patterns in text. Imagine you're a librarian with a magical magnifying glass that can find not just specific words, but patterns in books.

### Key Concepts

1. **Pattern Matching**: Instead of searching for exact words, regex lets you search for patterns. For example, you could search for:
   - All words that start with "pre"
   - Any sequence of five letters
   - All dates in a specific format

2. **Text Processing**: Once you find these patterns, you can do things like:
   - Highlight them
   - Replace them with something else
   - Extract them for further study

### Examples of Regex
To give you a better idea of what regular expressions look like and how they work, let's look at an example:

```{r}
p_load(stringr)

# Sample text
text <- "Jane Austen wrote Pride and Prejudice. Elizabeth Bennet is the protagonist."

# Regex pattern for capitalized words
pattern <- "\\b[A-Z][a-z]+\\b"

# Find all matches
matches <- str_extract_all(text, pattern)

# Print the matches
print(matches)

# To see which words were matched in context
str_view(text, pattern)
```

:::{.callout-note}
## Regex breakdown

Let's break down the regex pattern `\\b[A-Z][a-z]+\\b`:

1. `\\b`: This represents a word boundary. In R, we need to escape the backslash, so we use two. It ensures we're matching whole words, not parts of words.

2. `[A-Z]`: This character class matches any single uppercase letter from A to Z.

3. `[a-z]+`: This matches one or more lowercase letters.
   - `[a-z]` is a character class that matches any single lowercase letter.
   - The `+` quantifier means "one or more" of the preceding element.

4. `\\b`: Another word boundary to end the match.

So, this pattern matches:
- Words that start with a capital letter (like names or the first word of a sentence)
- Followed by one or more lowercase letters
- As whole words, not parts of larger words

It won't match:
- ALL CAPS words
- words with numbers or symbols
- Single-letter capitalized words like "I" or "A"

This pattern is useful for finding proper nouns in the middle of sentences, like names of people or places.
:::


### POSIX Character Classes: A Friendly Starting Point

You may find that regex can be quite hard to read for humans. POSIX character classes are pre-defined sets of characters that make regex more accessible and portable across different systems. They simplify regex patterns and address some common challenges in text processing:

1. **Simplification**: POSIX classes provide easy-to-remember shorthand for common character groups. Instead of writing `[A-Za-z]` to match any letter, you can use `[:alpha:]`.

2. **Consistency**: They ensure consistent behavior across different operating systems and programming languages. For example, `[A-Z]` might behave differently in some contexts depending on the locale settings, but `[:upper:]` is always consistent.

3. **Internationalization**: POSIX classes can handle characters beyond the ASCII range, making them useful for working with texts in various languages.

4. **Readability**: They make regex patterns more readable and self-explanatory, which is especially helpful when sharing code or working in teams.

Here are some useful POSIX character classes:

- `[:alpha:]`: Matches any alphabetic character (equivalent to `[A-Za-z]` in English texts)
- `[:digit:]`: Matches any digit (equivalent to `[0-9]`)
- `[:lower:]`: Matches any lowercase letter
- `[:upper:]`: Matches any uppercase letter
- `[:punct:]`: Matches any punctuation character
- `[:space:]`: Matches any whitespace character (spaces, tabs, newlines)

By using these classes, you can create more robust and readable regex patterns. For example, instead of `[A-Za-z0-9]` to match any alphanumeric character, you could use `[[:alpha:][:digit:]]`, which is clearer in its intent and works across different language settings.

#### Examples in Humanities Context

1. Finding capitalized words: 
   - Pattern: `[[:upper:]][[:lower:]]+`
   - This would match words like "Shakespeare", "London", "Renaissance"

2. Identifying years:
   - Pattern: `[[:digit:]]{4}`
   - This would match years like "1564", "1616", "2023"

3. Locating punctuation:
   - Pattern: `[[:punct:]]`
   - This would find all punctuation marks in a text

Remember, regex is a tool that becomes more useful as you practice. Start simple, and you'll gradually be able to create more complex patterns for your research needs!


## Learning Check üèÅ 

<div id="quiz-qta-intro"></div>

<script>
createQuiz({
  questions: [
    {
      text: "What is the primary purpose of Quantitative Text Analysis (QTA)?",
      options: [
        "To translate texts into different languages",
        "To analyze large volumes of text data using computational techniques",
        "To improve the grammar and style of written texts",
        "To create new literary works using artificial intelligence"
      ]
    },
    {
      text: "Which of the following is NOT a benefit of QTA in Digital Humanities?",
      options: [
        "Analyzing vast collections of texts",
        "Observing broader patterns in literature",
        "Replacing close reading entirely",
        "Empirically testing literary theories"
      ]
    },
    {
      text: "In the QTA workflow, what needs to be done before text can be transformed to numbers (i.e., text representation)?",
      options: [
        "Topic Modeling",
        "Sentiment Analysis",
        "Preprocessing Steps",
        "Visualization"
      ]
    },
    {
      text: "What is the main purpose of regular expressions (regex) in text analysis?",
      options: [
        "To translate text between languages",
        "To find and match patterns in text",
        "To increase the font size of text",
        "To create new words in a language"
      ]
    },
    {
      text: "Which POSIX character class would you use to match any punctuation mark?",
      options: [
        "[:alpha:]",
        "[:digit:]",
        "[:lower:]",
        "[:punct:]"
      ]
    }
  ],
  answers: [1, 2, 2, 1, 3]
}, "quiz-qta-intro");
</script>


Now let's proceed to preprocess the mock dataset!

## Tidy Structuring

The first step in our preprocessing pipeline is to ensure our data is in a tidy format. Remember a tidy format means one observation per row and the unit of the observation, whether it is a sentence, or a word, varies from project to project depending on the nature of your study and research questions. For this example, let's assume we want one sentence per row. We'll use `separate_rows()` with a regular expression (regex) pattern to achieve this:

```{r}
tidy_data <- mock_data %>%
  separate_rows(text, sep = "(?<=[.!?])\\s+(?=[A-Z])")

print(tidy_data)
```


::: {.callout-note}
## Understanding `separate_rows()` 

1. `separate_rows()` Function:
   - Part of the tidyr package in tidyverse
   - Separates a column into multiple rows based on a delimiter
   - Syntax: `separate_rows(data, column, sep = delimiter)`

2. Regular Expression (Regex) Pattern:
   - `(?<=[.!?])`: Positive lookbehind, matches a position after a period, exclamation mark, or question mark
   - `\\s+`: Matches one or more whitespace characters
   - `(?=[A-Z])`: Positive lookahead, matches a position before an uppercase letter
   - Combined, this pattern splits the text at sentence boundaries

3. How it works:
   - `separate_rows()` applies the regex pattern to split the 'text' column
   - Each resulting sentence becomes a new row in the dataframe
   - The original row's other columns (if any) are duplicated for each new sentence row
:::


## Noise Removal

### Capitalization Removal

Converting text to lowercase is a common preprocessing step that helps standardize the text and reduce the vocabulary size. Let's apply this to our tidy data:

```{r}
lowercase_data <- tidy_data %>%
  mutate(text = tolower(text))

print(lowercase_data)
```

### Punctuation and Special Character Removal

Removing punctuation and special characters can help focus on the words themselves. We'll use a regular expression to remove these:

```{r}
clean_data <- lowercase_data %>%
  # Remove punctuation
  mutate(text = str_replace_all(text, "[[:punct:]]", "")) %>%
  # Remove special characters (including emojis)
  mutate(text = str_replace_all(text, "[^[:alnum:][:space:]]", ""))

print(clean_data)
```

### Numbers Removal

Depending on your analysis goals, you might want to remove numbers. Here's how to do that:

```{r}
no_numbers_data <- clean_data %>%
  mutate(text = str_replace_all(text, "\\d+", ""))

print(no_numbers_data)
```

## Tokenization

::: {.callout-note}
## What is a token?
In text analysis, a "token" is the smallest unit of text that we analyze. Most commonly, a token is a single word, but it could also be a character, a punctuation mark, or even a phrase, depending on our analysis needs.

For example, in the sentence "The quick brown fox", each word ("The", "quick", "brown", "fox") could be considered a token.
:::

::: {.callout-tip}
## What is tokenization?
Tokenization is the process of breaking down a piece of text into its individual tokens. It's like taking a sentence and splitting it into a list of words.

Why do we do this? Computers can't understand text the way humans do. By breaking text into tokens, we create a format that's easier for computers to process and analyze.
:::

We'll use the `unnest_tokens()` function from the `tidytext` package:

```{r}
p_load(tidytext)

tokenized_data <- no_numbers_data %>%
  unnest_tokens(output = word, input = text, token = "words")

print(tokenized_data)

```

:::{.callout-tip}
## `unnest_tokens()` function

The `unnest_tokens()` function from the `tidytext` package is a powerful tool for text analysis:

- Purpose: Splits a column into tokens, creating a one-token-per-row structure.

Key Arguments:

- output: Name of the new column for tokens
- input: Column to be tokenized
- token: Tokenization unit (e.g., "words", "sentences", "ngrams")

The `unnest_tokens()` is built on the `tokenizers` package, which means there are a lot of arguments you can tweak to perform multiple operations at once. Below are some common features that you may want to pay attention to:

Features:

- Convert to lowercase by default (to_lower = TRUE)
- Strip punctuations by default (strip_punct = TRUE)
- Preserves numbers by default (strip_numeric = FALSE)
- Automaticlaly removes extra white space

:::


## Stopwords Removal

::: {.callout-note}
## What are stopwords?

Stopwords are common words that are often removed from text analysis because they typically don't carry much meaning on their own (i.e., "filler" words in language). These are words like "the", "is", "at", "which", and "on".

In text analysis, removing stopwords helps us focus on the words that are more likely to carry the important content or meaning of the text. It's like distilling a sentence down to its key concepts.
:::

Now, let's remove stopwords from our tokenized data:

```{r}
data(stop_words)

data_without_stopwords <- tokenized_data %>%
  anti_join(stop_words)
```

::: {.callout-tip}
## `anti_join()` Function

The `anti_join()` function is a way to remove items from one list based on another list. Here is how it works:

1. Imagine you have two lists:

   - List A: Your main list (in this case, our tokenized keywords)
   - List B: A list of items you want to remove (in this case, stopwords)

2. `anti_join()` goes through List A and removes any item that appears in List B.

3. The result is a new list that contains only the items from List A that were not in List B.

In our context:

- List A is our tokenized keywords
- List B is the list of stopwords
- The result is our keywords with all the stopwords removed

It's like having a big box of Lego bricks (your keywords) and a list of colors you don't want (stopwords). `anti_join()` helps you quickly remove all the Lego bricks of those unwanted colors, leaving you with only the colors you are interested in.

This function is particularly useful for cleaning text data, as it allows us to efficiently remove common words that might not be relevant to our analysis.
:::


::: {.callout-warning}
## Caution with Stopword Removal

While stopword removal is a common preprocessing step, it's important to consider its implications:

1. **Context Matters**: Words considered "stop words" in one context might be meaningful in another. For example, "the" in "The Who" (band name) carries meaning.

2. **Negations**: Removing words like "not" can invert the meaning of surrounding words, potentially affecting sentiment analysis.

3. **Phrase Meaning**: Some phrases lose their meaning without stop words. "To be or not to be" becomes simply "be be" after stopword removal.

4. **Language Specificity**: Stop word lists are language-specific. Ensure you're using an appropriate list for your text's language.

5. **Research Questions**: Your specific research questions should guide whether and how you remove stop words. Some analyses might benefit from keeping certain stop words.
:::

::: {.callout-tip}
## Customizing Stopword Lists (Advanced)

The `stop_words` dataset in the tidytext package contains stop words from three lexicons: "onix", "SMART", and "snowball". You can customize your stopword removal by:

1. Using only one lexicon: 
   ```{r}
   stop_words %>% filter(lexicon == "snowball")
   ```

2. Adding or removing words from the list:
   ```{r}
   custom_stop_words <- stop_words %>%
     add_row(word = "custom_word", lexicon = "custom")
   ```

3. Creating your own domain-specific stop word list based on your corpus and research needs.
:::

Remember, the goal of stopword removal is to reduce noise in the data and focus on meaningful content. However, what constitutes "meaningful" can vary depending on your specific analysis goals and the nature of your text data.

## Stemming and Lemmatization

Stemming and lemmatization are text normalization techniques used to reduce words to their base or root form. This process helps in grouping together different inflected forms of a word, which can be useful for various text analysis tasks.

### What are Stemming and Lemmatization?

1. **Stemming** is a simple, rule-based process of removing the ends of words (suffixes) to reduce them to their base form. For example:

   - "running" ‚Üí "run"
   - "cats" ‚Üí "cat"
   - "better" ‚Üí "better" (remains unchanged)

2. **Lemmatization** is a more sophisticated approach that considers the context and part of speech of a word to determine its base form (lemma). For example:

   - "running" ‚Üí "run"
   - "better" ‚Üí "good"
   - "are" ‚Üí "be"

### When to Use Them?

- Use **stemming** when:
  * You need a quick, computationally efficient method.
  * The meaning of the stem is clear enough for your analysis.
  * You're working with a large dataset and processing speed is crucial.

- Use **lemmatization** when:
  * You need more accurate, dictionary-based results.
  * The precise meaning of the word is important for your analysis.
  * You have the computational resources to handle a more intensive process.

### Applying Stemming

In this example, we'll apply stemming using the SnowballC package, which implements the Porter stemming algorithm:

```{r}
p_load(SnowballC)

stemmed_data <- data_without_stopwords %>%
  mutate(stem = wordStem(word))

print(stemmed_data)
```


::: {.callout-tip}
## Lemmatization Alternative

For lemmatization, you can use the `textstem` package:

```{r}
p_load(textstem)

lemmatized_data <- data_without_stopwords %>%
  mutate(lemma = lemmatize_words(word))

print(lemmatized_data)
```

Lemmatization often produces more intuitive results but can be slower for large datasets.
:::

### Stemming vs. Lemmatization

In humanities research, the choice between stemming and lemmatization can significantly impact your analysis:

1. **Preserving Meaning**: Lemmatization often preserves meaning better, which can be crucial for literary analysis or historical research.

2. **Handling Irregular Forms**: Lemmatization is better at handling irregular forms (e.g., "better" ‚Üí "good"), which is common in many languages and especially important in analyzing older texts.

3. **Computational Resources**: If you're working with vast corpora, stemming might be more practical due to its speed.

4. **Language Specificity**: For languages other than English, or for multilingual corpora, lemmatization often provides better results as it's more adaptable to language-specific rules.

5. **Research Questions**: Consider your research questions. If distinguishing between, say, "democratic" and "democracy" is crucial, lemmatization might be more appropriate.

Remember, the choice of preprocessing steps, including whether to use stemming or lemmatization, should be guided by your specific research questions and the nature of your text data. It's often helpful to experiment with different approaches and see which yields the most meaningful results for your particular study.

## Hands-On Coding üíª

Now, let's apply our text preprocessing skills to a real dataset. We'll use the answers submitted by you in response to the "what is digital humanities? write down three keywords that come to mind" question. Our goal is to clean and prepare this text data for analysis.

First, let's download the dataset:

[Download DH Keywords dataset](data/dh_keywords.csv)

Save it in the folder name `data` in your working directory. 

We can then load the csv file into RStudio:

```{r}
p_load(tidyverse)

# Load the dataset
dh_keywords <- read_csv("data/dh_keywords.csv", col_names = FALSE)
colnames(dh_keywords) <- "keyword"

# Display the first few rows
head(dh_keywords)
```


#### Exercise 1: Planning Preprocessing Steps

Before we start coding, let's think about what steps we need to take to clean and standardize our DH keywords dataset, and in what order these steps should be performed.

Look at the first few rows of our dataset:

```{r}
head(dh_keywords)
```

Now, consider the following questions:

1. What issues do you notice in the data that need to be addressed?
2. What preprocessing steps would you take to clean this data?
3. In what order should these steps be performed, and why?

<div id="hint-plan-1" style="display: none;">
::: {.callout-tip collapse="false"}
## Hint 1
Notice that many rows contain more than one word. We may need to either separate rows so that there is only one entry per row or tokenize the data so that there is only one word per row.
:::
</div>

<div id="hint-plan-2" style="display: none;">
::: {.callout-tip collapse="false"}
## Hint 2
After tokenization, consider issues like inconsistent capitalization, punctuation, extra whitespace, and potentially empty entries.
:::
</div>


<script>
createProgressiveHints('hint-plan', 2);
</script>

<div id="solution-plan" style="display: none;">

A possible preprocessing plan could be:

1. Tokenize the data so that we have one word per row.
   - This ensures we're working with individual words.
2. Convert all text to lowercase
   - This ensures consistency in our text data
3. Remove all punctuation
   - This focuses our analysis on the words themselves
4. Trim leading and trailing whitespace and replace multiple spaces with a single space
   - This cleans up any formatting inconsistencies
5. Remove stop words
   - This eliminates common words that don't carry much meaning in analysis
6. Remove any empty entries that might result from the above steps
   - This ensures we're only working with meaningful data

This order ensures that we first isolate individual keywords, then standardize the text (lowercase) before removing elements (punctuation), clean up spacing issues, and finally remove any entries that became empty as a result of our preprocessing.

</div>

<script>
createToggleSection('solution-button-plan', 'solution-plan', 'Show Possible Plan');
</script>

#### Exercise 2: Implementing the Preprocessing Steps

Now that we have a plan, let's implement these preprocessing steps in R. Use the `tidyr`, `dplyr`, and `stringr` packages to clean the `dh_keywords` dataset according to your plan.

<div id="hint-implement-1" style="display: none;">
::: {.callout-tip collapse="false"}
## Hint 1
Use `unnest_tokens()` from tidytext to tokenize the data into individual words. Remember, by default this will take care of the lowercasing and punctuation removal for us.
:::
</div>

<div id="hint-implement-2" style="display: none;">
::: {.callout-tip collapse="false"}
## Hint 2
Use `anti_join()` with a stop words dataset to remove common words.
:::
</div>

<div id="hint-implement-3" style="display: none;">
::: {.callout-tip collapse="false"}
## Hint 3
Use `filter()` with `nchar()` to remove empty entries.
:::
</div>

<script>
createProgressiveHints('hint-implement', 3);
</script>

<div id="template-implement" style="display: none;">

````{r eval=FALSE}
p_load(tidytext)

dh_keywords_clean <- dh_keywords %>%
  unnest_tokens(output = word, input = keyword) %>%
  anti_join(stop_words) %>%
  filter(_ > _)

# Display the final result and the number of rows
print(dh_keywords_clean)
paste("Number of keywords after preprocessing:", nrow(dh_keywords_clean))
````

</div>

<script>
createToggleSection('template-button-implement', 'template-implement', 'Show Template');
</script>

<div id="solution-implement" style="display: none;">

````{r}
p_load(tidytext)

dh_keywords_clean <- dh_keywords %>%
  unnest_tokens(output = word, input = keyword) %>%
  anti_join(stop_words) %>%
  filter(nchar(word) > 0)

# Display the final result and the number of rows
print(dh_keywords_clean)
paste("Number of keywords after preprocessing:", nrow(dh_keywords_clean))
````

</div>

<script>
createToggleSection('solution-button-implement', 'solution-implement', 'Show Solution');
</script>




# Text Representation üßÆ

After preprocessing our text data, the next crucial step is to transform it into a format that machines can understand and analyze. This process is called text representation.

::: {.callout-note}
## Key Concepts in Text Representation

1. **Count-based methods**:

   - Bag-of-words representation: Represents text as an unordered collection of words, counting their occurrences.
   - Term frequency-inverse document frequency (TF-IDF): Reflects word importance in a document relative to a corpus.

2. **Context-based methods**:

   - N-grams analysis: Examines sequences of N words to capture local context.
   - Co-occurrence matrix: Represents how often words appear together within a certain distance.
   - Word embeddings (e.g., Word2Vec, GloVe): Dense vector representations capturing semantic relationships.
:::

Let's first focus on one of the most fundamental and widely used text representation methods: the Bag-of-Words (BoW) model.

## Bag-of-Words (BoW)

The Bag-of-Words model is a simple yet powerful way to represent text data. It treats each document (unit of analysis) as an unordered collection of words, disregarding grammar and word order but keeping track of word frequency.
![](images/BOW.png)

::: {.callout-tip}
## Key Features of Bag-of-Words

- Counts how many times each word appears in a text or document
- Creates a list of all unique words used across all documents
- Treats each unique word as a separate piece of information
- Ignores word order or grammar and only represent how often words appear

Note: In text analysis, we often use the term "document" to refer to any unit of text we're analyzing. This could be a book, an article, a tweet, or even a single sentence, depending on our research goals.
:::

Let's look at a simple example to understand how BoW works:

```{r}
library(tidytext)
library(dplyr)

# Sample documents
docs <- c(
  "The cat sat on the mat",
  "The dog chased the cat",
  "The mat was red"
)

# Create a tibble
text_df <- tibble(doc_id = 1:3, text = docs)

# Tokenize and count words
bow_representation <- text_df %>%
  unnest_tokens(word, text) %>%
  count(doc_id, word, sort = TRUE)

# Display the BoW representation
print(bow_representation)
```

In this representation:
- Each row represents a word in a specific document
- 'doc_id' identifies the document
- 'word' is the tokenized word
- 'n' represents the count of that word in the document

This BoW representation allows us to see the frequency of each word in each document, forming the basis for various text analysis techniques.


::: {.callout-tip}
## `count()` function

The `count()` function from `dplyr` is a powerful tool for summarizing data:

- Purpose: Counts the number of rows with each unique combination of variables.
- Basic usage: `count(data, variable_to_count)`
- Key features:
  * Automatically arranges results in descending order with `sort = TRUE`
  * Can count multiple variables at once: `count(var1, var2)`
  * Allows custom naming of the count column: `count(var, name = "my_count")`
- Tip: Combine with `slice_head()` or `slice_max()` to focus on top results
:::

::: {.callout-note}
## Advantages and Limitations of BoW

Advantages:
- Simple and intuitive
- Computationally efficient
- Effective for many text classification tasks

Limitations:
- Loses word order information
- Can result in high-dimensional, sparse vectors
- Doesn't capture semantics or context
- Ignores word relationships and meanings
:::

Despite its limitations, the Bag-of-Words model remains a fundamental technique in text analysis, often serving as a starting point for more complex analyses or as a baseline for comparing more sophisticated models.


Now, let's generate the BoW representation of our preprocessed DH Keywords data.

```{r}
word_frequencies <- dh_keywords_clean %>%
  count(word, sort = TRUE)

word_frequencies
```

# Visualizing Text Data üìä

Now that we have transformed our keywords into numbers, how do we visualize them?

## Word Clouds

Word clouds are a popular way to visualize the most frequent words in a text, with the size of each word proportional to its frequency.

```{r}
p_load(wordcloud2)

# Create a word cloud
wordcloud2(data = word_frequencies %>% slice_head(n = 50), size = 0.5)

```

Alternatively, you can also use the following code to tweak the style of the plot:

```{r eval=FALSE}

# Make it more aesthetically pleasing
p_load(RColorBrewer)

# Create a color palette
color_palette <- brewer.pal(8, "Dark2")

wordcloud2(
  data = word_frequencies %>% slice_head(n = 50),  # Use top 50 most frequent words
  size = 0.6,                    # Increase text size for better readability
  color = rep_len(color_palette, 50),  # Apply color palette to words
  backgroundColor = "white",     # Set background color to white
  rotateRatio = 0.3,             # Reduce word rotation for cleaner look
  shape = "circle",              # Set overall shape of the word cloud
  fontFamily = "Arial",          # Use Arial font for consistency
  fontWeight = "bold",           # Make text bold for emphasis
  minRotation = -pi/6,           # Set minimum rotation angle (30 degrees left)
  maxRotation = pi/6             # Set maximum rotation angle (30 degrees right)
)
```

![](images/wordcloud2.png)

## Bar Charts of Word Frequency

Bar charts offer a more precise way to visualize word frequencies, especially for comparing the most common words.

```{r}
p_load(ggplot2)

word_frequencies %>%
  slice_head(n = 20) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(fill="Steelblue") + # Color all bars in blue
  coord_flip() +
  theme_minimal()+
  theme(legend.position = "none") +
  labs(x = "Word", y = "Frequency", title = "Top 10 Most Frequent DH Keywords")
```


Now, what problem do you notice in the above analysis? You may notice that the top keywords/words are perhaps too decontexualized. For instance, the "science" and "computer" entries are perhaps originally "computer science". 

How do we add a bit more contextual information to this?

## N-grams

N-grams are sequences of n items from a given text. These items can be words, characters, or even syllables. N-grams help capture phrases and word associations, providing context that single words might miss. This approach can address some of the limitations we observed with the Bag-of-Words model, particularly its inability to preserve word order and capture multi-word concepts.

![](images/ngrams.png)

::: {.callout-note}
## Understanding N-grams

- Unigrams (n=1): Single words, e.g., "digital", "humanities"
- Bigrams (n=2): Two consecutive words, e.g., "digital humanities"
- Trigrams (n=3): Three consecutive words, e.g., "natural language processing"
- And so on for larger n...

N-grams preserve word order, which can be crucial for understanding meaning and context in text analysis.
:::

By using n-grams, we can potentially recover some of the multi-word terms that were split in our earlier analysis, such as "computer science" or "artificial intelligence".


```{r}
bigrams <- dh_keywords %>%
  unnest_ngrams(bigram, keyword, n = 2)

# Display the most common bigrams
bigrams %>%
  count(bigram, sort = TRUE) %>%
  filter(nchar(bigram) > 0) %>%
  head(n = 10)
```

:::

::: {.callout-note}
## Considering Multiple N-gram Sizes

Researchers often use a combination of n-gram sizes (e.g., unigrams, bigrams, and trigrams) for several reasons:

1. **Comprehensive Coverage**: Different n-gram sizes capture different aspects of language.
2. **Balancing Specificity and Generality**: Larger n-grams are more specific but occur less frequently.
3. **Handling Varied Phrases**: Some meaningful phrases are two words, others three or more.

However, using multiple n-gram sizes also has challenges:

- **Increased Complexity**: More data to process and analyze.
- **Overlapping Information**: Bigrams and trigrams may contain redundant information.
- **Sparse Data**: Larger n-grams may occur very infrequently, leading to sparse data issues.

The choice depends on your research questions and the nature of your texts.
:::

Here's an example of how we might generate n-grams of different sizes:

```{r eval=FALSE}
p_load(tidytext)

mixed_ngrams = dh_keywords %>%
  unnest_ngrams(ngrams, keyword, n = 3, n_min = 1)


# Display the top 5 of each
mixed_ngrams %>%
  count(ngrams, sort = TRUE) %>%
  mutate(keyword_len = str_count(ngrams, "\\S+")) %>%
  group_by(keyword_len) %>%
  slice_max(n, n = 5) 
```

This approach allows us to see the most common phrases of different lengths in our dataset, providing a more comprehensive view of the language used in digital humanities keywords.


## Learning Check üèÅ

<div id="quiz-text-analysis"></div>

<script>
createQuiz({
  questions: [
    {
      text: "What is tokenization in text analysis?",
      options: [
        "The process of removing punctuation from text",
        "Breaking down text into smaller units like words or sentences",
        "Converting all text to lowercase",
        "Removing common words from text"
      ]
    },
    {
      text: "What are stopwords in text analysis?",
      options: [
        "Words that appear most frequently in a text",
        "Words that are misspelled in a text",
        "Common words like 'the', 'is', 'at' that are often removed from analysis",
        "Words that are unique to a specific text"
      ]
    },
    {
      text: "In a word cloud visualization, what does the size of a word typically represent?",
      options: [
        "The length of the word",
        "The alphabetical order of the word",
        "The frequency of the word in the text",
        "The sentiment of the word"
      ]
    },
    {
      text: "What is the primary purpose of the Bag-of-Words (BoW) model in text representation?",
      options: [
        "To preserve the grammatical structure of sentences",
        "To represent text as an unordered collection of word frequencies",
        "To analyze the sentiment of each word in a text",
        "To identify the main topics in a document"
      ]
    },
    {
      text: "Which of the following is NOT a limitation of the Bag-of-Words model?",
      options: [
        "It loses word order information",
        "It doesn't capture context or semantics",
        "It results in high-dimensional, sparse vectors",
        "It's computationally intensive and slow to process"
      ]
    }
  ],
  answers: [1, 2, 2, 1, 3]
}, "quiz-text-analysis");
</script>

This learning check will help reinforce the key concepts covered in this section on basic text analysis and visualization techniques.




# Conclusion

::: {.callout-note}
## Key Takeaways
In this chapter, we've covered:

- The basics of Quantitative Text Analysis (QTA) and its applications in Digital Humanities
- Essential text preprocessing techniques including tokenization, noise removal, and stopword removal
- Text representation methods, focusing on the Bag-of-Words model and n-grams
- Visualization techniques for text data, including word clouds and frequency bar charts
- Hands-on application of these concepts to analyze Digital Humanities keywords
:::

These foundational skills in Quantitative Text Analysis provide a powerful toolkit for exploring and analyzing textual data in Digital Humanities. As we progress in our journey, we'll build upon these concepts to perform more sophisticated text analysis techniques and derive deeper insights from textual data.

::: {.topic-flow}
::: {.topic-box .highlight-blue}
QTA Basics
:::
::: {.topic-box .highlight-green}
Text Preprocessing
:::
::: {.topic-box .highlight-yellow}
Text Representation
:::
::: {.topic-box .highlight-pink}
Text Visualization
:::
:::



<iframe src="float_traffic.html" width="200px" height="200px" style="border:none; position: fixed; bottom: 10px; right: 10px; z-index: 9999;" scrolling="no"></iframe>

<iframe src="float_dash.html" width="200px" height="200px" style="border:none; position: fixed; bottom: 10px; left: 10px; z-index: 9999;" scrolling="no"></iframe>