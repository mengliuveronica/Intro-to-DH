{
  "hash": "3463133ce171ae5a43647d2b0ebaf85c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"§6 Quantitative Text Analysis Basics 📊\"\nformat:\n  html:\n    toc: true\n    code-fold: false\n    code-link: true\n    highlight-style: github\n    include-in-header:\n      text: |\n        <script src=\"quiz.js\"></script>\n        <script src=\"content-switch.js\"></script>\nfilters: \n - collapse-callout.lua\nexecute:\n  freeze: auto\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the world of quantitative text analysis! In this chapter, we'll explore the fundamental concepts and techniques used to prepare text data for analysis. We'll use a collection of classic novels as our dataset to demonstrate these concepts.\n\n::: {.callout-note}\n## Learning Objectives\n- 📖 Understand the basics of text analysis and its applications in Digital Humanities\n- 🧹 Learn essential text preprocessing techniques\n- 🔤 Explore basic text analysis concepts like tokenization and word frequency\n- 📊 Visualize text data using simple techniques like word clouds\n:::\n\n# What is Quantitative Text Analysis? 🤔\n\nQuantitative text analysis (QTA) is a method of analyzing large volumes of text data using computational techniques. It allows researchers to extract meaningful patterns, themes, and insights from textual data that would be difficult or impossible to analyze manually.\n\n## QTA in Digital Humanities\n\nIn Digital Humanities, QTA offers powerful tools for exploring large text corpora:\n\n1. **Scale**: Analyze vast collections of texts, revealing patterns across time periods, genres, or authors.\n\n2. **Distant Reading**: Observe broader patterns in literature and cultural production.\n\n3. **Hypothesis Testing**: Empirically test literary and cultural theories across large datasets.\n\n4. **Discovery**: Reveal unexpected patterns or connections, sparking new research questions.\n\n5. **Interdisciplinary**: Combine methods from linguistics, computer science, and statistics with humanistic inquiry.\n\n6. **Visualization**: Present textual data in new, visually interpretable ways.\n\nQTA complements traditional close reading, offering Digital Humanities scholars new perspectives on cultural, historical, and literary phenomena.\n\n## QTA Workflow\n\n![](images/QTA%20workflow.png)\n\n::: {.callout-note}\n## QTA Workflow\n\nThe Quantitative Text Analysis (QTA) workflow illustrates the systematic process of analyzing textual data using computational methods. This workflow can be divided into two main stages: Preprocessing Steps and Quantitative Text Analysis.\n\n**Preprocessing Steps**:\nThis initial stage is crucial for preparing raw text data for analysis. It involves several operations:\n\n- *Tidy Structure*: Organizing the data into a consistent, clean format. Tidy format refers to one observation/value per row. \n- *Lowercase Conversion*: Transforming all text to lowercase for uniformity.\n- *Punctuation and/or Special Character Removal*: Eliminating punctuation marks and/or special characters that might interfere with analysis.\n- *Numbers Removal*: Deleting numerical digits if they're not relevant to the study.\n- *Tokenization*: Breaking down the text into individual units (usually words).\n- *Normalization*: Standardizing text to a more consistent form (e.g., handling special characters, expanding contractions).\n- *Stopwords Removal*: Eliminating common words that typically don't carry significant meaning.\n- *Stemming/Lemmatization*: Reducing words to their root form to group similar words together.\n\n[These steps are not always applied in a fixed order and may be selectively used depending on the specific requirements of the analysis.]{.highlight-blue} The output of this stage is processed text ready for analysis.\n\n**Quantitative Text Analysis**:\n\nThis stage involves applying various analytical techniques to the preprocessed text. Some common methods include:\n\n- *Word Frequency Analysis*: Counting and analyzing the occurrence of words.\n- *N-gram Analysis*: Examining sequences of N words to understand context and phrases.\n- *Topic Modeling*: Discovering abstract topics that occur in a collection of documents.\n- *Sentiment Analysis*: Determining the emotional tone behind a body of text.\n- And more...\n\nThis workflow emphasizes the importance of proper text preprocessing before conducting any advanced analysis. It provides a framework for researchers to follow, ensuring that their textual data is appropriately prepared and analyzed using quantitative methods. The diagram's structure, with its clear progression from preprocessing to analysis, highlights the sequential nature of the QTA process while also indicating the flexibility within each stage.\n:::\n\n:::{.callout-tip}\n## Install Package Manager `pacman`\n\nWe will be using some new packages that you probably haven't installed. To streamline the process of package installation, let's introduce a helpful tool for managing R packages: `pacman`. The `pacman` package is a convenient package management tool for R that simplifies the process of loading and installing multiple packages.\n\nKey features of `pacman`:\n\n1. It combines the functionality of `install.packages()` and `library()` into a single function `p_load()`.\n2. It automatically installs packages if they're not already installed.\n3. It can load multiple packages with a single line of code.\n\nLet's install `pacman`:\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(\"pacman\")) install.packages(\"pacman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: pacman\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\nOnce we load the `pacman` library, we can use `p_load()` to efficiently load (and install if necessary) the packages we'll need:\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\np_load(tidyverse, tidytext)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\nThis single line will ensure all the packages we need are installed and loaded, streamlining our setup process.\n:::\n\n\n\n# Text Preprocessing Techniques 🧹\n\n::: {.callout-note}\n## Understanding Text Preprocessing\n\nText preprocessing is the crucial first step in quantitative text analysis. It involves cleaning and standardizing raw text data to make it suitable for computational analysis.\n\n**Why is it important?**\n\n- Improves data quality and consistency\n- Reduces noise and irrelevant information\n- Enhances the accuracy of subsequent analyses\n- Makes text data more manageable for computational processing\n\n**Fundamental considerations:**\n\n1. **Purpose**: The preprocessing steps you choose should align with your research questions and analysis goals.\n2. **Language**: Different languages may require specific preprocessing techniques.\n3. **Domain**: The nature of your texts (e.g., literary, social media, historical) may influence preprocessing decisions.\n4. **Information preservation**: Be cautious not to remove potentially important information during preprocessing.\n5. **Reproducibility**: Document your preprocessing steps to ensure your analysis can be replicated.\n\nRemember, there's no one-size-fits-all approach to preprocessing. The techniques you apply should be carefully considered based on your specific research context.\n:::\n\nTo showcase the various techniques for text preprocessing, let's first create a mock dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmock_data <- tibble(\n  text = c(\n    \"The Quick Brown Fox Jumps Over the Lazy Dog! Data Science meets Cultural Studies.\",\n    \"Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literature & History\",\n    \"R Programming for Text Analysis - Chapter 3. Machine Learning for Textual Analysis\",\n    \"NLP techniques & their applications in DH research; Computational Methods in Humanities Research?\",\n    \"20+ ways to visualize data: graphs, charts, and more! Digital Archives and Text Mining Techniques.\"\n  )\n)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\nLet's take a closer look at our mock dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(mock_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 1\n  text                                                                          \n  <chr>                                                                         \n1 The Quick Brown Fox Jumps Over the Lazy Dog! Data Science meets Cultural Stud…\n2 Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literat…\n3 R Programming for Text Analysis - Chapter 3. Machine Learning for Textual Ana…\n4 NLP techniques & their applications in DH research; Computational Methods in …\n5 20+ ways to visualize data: graphs, charts, and more! Digital Archives and Te…\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.callout-note}\n## Reflection 🤔\n\nExamine the mock dataset above and reflect on the following questions:\n\n- What characteristics or elements do you notice that might need preprocessing for effective text analysis? \n\n- What challenges might these elements pose for text analysis? How might preprocessing help address these challenges?\n:::\n\n<details>\n<summary>Click to Reveal Some Insights</summary>\n\n1. **Capitalization**: Words are inconsistently capitalized (e.g., \"The Quick Brown Fox\" vs. \"Data Science\"). This could lead to treating identical words as different entities.\n\n2. **Punctuation**: Various punctuation marks are present, including periods, exclamation marks, colons, and semicolons. These might interfere with word tokenization and analysis.\n\n3. **Numbers**: Some entries contain numbers (e.g., \"101\", \"2024\", \"3\", \"20+\"). Depending on the analysis goals, these might need to be removed or treated specially.\n\n4. **Special Characters**: There are ampersands (&) and hyphens (-) which might need special handling.\n\n5. **Sentence Structure**: Each row contains multiple sentences. For sentence-level analysis, we might need to split these.\n\n6. **Abbreviations**: \"NLP\" and \"DH\" are present. We might need to decide whether to expand these or treat them as single tokens.\n\n7. **Stop Words**: Common words like \"the\", \"and\", \"for\" are present. These might not contribute much meaning to the analysis.\n\nThese observations highlight the need for various preprocessing steps, including:\n- Converting text to lowercase for consistency\n- Removing or standardizing punctuation\n- Handling numbers and special characters\n- Sentence tokenization\n- Word tokenization\n- Removing stop words\n\nBy addressing these elements through preprocessing, we can prepare our text data for more effective and accurate analysis.\n</details>\n\n## Understanding Regular Expressions\n\nBefore we dive into analyzing our mock data, let's explore a powerful tool in text analysis: **Regular Expressions**. \n\nHave you ever wondered how computer programs like Excel or Word can find the exact word or phrase you're searching for? Or how they can replace all instances of a word throughout a document in just seconds? These everyday text operations are powered by a concept called **pattern matching**, and regular expressions take this idea to a whole new level.\n\nIn this section, we'll introduce the basics of regular expressions, focusing on concepts and examples relevant to humanities research. We'll start with simple patterns similar to what you might use in Excel, and gradually move to more powerful expressions. By the end, you'll have a foundation that will help you better understand and implement the text preprocessing techniques we'll use in our analysis.\n\n\n::: {.callout-tip}\n## What are Regular Expressions?\n\nRegular Expressions, often called **regex**, are like a special language for describing patterns in text. Imagine you're a librarian with a magical magnifying glass that can find not just specific words, but patterns in books.\n\n### The Concept\n\n1. **Pattern Matching**: Instead of searching for exact words, regex lets you search for patterns. For example, you could search for:\n   - All words that start with \"pre\"\n   - Any sequence of five letters\n   - All dates in a specific format\n\n2. **Text Processing**: Once you find these patterns, you can do things like:\n   - Highlight them\n   - Replace them with something else\n   - Extract them for further study\n\n### Examples of Regex\nTo give you a better idea of what regular expressions look like and how they work, let's look at an example:\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(stringr)\n\n# Sample text\ntext <- \"Jane Austen wrote Pride and Prejudice. Elizabeth Bennet is the protagonist.\"\n\n# Regex pattern for capitalized words\npattern <- \"\\\\b[A-Z][a-z]+\\\\b\"\n\n# Find all matches\nmatches <- str_extract_all(text, pattern)\n\n# Print the matches\nprint(matches)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] \"Jane\"      \"Austen\"    \"Pride\"     \"Prejudice\" \"Elizabeth\" \"Bennet\"   \n```\n\n\n:::\n\n```{.r .cell-code}\n# To see which words were matched in context\nstr_view(text, pattern)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] │ <Jane> <Austen> wrote <Pride> and <Prejudice>. <Elizabeth> <Bennet> is the protagonist.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n:::{.callout-note}\n## Regex breakdown\n\nLet's break down the regex pattern `\\\\b[A-Z][a-z]+\\\\b`:\n\n1. `\\\\b`: This represents a word boundary. In R, we need to escape the backslash, so we use two. It ensures we're matching whole words, not parts of words.\n\n2. `[A-Z]`: This character class matches any single uppercase letter from A to Z.\n\n3. `[a-z]+`: This matches one or more lowercase letters.\n   - `[a-z]` is a character class that matches any single lowercase letter.\n   - The `+` quantifier means \"one or more\" of the preceding element.\n\n4. `\\\\b`: Another word boundary to end the match.\n\nSo, this pattern matches:\n- Words that start with a capital letter (like names or the first word of a sentence)\n- Followed by one or more lowercase letters\n- As whole words, not parts of larger words\n\nIt won't match:\n- ALL CAPS words\n- words with numbers or symbols\n- Single-letter capitalized words like \"I\" or \"A\"\n\nThis pattern is useful for finding proper nouns in the middle of sentences, like names of people or places.\n:::\n\n\n### POSIX Character Classes: A Friendly Starting Point\n\nYou may find that regex can be quite hard to read for humans. POSIX character classes are pre-defined sets of characters that make regex more accessible and portable across different systems. They simplify regex patterns and address some common challenges in text processing:\n\n1. **Simplification**: POSIX classes provide easy-to-remember shorthand for common character groups. Instead of writing `[A-Za-z]` to match any letter, you can use `[:alpha:]`.\n\n2. **Consistency**: They ensure consistent behavior across different operating systems and programming languages. For example, `[A-Z]` might behave differently in some contexts depending on the locale settings, but `[:upper:]` is always consistent.\n\n3. **Internationalization**: POSIX classes can handle characters beyond the ASCII range, making them useful for working with texts in various languages.\n\n4. **Readability**: They make regex patterns more readable and self-explanatory, which is especially helpful when sharing code or working in teams.\n\nHere are some useful POSIX character classes:\n\n- `[:alpha:]`: Matches any alphabetic character (equivalent to `[A-Za-z]` in English texts)\n- `[:digit:]`: Matches any digit (equivalent to `[0-9]`)\n- `[:lower:]`: Matches any lowercase letter\n- `[:upper:]`: Matches any uppercase letter\n- `[:punct:]`: Matches any punctuation character\n- `[:space:]`: Matches any whitespace character (spaces, tabs, newlines)\n\nBy using these classes, you can create more robust and readable regex patterns. For example, instead of `[A-Za-z0-9]` to match any alphanumeric character, you could use `[[:alpha:][:digit:]]`, which is clearer in its intent and works across different language settings.\n````\n\n### Examples in Humanities Context\n\n1. Finding capitalized words: \n   - Pattern: `[[:upper:]][[:lower:]]+`\n   - This would match words like \"Shakespeare\", \"London\", \"Renaissance\"\n\n2. Identifying years:\n   - Pattern: `[[:digit:]]{4}`\n   - This would match years like \"1564\", \"1616\", \"2023\"\n\n3. Locating punctuation:\n   - Pattern: `[[:punct:]]`\n   - This would find all punctuation marks in a text\n\nRemember, regex is a tool that becomes more useful as you practice. Start simple, and you'll gradually be able to create more complex patterns for your research needs!\n:::\n\n## Learning Check 🏁 \n\n<div id=\"quiz-qta-intro\"></div>\n\n<script>\ncreateQuiz({\n  questions: [\n    {\n      text: \"What is the primary purpose of Quantitative Text Analysis (QTA)?\",\n      options: [\n        \"To translate texts into different languages\",\n        \"To analyze large volumes of text data using computational techniques\",\n        \"To improve the grammar and style of written texts\",\n        \"To create new literary works using artificial intelligence\"\n      ]\n    },\n    {\n      text: \"Which of the following is NOT a benefit of QTA in Digital Humanities?\",\n      options: [\n        \"Analyzing vast collections of texts\",\n        \"Observing broader patterns in literature\",\n        \"Replacing close reading entirely\",\n        \"Empirically testing literary theories\"\n      ]\n    },\n    {\n      text: \"In the QTA workflow, what is typically the first stage?\",\n      options: [\n        \"Topic Modeling\",\n        \"Sentiment Analysis\",\n        \"Preprocessing Steps\",\n        \"Visualization\"\n      ]\n    },\n    {\n      text: \"What is the main purpose of regular expressions (regex) in text analysis?\",\n      options: [\n        \"To translate text between languages\",\n        \"To find and match patterns in text\",\n        \"To increase the font size of text\",\n        \"To create new words in a language\"\n      ]\n    },\n    {\n      text: \"Which POSIX character class would you use to match any punctuation mark?\",\n      options: [\n        \"[:alpha:]\",\n        \"[:digit:]\",\n        \"[:lower:]\",\n        \"[:punct:]\"\n      ]\n    }\n  ],\n  answers: [1, 2, 2, 1, 3]\n}, \"quiz-qta-intro\");\n</script>\n\n\nNow let's proceed to preprocess the mock dataset!\n\n## Tidy Structure\n\nThe first step in our preprocessing pipeline is to ensure our data is in a tidy format, with one sentence per row. We'll use `separate_rows()` with a regular expression (regex) pattern to achieve this:\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_data <- mock_data %>%\n  separate_rows(text, sep = \"(?<=[.!?])\\\\s+(?=[A-Z])\")\n\nprint(tidy_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 1\n  text                                                                          \n  <chr>                                                                         \n1 The Quick Brown Fox Jumps Over the Lazy Dog!                                  \n2 Data Science meets Cultural Studies.                                          \n3 Digital Humanities 101: An Introduction (2024); Exploring Big Data in Literat…\n4 R Programming for Text Analysis - Chapter 3.                                  \n5 Machine Learning for Textual Analysis                                         \n6 NLP techniques & their applications in DH research; Computational Methods in …\n7 20+ ways to visualize data: graphs, charts, and more!                         \n8 Digital Archives and Text Mining Techniques.                                  \n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-note}\n## Understanding `separate_rows()` \n\n1. `separate_rows()` Function:\n   - Part of the tidyr package in tidyverse\n   - Separates a column into multiple rows based on a delimiter\n   - Syntax: `separate_rows(data, column, sep = delimiter)`\n\n2. Regular Expression (Regex) Pattern:\n   - `(?<=[.!?])`: Positive lookbehind, matches a position after a period, exclamation mark, or question mark\n   - `\\\\s+`: Matches one or more whitespace characters\n   - `(?=[A-Z])`: Positive lookahead, matches a position before an uppercase letter\n   - Combined, this pattern splits the text at sentence boundaries\n\n3. How it works:\n   - `separate_rows()` applies the regex pattern to split the 'text' column\n   - Each resulting sentence becomes a new row in the dataframe\n   - The original row's other columns (if any) are duplicated for each new sentence row\n:::\n\n\n\n\n\n## Learning Check 🏁 \n\n<div id=\"quiz-qta\"></div>\n\n<script>\ncreateQuiz({\n  questions: [\n    {\n      text: \"What is the primary purpose of text preprocessing in quantitative text analysis?\",\n      options: [\n        \"To make the text more readable for humans\",\n        \"To prepare the text for computational analysis\",\n        \"To translate the text into different languages\",\n        \"To increase the text's word count\"\n      ]\n    },\n    {\n      text: \"What does the `tolower()` function do in R?\",\n      options: [\n        \"Removes all lowercase letters\",\n        \"Converts text to uppercase\",\n        \"Converts text to lowercase\",\n        \"Removes all uppercase letters\"\n      ]\n    },\n    {\n      text: \"In the context of regex, what does the pattern `[[:punct:]]` match?\",\n      options: [\n        \"All alphabetic characters\",\n        \"All numeric characters\",\n        \"All punctuation characters\",\n        \"All whitespace characters\"\n      ]\n    },\n    {\n      text: \"What is the purpose of the `str_replace_all()` function in text preprocessing?\",\n      options: [\n        \"To count the occurrences of a pattern\",\n        \"To extract specific patterns from text\",\n        \"To replace all occurrences of a pattern with another string\",\n        \"To split text into individual words\"\n      ]\n    }\n  ],\n  answers: [1, 2, 2, 2]\n}, \"quiz-qta\");\n</script>\n\n\n## Hands-On Coding 💻\n\nNow, let's apply our text preprocessing skills to a real dataset. We'll use the answers submitted by you in response to the \"what is digital humanities? write down three keywords that come to mind\" question. Our goal is to clean and prepare this text data for analysis.\n\nFirst, let's download the dataset:\n\n[Download DH Keywords dataset](data/dh_keywords.csv)\n\nSave it in the folder name `data` in your working directory. \n\nWe can then load the csv file into RStudio:\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(tidyverse)\n\n# Load the dataset\ndh_keywords <- read_csv(\"data/dh_keywords.csv\", col_names = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 180 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): X1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ncolnames(dh_keywords) <- \"keyword\"\n\n# Display the first few rows\nhead(dh_keywords)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  keyword\n  <chr>  \n1 math   \n2 chatgpt\n3 AI     \n4 ai     \n5 AI     \n6 ai     \n```\n\n\n:::\n:::\n\n### Exercise 1: Lowercase Conversion\n\nConvert all keywords to lowercase.\n\n<div id=\"hint-preprocess1-1\" style=\"display: none;\">\n::: {.callout-tip collapse=\"false\"}\n## Hint\nUse the `mutate()` function with `tolower()` to convert the text to lowercase.\n:::\n</div>\n\n<script>\ncreateProgressiveHints('hint-preprocess1', 1);\n</script>\n\n<div id=\"template-preprocess1\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_lower <- dh_keywords %>%\n  mutate(keyword = _(keyword))\n\nhead(dh_keywords_lower)\n```\n:::\n\n</div>\n\n<script>\ncreateToggleSection('template-button-preprocess1', 'template-preprocess1', 'Show Template');\n</script>\n\n<div id=\"solution-preprocess1\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_lower <- dh_keywords %>%\n  mutate(keyword = tolower(keyword))\n\nhead(dh_keywords_lower)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  keyword\n  <chr>  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n```\n\n\n:::\n:::\n\n</div>\n\n<script>\ncreateToggleSection('solution-button-preprocess1', 'solution-preprocess1');\n</script>\n\n### Exercise 2: Remove Punctuation\n\nRemove all punctuation from the keywords.\n\n<div id=\"hint-preprocess2-1\" style=\"display: none;\">\n::: {.callout-tip collapse=\"false\"}\n## Hint\nUse `str_replace_all()` with a regular expression to remove punctuation.\n:::\n</div>\n\n<script>\ncreateProgressiveHints('hint-preprocess2', 1);\n</script>\n\n<div id=\"template-preprocess2\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_clean <- dh_keywords_lower %>%\n  mutate(keyword = str_replace_all(keyword, _, \"\"))\n\nhead(dh_keywords_clean)\n```\n:::\n\n</div>\n\n<script>\ncreateToggleSection('template-button-preprocess2', 'template-preprocess2', 'Show Template');\n</script>\n\n<div id=\"solution-preprocess2\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_clean <- dh_keywords_lower %>%\n  mutate(keyword = str_replace_all(keyword, \"[[:punct:]]\", \"\"))\n\nhead(dh_keywords_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  keyword\n  <chr>  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n```\n\n\n:::\n:::\n\n</div>\n\n<script>\ncreateToggleSection('solution-button-preprocess2', 'solution-preprocess2');\n</script>\n\n### Exercise 3: Remove Extra Whitespace\n\nRemove any leading, trailing, or extra whitespace from the keywords.\n\n<div id=\"hint-preprocess3-1\" style=\"display: none;\">\n::: {.callout-tip collapse=\"false\"}\n## Hint\nUse `str_trim()` to remove leading and trailing whitespace, and `str_squish()` to replace multiple spaces with a single space.\n:::\n</div>\n\n<script>\ncreateProgressiveHints('hint-preprocess3', 1);\n</script>\n\n<div id=\"template-preprocess3\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_trimmed <- dh_keywords_clean %>%\n  mutate(keyword = str_trim(_(_)))\n\nhead(dh_keywords_trimmed)\n```\n:::\n\n</div>\n\n<script>\ncreateToggleSection('template-button-preprocess3', 'template-preprocess3', 'Show Template');\n</script>\n\n<div id=\"solution-preprocess3\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_trimmed <- dh_keywords_clean %>%\n  mutate(keyword = str_trim(str_squish(keyword)))\n\nhead(dh_keywords_trimmed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  keyword\n  <chr>  \n1 math   \n2 chatgpt\n3 ai     \n4 ai     \n5 ai     \n6 ai     \n```\n\n\n:::\n:::\n\n</div>\n\n<script>\ncreateToggleSection('solution-button-preprocess3', 'solution-preprocess3');\n</script>\n\n### Exercise 4: Remove Empty Entries\n\nRemove any rows that are empty after our preprocessing steps.\n\n<div id=\"hint-preprocess4-1\" style=\"display: none;\">\n::: {.callout-tip collapse=\"false\"}\n## Hint\nUse `filter()` with `nchar()` to keep only non-empty entries.\n:::\n</div>\n\n<script>\ncreateProgressiveHints('hint-preprocess4', 1);\n</script>\n\n<div id=\"template-preprocess4\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_final <- dh_keywords_trimmed %>%\n  filter(_(keyword) > _)\n\n# Display the final result and the number of rows\nprint(dh_keywords_final)\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_final))\n```\n:::\n\n</div>\n\n<script>\ncreateToggleSection('template-button-preprocess4', 'template-preprocess4', 'Show Template');\n</script>\n\n<div id=\"solution-preprocess4\" style=\"display: none;\">\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_final <- dh_keywords_trimmed %>%\n  filter(nchar(keyword) > 0)\n\n# Display the final result and the number of rows\nprint(dh_keywords_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 180 × 1\n   keyword\n   <chr>  \n 1 math   \n 2 chatgpt\n 3 ai     \n 4 ai     \n 5 ai     \n 6 ai     \n 7 coding \n 8 ai     \n 9 ai     \n10 data   \n# ℹ 170 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\npaste(\"Number of keywords after preprocessing:\", nrow(dh_keywords_final))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Number of keywords after preprocessing: 180\"\n```\n\n\n:::\n:::\n\nCombined version of all the preprocessing steps:\n\n::: {.cell}\n\n```{.r .cell-code}\ndh_keywords_final <- dh_keywords %>%\n  mutate(keyword = tolower(keyword),\n         keyword = str_replace_all(keyword, \"[[:punct:]]\", \"\"),\n         keyword = str_trim(str_squish(keyword))) %>%\n  filter(nchar(keyword) > 0)\n```\n:::\n\n</div>\n\n<script>\ncreateToggleSection('solution-button-preprocess4', 'solution-preprocess4');\n</script>\n\nCongratulations! You've successfully preprocessed the DH keywords dataset. These clean keywords are now ready for further analysis, such as frequency counting, visualization, or more advanced text analysis techniques.\n\n\n# Basic Text Analysis Concepts 🔤\n\nIn this section, we'll explore fundamental concepts in text analysis that form the building blocks for more advanced techniques. These concepts are crucial for understanding how we can extract meaningful insights from textual data.\n\n\n\n## Tokenization\n\nTokenization is a foundational step in text analysis, involving the process of breaking down text into smaller, meaningful units called tokens. These tokens can be words, phrases, or even sentences, depending on the level of analysis you're performing.\n\n### Word Tokenization\n\nWord tokenization is the most common form of tokenization, where text is split into individual words. This process allows us to analyze text at its most granular level. Let's use our preprocessed DH keywords dataset to demonstrate:\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(tidytext, tidyverse)\n\nword_tokens <- dh_keywords_final %>%\n  unnest_tokens(word, keyword)\n\n# Display the first few tokens\nword_tokens %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 1\n   word   \n   <chr>  \n 1 math   \n 2 chatgpt\n 3 ai     \n 4 ai     \n 5 ai     \n 6 ai     \n 7 coding \n 8 ai     \n 9 ai     \n10 data   \n```\n\n\n:::\n:::\n\n:: {.callout-tip}\n## The `unnest_tokens()` function\nThe `unnest_tokens()` function from the tidytext package is particularly useful because it integrates seamlessly with the tidyverse ecosystem. It splits the text into tokens and transforms the data into a one-token-per-row format, which is ideal for further analysis using dplyr, ggplot2, and other tidy tools.\n:::\n\nIn this example, our keywords are already individual words, so the tokenization might not show a significant difference. However, for longer texts like novels or articles, this process would break down sentences into individual words, allowing for more detailed analysis.\n\n## Stemming and Lemmatization\n\nStemming and lemmatization are techniques used to reduce words to their base or root form. This process is crucial for grouping similar words together and reducing the vocabulary size, which can improve the efficiency and effectiveness of text analysis.\n\n### Stemming\n\nStemming is a simple, rule-based approach to reduce words to their stem or root form. It typically involves removing the ends of words, which can sometimes result in stems that are not actual words.\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(SnowballC)\n\nstemmed_words <- word_tokens %>%\n  mutate(stem = wordStem(word))\n\n# Display a few examples\nstemmed_words %>%\n  select(word, stem) %>%\n  distinct() %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   word          stem   \n   <chr>         <chr>  \n 1 math          math   \n 2 chatgpt       chatgpt\n 3 ai            ai     \n 4 coding        code   \n 5 data          data   \n 6 plus          plu    \n 7 humanbeing    humanb \n 8 social        social \n 9 media         media  \n10 informational inform \n```\n\n\n:::\n:::\n\n::: {.callout-tip}\n## The `wordStem()` function\nThe `wordStem()` function from the SnowballC package implements the Porter stemming algorithm, which is widely used in text processing. While stemming is fast and easy to implement, it can sometimes produce stems that are not real words, which might be confusing in some contexts.\n:::\n\n### Lemmatization\n\nLemmatization is a more sophisticated approach that considers the context and part of speech of a word to determine its base form or lemma. Unlike stemming, lemmatization always produces real words.\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(textstem)\n\nlemmatized_words <- word_tokens %>%\n  mutate(lemma = lemmatize_words(word))\n\n# Display a few examples\nlemmatized_words %>%\n  select(word, lemma) %>%\n  distinct() %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   word          lemma        \n   <chr>         <chr>        \n 1 math          math         \n 2 chatgpt       chatgpt      \n 3 ai            ai           \n 4 coding        code         \n 5 data          datum        \n 6 plus          plus         \n 7 humanbeing    humanbeing   \n 8 social        social       \n 9 media         medium       \n10 informational informational\n```\n\n\n:::\n:::\n\n::: {.callout-tip}\n## The `lemmatize_words()` function\nThe `lemmatize_words()` function from the textstem package uses a dictionary-based approach to find the lemma of each word. Lemmatization is generally more accurate than stemming but can be slower and more computationally intensive.\n:::\n\n::: {.callout-note}\n## How to Choose Between Stemming and Lemmatization?\n\nChoosing between stemming and lemmatization depends on your specific needs, resources, and the nature of your text analysis project:\n\n1. **Speed vs. Accuracy**: \n\n   - Stemming is faster but can produce non-words or incorrect roots.\n   - Lemmatization is slower but more accurate, always producing real words.\n\n2. **Language Complexity**:\n\n   - For languages with simple morphology (like English), stemming might suffice.\n   - For languages with complex morphology (like German or Arabic), lemmatization is often preferred.\n   - For languages without clear word boundaries (like Chinese or Japanese):\n     * Neither traditional stemming nor lemmatization applies directly.\n     * Word segmentation is a crucial first step.\n     * After segmentation, techniques similar to lemmatization may be used to normalize words.\n\n3. **Domain Specificity**:\n\n   - In specialized fields (e.g., medical texts), lemmatization's accuracy can be crucial because it preserves the precise meanings of complex, domain-specific terms that might be lost or conflated through simpler stemming methods.\n   - For general-purpose analysis, stemming's speed might be more valuable.\n\n4. **Available Resources**:\n\n   - Stemming requires less computational power and no additional linguistic resources.\n   - Lemmatization often needs dictionaries and more processing power.\n\n5. **Project Goals**:\n\n   - For quick, exploratory analysis, stemming can be sufficient.\n   - For in-depth linguistic analysis or when meaning preservation is crucial, use lemmatization.\n\n6. **Text Quality**:\n\n   - For well-edited texts, both methods work well.\n   - For informal texts (e.g., social media), stemming might handle variations and misspellings better.\n\n7. **Interpretability**:\n\n   - Stemmed words can be harder for humans to interpret.\n   - Lemmatized words are always real words, making results more readable.\n\nIn practice, many researchers try both methods and compare results. For humanities projects, especially those involving literary analysis or historical texts, lemmatization often provides more meaningful and interpretable results, despite the additional computational cost.\n:::\n\n## N-grams\n\nN-grams are contiguous sequences of n items from a given text. These items can be words, characters, or even syllables. N-grams help capture phrases and word associations, providing context that single words might miss.\n\n::: {.callout-note}\n## Understanding N-grams\n\n- Unigrams (n=1): Single words, e.g., \"digital\", \"humanities\"\n- Bigrams (n=2): Two consecutive words, e.g., \"digital humanities\"\n- Trigrams (n=3): Three consecutive words, e.g., \"natural language processing\"\n- And so on for larger n...\n\nN-grams preserve word order, which can be crucial for understanding meaning and context in text analysis.\n:::\n\nLet's create bigrams from our original (non-preprocessed) DH keywords dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams <- dh_keywords %>%\n  unnest_tokens(bigram, keyword, token = \"ngrams\", n = 2)\n\n# Display the most common bigrams\nbigrams %>%\n  count(bigram, sort = TRUE) %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   bigram               n\n   <chr>            <int>\n 1 <NA>                67\n 2 ai coding            6\n 3 coding data          6\n 4 data ai              6\n 5 data coding          6\n 6 computer science     5\n 7 data analysis        5\n 8 ai data              4\n 9 social science       4\n10 analysis coding      3\n```\n\n\n:::\n:::\n\n::: {.callout-tip}\n## Why use bigrams?\n\nBigrams can reveal common phrases and word pairs in the text that single words miss. For instance, \"digital humanities\" carries more specific meaning than \"digital\" and \"humanities\" separately. In literary analysis, bigrams like \"stream consciousness\" might indicate modernist writing techniques.\n:::\n\nN-grams are particularly useful for:\n\n1. **Phrase Detection**: Identifying common expressions or technical terms.\n2. **Contextual Analysis**: Understanding how words are used in context.\n3. **Language Modeling**: Predicting the next word based on previous words.\n4. **Stylometric Analysis**: Studying an author's writing style.\n\n::: {.callout-note}\n## Considering Multiple N-gram Sizes\n\nResearchers often use a combination of n-gram sizes (e.g., unigrams, bigrams, and trigrams) for several reasons:\n\n1. **Comprehensive Coverage**: Different n-gram sizes capture different aspects of language.\n2. **Balancing Specificity and Generality**: Larger n-grams are more specific but occur less frequently.\n3. **Handling Varied Phrases**: Some meaningful phrases are two words, others three or more.\n\nHowever, using multiple n-gram sizes also has challenges:\n- **Increased Complexity**: More data to process and analyze.\n- **Overlapping Information**: Bigrams and trigrams may contain redundant information.\n- **Sparse Data**: Larger n-grams may occur very infrequently, leading to sparse data issues.\n\nThe choice depends on your research questions and the nature of your texts.\n:::\n\nHere's an example of how we might generate n-grams of different sizes:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(dplyr)\n\n# Function to generate n-grams\ngenerate_ngrams <- function(data, n) {\n  data %>%\n    unnest_tokens(ngram, keyword, token = \"ngrams\", n = n) %>%\n    count(ngram, sort = TRUE) %>%\n    mutate(n = n)\n}\n\n# Generate unigrams, bigrams, and trigrams\nunigrams <- generate_ngrams(dh_keywords, 1)\nbigrams <- generate_ngrams(dh_keywords, 2)\ntrigrams <- generate_ngrams(dh_keywords, 3)\n\n# Combine the results\nall_ngrams <- bind_rows(unigrams, bigrams, trigrams)\n\n# Display the top 5 of each\nall_ngrams %>%\n  group_by(n) %>%\n  slice_head(n = 5) %>%\n  arrange(n, desc(nn))\n```\n:::\n\nThis approach allows us to see the most common phrases of different lengths in our dataset, providing a more comprehensive view of the language used in digital humanities keywords.\n\n::: {.callout-warning}\n## Preprocessing Considerations\n\nWhen working with n-grams, it's often better to use non-preprocessed text:\n\n1. Punctuation and capitalization can be meaningful for phrase detection.\n2. Stopwords might be important parts of meaningful phrases.\n3. Word order matters, so stemming or lemmatization might obscure important distinctions.\n\nPreprocessing can be applied after n-gram generation if needed, but this allows for more nuanced analysis of phrases and expressions.\n:::\n\nBy considering different n-gram sizes and using non-preprocessed text, we can gain richer insights into the language patterns and meaningful phrases in our digital humanities corpus.\n\n## Stopword Removal\n\nStopwords are common words (like \"the\", \"is\", \"at\") that are often removed from text analysis because they typically don't carry much meaning on their own. Removing stopwords can help focus the analysis on more meaningful content words.\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(tidytext)\n\ndata(stop_words)\n\nwords_without_stopwords <- word_tokens %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare word counts before and after stopword removal\npaste(\"Words before stopword removal:\", nrow(word_tokens))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Words before stopword removal: 393\"\n```\n\n\n:::\n\n```{.r .cell-code}\npaste(\"Words after stopword removal:\", nrow(words_without_stopwords))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Words after stopword removal: 374\"\n```\n\n\n:::\n:::\n\n::: {.callout-tip}\n## The `anti_join()` function\nThe `anti_join()` function is used here to remove stopwords from our dataset. It keeps only the rows from the first dataset (word_tokens) that do not have a match in the second dataset (stop_words). This effectively removes all the stopwords from our text data.\n:::\n\n\n## Word Frequency Analysis\n\nWord frequency analysis involves counting how often each word appears in a text. This can give us insights into the most important or prevalent themes in the text.\n\n::: {.cell}\n\n```{.r .cell-code}\nword_frequencies <- words_without_stopwords %>%\n  count(word, sort = TRUE)\n\n# Display the top 10 most frequent words\nword_frequencies %>%\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   word            n\n   <chr>       <int>\n 1 data           53\n 2 ai             39\n 3 coding         31\n 4 analysis       18\n 5 technology     17\n 6 science        13\n 7 computer       12\n 8 statistics      8\n 9 programming     7\n10 digital         5\n```\n\n\n:::\n:::\n\n# Visualizing Text Data 📊\n\n## Word Clouds\n\nWord clouds are a popular way to visualize the most frequent words in a text, with the size of each word proportional to its frequency.\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(wordcloud2)\n\n# Create a word cloud\nwordcloud2(data = word_frequencies %>% slice_head(n = 50), size = 0.5)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"wordcloud2 html-widget html-fill-item\" id=\"htmlwidget-d0fb6cc5b98c385f9b0e\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d0fb6cc5b98c385f9b0e\">{\"x\":{\"word\":[\"data\",\"ai\",\"coding\",\"analysis\",\"technology\",\"science\",\"computer\",\"statistics\",\"programming\",\"digital\",\"ethics\",\"chatgpt\",\"code\",\"human\",\"research\",\"tools\",\"artificial\",\"culture\",\"humanities\",\"interdisciplinary\",\"quantitative\",\"social\",\"society\",\"text\",\"complicated\",\"corpus\",\"creativity\",\"difficult\",\"humanity\",\"innovative\",\"intelligence\",\"linguistics\",\"multimedia\",\"network\",\"processing\",\"program\",\"programing\",\"video\",\"abstract\",\"advanced\",\"aicoding\",\"aidata\",\"aidatatext\",\"aiit\",\"aiprogramingdata\",\"aiprogramming\",\"analysing\",\"analytical\",\"applying\",\"auxiliary\"],\"freq\":[53,39,31,18,17,13,12,8,7,5,5,4,4,4,4,4,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":1.69811320754717,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.7853981633974483,\"maxRotation\":0.7853981633974483,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}</script>\n```\n\n:::\n\n```{.r .cell-code}\n# Make it more aesthetically pleasing\np_load(RColorBrewer)\n\n# Create a color palette\ncolor_palette <- brewer.pal(8, \"Dark2\")\n\nwordcloud2(\n  data = word_frequencies %>% slice_head(n = 50),  # Use top 50 most frequent words\n  size = 0.6,                    # Increase text size for better readability\n  color = rep_len(color_palette, 50),  # Apply color palette to words\n  backgroundColor = \"white\",     # Set background color to white\n  rotateRatio = 0.3,             # Reduce word rotation for cleaner look\n  shape = \"circle\",              # Set overall shape of the word cloud\n  fontFamily = \"Arial\",          # Use Arial font for consistency\n  fontWeight = \"bold\",           # Make text bold for emphasis\n  minRotation = -pi/6,           # Set minimum rotation angle (30 degrees left)\n  maxRotation = pi/6             # Set maximum rotation angle (30 degrees right)\n)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"wordcloud2 html-widget html-fill-item\" id=\"htmlwidget-92f1053e576f99270879\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-92f1053e576f99270879\">{\"x\":{\"word\":[\"data\",\"ai\",\"coding\",\"analysis\",\"technology\",\"science\",\"computer\",\"statistics\",\"programming\",\"digital\",\"ethics\",\"chatgpt\",\"code\",\"human\",\"research\",\"tools\",\"artificial\",\"culture\",\"humanities\",\"interdisciplinary\",\"quantitative\",\"social\",\"society\",\"text\",\"complicated\",\"corpus\",\"creativity\",\"difficult\",\"humanity\",\"innovative\",\"intelligence\",\"linguistics\",\"multimedia\",\"network\",\"processing\",\"program\",\"programing\",\"video\",\"abstract\",\"advanced\",\"aicoding\",\"aidata\",\"aidatatext\",\"aiit\",\"aiprogramingdata\",\"aiprogramming\",\"analysing\",\"analytical\",\"applying\",\"auxiliary\"],\"freq\":[53,39,31,18,17,13,12,8,7,5,5,4,4,4,4,4,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1],\"fontFamily\":\"Arial\",\"fontWeight\":\"bold\",\"color\":[\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\",\"#7570B3\",\"#E7298A\",\"#66A61E\",\"#E6AB02\",\"#A6761D\",\"#666666\",\"#1B9E77\",\"#D95F02\"],\"minSize\":0,\"weightFactor\":2.037735849056604,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.5235987755982988,\"maxRotation\":0.5235987755982988,\"shuffle\":true,\"rotateRatio\":0.3,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}</script>\n```\n\n:::\n:::\n\n## Bar Charts of Word Frequency\n\nBar charts offer a more precise way to visualize word frequencies, especially for comparing the most common words.\n\n::: {.cell}\n\n```{.r .cell-code}\np_load(ggplot2)\n\nword_frequencies %>%\n  slice_head(n = 10) %>%\n  ggplot(aes(x = reorder(word, n), y = n, fill = desc(n))) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal()+\n  theme(legend.position = \"none\") +\n  labs(x = \"Word\", y = \"Frequency\", title = \"Top 10 Most Frequent DH Keywords\")\n```\n\n::: {.cell-output-display}\n![](06-qta_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n## Learning Check 🏁\n\n<div id=\"quiz-text-analysis\"></div>\n\n<script>\ncreateQuiz({\n  questions: [\n    {\n      text: \"What is tokenization in text analysis?\",\n      options: [\n        \"The process of removing punctuation from text\",\n        \"Breaking down text into smaller units like words or sentences\",\n        \"Converting all text to lowercase\",\n        \"Removing common words from text\"\n      ]\n    },\n    {\n      text: \"What are stopwords in text analysis?\",\n      options: [\n        \"Words that appear most frequently in a text\",\n        \"Words that are misspelled in a text\",\n        \"Common words like 'the', 'is', 'at' that are often removed from analysis\",\n        \"Words that are unique to a specific text\"\n      ]\n    },\n    {\n      text: \"What does a word frequency analysis show?\",\n      options: [\n        \"The length of words in a text\",\n        \"The order of words in a text\",\n        \"How often each word appears in a text\",\n        \"The sentiment of words in a text\"\n      ]\n    },\n    {\n      text: \"In a word cloud visualization, what does the size of a word typically represent?\",\n      options: [\n        \"The length of the word\",\n        \"The alphabetical order of the word\",\n        \"The frequency of the word in the text\",\n        \"The sentiment of the word\"\n      ]\n    }\n  ],\n  answers: [1, 2, 2, 2]\n}, \"quiz-text-analysis\");\n</script>\n\nThis learning check will help reinforce the key concepts covered in this section on basic text analysis and visualization techniques.\n\n\n<iframe src=\"float_traffic.html\" width=\"200px\" height=\"200px\" style=\"border:none; position: fixed; bottom: 10px; right: 10px; z-index: 9999;\" scrolling=\"no\"></iframe>\n\n<iframe src=\"float_dash.html\" width=\"200px\" height=\"200px\" style=\"border:none; position: fixed; bottom: 10px; left: 10px; z-index: 9999;\" scrolling=\"no\"></iframe>",
    "supporting": [
      "06-qta_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/wordcloud2-0.0.1/wordcloud.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/wordcloud2-0.0.1/wordcloud2-all.js\"></script>\n<script src=\"../site_libs/wordcloud2-0.0.1/hover.js\"></script>\n<script src=\"../site_libs/wordcloud2-binding-0.2.1/wordcloud2.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}